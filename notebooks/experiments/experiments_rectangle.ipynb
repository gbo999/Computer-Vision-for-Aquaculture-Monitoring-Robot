{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0: full length:GX010067_33_625.jpg_gamma_stabilized_00001, bx=2875, by=1822, width=51, height=17, length=10\n",
      "Processing row 1: full length:GX010067_33_625.jpg_gamma_stabilized_00001, bx=2875, by=1822, width=51, height=17, length=53\n",
      "Processing row 2: full length:GX010067_33_625.jpg_gamma_stabilized_00001, bx=2878, by=524, width=489, height=462, length=126\n",
      "Processing row 3: full length:GX010067_33_625.jpg_gamma_stabilized_00001, bx=3643, by=1762, width=344, height=383, length=96\n",
      "Processing row 4: full length:GX010067_33_625.jpg_gamma_stabilized_00001, bx=997, by=1944, width=821, height=225, length=160\n",
      "Processing row 5: full length:GX010067_33_625.jpg_gamma_stabilized_00001, bx=82, by=2094, width=505, height=44, length=95\n",
      "Saved output image to annot.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import math\n",
    "# Load the CSV file\n",
    "file_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\excel_examples\\Full_length.xlsx\"\n",
    "data=pd.read_excel(file_path)\n",
    "# Process each row in the CSV\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1/full body\\undistorted_GX010067_33_625.jpg_gamma.jpg\" # Adjust the path as needed\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    # Extract the details from the row\n",
    "    label = row['Label']\n",
    "\n",
    "    bx = int(row['BX']*5.3)\n",
    "    by = int(row['BY']*5.3)\n",
    "    width = int(row['Width']*5.3)\n",
    "    height = int(row['Height']*5.3)\n",
    "    length = int(row['Length'])\n",
    "    angle = row['Angle']\n",
    "\n",
    "\n",
    "    #draw rectangle\n",
    "    color = (0, 255, 0)  # Green color in BGR\n",
    "    thickness = 2\n",
    "    image = cv2.rectangle(image, (bx, by), (bx + width, by + height), color, thickness)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Processing row {index}: {label}, bx={bx}, by={by}, width={width}, height={height}, length={length}\")   \n",
    "\n",
    "    if angle<0 and angle>-90:\n",
    "            start_point = (bx, by+height)\n",
    "            end_point = ( bx+width,by+height )             \n",
    "    elif angle < -90 and angle > -180:\n",
    "             start_point = (bx, by+height)\n",
    "             end_point = ( bx+width,by  )  \n",
    "\n",
    "    elif angle < 180 and angle > 90:\n",
    "             start_point = (bx, by)\n",
    "             end_point = ( bx+width,by+height  )\n",
    "    else:\n",
    "                start_point = (bx, by+height)\n",
    "                end_point = ( bx+width,by+height ) \n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    color = (255, 0, 0)  # Blue color in BGR\n",
    "    thickness = 2\n",
    "    image = cv2.line(image, start_point, end_point, color, thickness)\n",
    "\n",
    "    # Draw a large circle at (bx, by)\n",
    "    circle_radius = 10\n",
    "    circle_color = (0, 0, 255)  # Red color in BGR\n",
    "    image = cv2.circle(image, (bx, by), circle_radius, circle_color, -1)\n",
    "\n",
    "    # Draw circles at the corners of the bounding box\n",
    "    corners = [(bx + width, by), (bx, by + height), (bx + width, by + height)]\n",
    "    for corner in corners:\n",
    "        image = cv2.circle(image, corner, circle_radius, circle_color, -1)\n",
    "\n",
    "    # Add the bx, by text\n",
    "    text_bx_by = f'({bx}, {by})'\n",
    "    org_bx_by = (bx + 15, by + 15)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.6\n",
    "    text_color = (0, 255, 0)  # Green color in BGR\n",
    "    text_thickness = 2\n",
    "    image = cv2.putText(image, text_bx_by, org_bx_by, font, font_scale, text_color, text_thickness, cv2.LINE_AA)\n",
    "\n",
    "    # Add the length text\n",
    "    text_length = f'Length: {length:.2f}'\n",
    "    org_length = (bx+width, by - 10)\n",
    "    image = cv2.putText(image, text_length, org_length, font, font_scale, text_color, text_thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # Save or display the image\n",
    "output_path = f'annot.jpg'  # Adjust the path as needed\n",
    "cv2.imwrite(output_path, image)\n",
    "print(f\"Saved output image to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0: GX010082_212_2827.jpg, bx=3004, by=736, width=5, height=441, length=440\n",
      "Processing row 1: GX010082_212_2827.jpg, bx=3276, by=668, width=5, height=493, length=492\n",
      "Processing row 2: GX010082_212_2827.jpg, bx=3456, by=864, width=537, height=21, length=536\n",
      "Processing row 3: GX010082_212_2827.jpg, bx=3448, by=980, width=589, height=17, length=588\n",
      "Processing row 4: GX010082_212_2827.jpg, bx=3888, by=524, width=225, height=225, length=316\n",
      "Processing row 5: GX010082_212_2827.jpg, bx=3788, by=476, width=205, height=205, length=288\n",
      "Processing row 6: GX010082_212_2827.jpg, bx=2476, by=360, width=401, height=369, length=543\n",
      "Processing row 7: GX010082_212_2827.jpg, bx=2424, by=440, width=357, height=329, length=484\n",
      "Processing row 8: GX010082_212_2827.jpg, bx=3560, by=1238, width=200, height=58, length=207\n",
      "Processing row 9: GX010082_212_2827.jpg, bx=3531, by=1193, width=197, height=63, length=205\n",
      "Processing row 10: GX010082_212_2827.jpg, bx=2818, by=1242, width=160, height=42, length=164\n",
      "Processing row 11: GX010082_212_2827.jpg, bx=2810, by=1291, width=168, height=46, length=172\n",
      "Saved output image to annot2.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import math\n",
    "# Load the CSV file\n",
    "file_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\each video from research extracted to images\\kalkar - january 2024/1.1/65\\Results.csv\"\n",
    "data=pd.read_csv(file_path)\n",
    "# Process each row in the CSV\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\each video from research extracted to images\\kalkar - january 2024/1.1/65\\GX010082_212_2827.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    # Extract the details from the row\n",
    "    label = row['Label']\n",
    "\n",
    "    bx = int(row['BX'])\n",
    "    by = int(row['BY'])\n",
    "    width = int(row['Width'])\n",
    "    height = int(row['Height'])\n",
    "    length = int(row['Length'])\n",
    "    angle = row['Angle']\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Processing row {index}: {label}, bx={bx}, by={by}, width={width}, height={height}, length={length}\")   \n",
    "\n",
    "    if angle>=0 and angle<45:\n",
    "            start_point =     (bx, by + height)\n",
    "            end_point =    (bx + width, by)  \n",
    "    elif angle >= 45 and angle <=90:\n",
    "            start_point =    (bx, by + height)\n",
    "            end_point =  (bx + width, by)       \n",
    "    elif angle > 90 and angle <=135:\n",
    "             start_point = (bx, by)\n",
    "             end_point = ( bx+width,by+height )\n",
    "\n",
    "    elif angle >135 and angle <=180:\n",
    "                start_point = (bx, by)\n",
    "                end_point = ( bx+width,by+height )           \n",
    "\n",
    "    elif angle >-45  and angle < 0:\n",
    "             start_point = (bx, by)\n",
    "             end_point = ( bx+width,by+height  )\n",
    "\n",
    "    elif angle >-90 and angle < -45:\n",
    "            start_point = (bx, by)\n",
    "            end_point = ( bx+width,by+height  )             \n",
    "    elif angle >= -135 and angle < -90:\n",
    "             start_point =(bx, by + height)\n",
    "             end_point =  (bx + width, by)        \n",
    "\n",
    "    else:\n",
    "            start_point =  (bx, by + height)\n",
    "            end_point = (bx + width, by)\n",
    "          \n",
    "\n",
    "    image = cv2.rectangle(image, (bx, by), (bx + width, by + height), color, thickness)\n",
    "\n",
    "\n",
    "    color = (255, 0, 0)  # Blue color in BGR\n",
    "    thickness = 2\n",
    "    image = cv2.line(image, start_point, end_point, color, thickness)\n",
    "\n",
    "    # Draw a large circle at (bx, by)\n",
    "    circle_radius = 10\n",
    "    circle_color = (0, 0, 255)  # Red color in BGR\n",
    "    image = cv2.circle(image, (bx, by), circle_radius, circle_color, -1)\n",
    "\n",
    "    # Draw circles at the corners of the bounding box\n",
    "    corners = [\n",
    "        (bx, by),                      # Top-left\n",
    "        (bx + width, by),              # Top-right\n",
    "        (bx, by + height),             # Bottom-left\n",
    "        (bx + width, by + height)      # Bottom-right\n",
    "        ]\n",
    "\n",
    "# Define colors for each corner (BGR format)\n",
    "    colors = [\n",
    "(255, 0, 0),   # Blue for Top-left\n",
    "(0, 255, 0),   # Green for Top-right\n",
    "(0, 0, 255),   # Red for Bottom-left\n",
    "(0, 255, 255)  # Yellow for Bottom-right\n",
    "]\n",
    "\n",
    "# Circle radius\n",
    "    circle_radius = 10\n",
    "\n",
    "# Draw circles at each corner with different colors\n",
    "    for corner, color in zip(corners, colors):\n",
    "        image = cv2.circle(image, corner, circle_radius, color, -1)\n",
    "\n",
    "    # Add the bx, by text\n",
    "text_bx_by = f'({bx}, {by})'\n",
    "org_bx_by = (bx + 15, by + 15)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.6\n",
    "text_color = (0, 255, 0)  # Green color in BGR\n",
    "text_thickness = 2\n",
    "image = cv2.putText(image, text_bx_by, org_bx_by, font, font_scale, text_color, text_thickness, cv2.LINE_AA)\n",
    "\n",
    "# Add the length text\n",
    "text_length = f'Length: {length:.2f}'\n",
    "org_length = (bx+width, by - 10)\n",
    "image = cv2.putText(image, text_length, org_length, font, font_scale, text_color, text_thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # Save or display the image\n",
    "output_path = f'annot2.jpg'  # Adjust the path as needed\n",
    "cv2.imwrite(output_path, image)\n",
    "print(f\"Saved output image to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the three Excel files into DataFrames\n",
    "file_1_path = \"path_to_first_excel_file.xlsx\"\n",
    "file_2_path = \"path_to_second_excel_file.xlsx\"\n",
    "file_3_path = \"path_to_third_excel_file.xlsx\"\n",
    "\n",
    "data_1 = pd.read_excel(file_1_path)\n",
    "data_2 = pd.read_excel(file_2_path)\n",
    "data_3 = pd.read_excel(file_3_path)\n",
    "\n",
    "# Add a 'File' column to each DataFrame to identify the source file\n",
    "data_1['File'] = 'File 1'\n",
    "data_2['File'] = 'File 2'\n",
    "data_3['File'] = 'File 3'\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_data = pd.concat([data_1, data_2, data_3])\n",
    "\n",
    "# Separate the scale rows (first two rows) from prawn data\n",
    "scales_data = combined_data.groupby('Image_Name').apply(lambda x: x.iloc[:2]).reset_index(drop=True)\n",
    "prawn_data = combined_data.groupby('Image_Name').apply(lambda x: x.iloc[2:]).reset_index(drop=True)\n",
    "\n",
    "# Define a tolerance for bounding box area comparison in the image (percentage tolerance)\n",
    "area_tolerance = 0.1  # 10% tolerance\n",
    "\n",
    "# Calculate the area of the bounding box in the image\n",
    "def calculate_area(row):\n",
    "    return row['Width'] * row['Height']\n",
    "\n",
    "# Function to check if two bounding box areas in the image are within the tolerance\n",
    "def areas_are_within_tolerance(area1, area2, tolerance):\n",
    "    return abs(area1 - area2) <= (tolerance * area1)\n",
    "\n",
    "# Create a unique identifier for each prawn based on the bounding box area in the image with tolerance\n",
    "def create_prawn_id(row, tolerance, prawn_data):\n",
    "    area1 = calculate_area(row)\n",
    "    bx1, by1 = row['BX'], row['BY']\n",
    "    for i, r in prawn_data.iterrows():\n",
    "        if r['Prawn_ID'] is not None:\n",
    "            continue\n",
    "        area2 = calculate_area(r)\n",
    "        bx2, by2 = r['BX'], r['BY']\n",
    "        if areas_are_within_tolerance(area1, area2, tolerance) and (bx1 == bx2 and by1 == by2):\n",
    "            prawn_data.at[i, 'Prawn_ID'] = f\"{row['Image_Name']}_{i}\"\n",
    "            return f\"{row['Image_Name']}_{i}\"\n",
    "    return f\"{row['Image_Name']}_{row.name}\"\n",
    "\n",
    "# Initialize Prawn_ID with None\n",
    "prawn_data['Prawn_ID'] = None\n",
    "\n",
    "# Apply the function to create a unique ID for each prawn based on the bounding box area in the image with tolerance\n",
    "prawn_data['Prawn_ID'] = prawn_data.apply(lambda row: create_prawn_id(row, area_tolerance, prawn_data), axis=1)\n",
    "\n",
    "# Group by the unique identifier (Prawn_ID) to combine measurements of the same prawn within the same image\n",
    "grouped = prawn_data.groupby('Prawn_ID')\n",
    "\n",
    "# Calculate statistics for each group (each prawn within each image)\n",
    "statistics_per_prawn = grouped['Length'].agg(\n",
    "    Avg_Length=np.mean,\n",
    "    Std_Dev=np.std,\n",
    "    Median_Length=np.median,\n",
    "    Measurement_Count='count'\n",
    ")\n",
    "\n",
    "# Calculate the uncertainty (standard deviation of the mean)\n",
    "statistics_per_prawn['Uncertainty'] = statistics_per_prawn['Std_Dev'] / np.sqrt(statistics_per_prawn['Measurement_Count'])\n",
    "\n",
    "# Filter out groups with fewer than 3 measurements (optional)\n",
    "statistics_per_prawn = statistics_per_prawn[statistics_per_prawn['Measurement_Count'] >= 3]\n",
    "\n",
    "# Calculate overall statistics across all prawns in all images\n",
    "overall_statistics = {\n",
    "    \"Overall Avg Length\": np.mean(statistics_per_prawn['Avg_Length']),\n",
    "    \"Overall Std Dev\": np.std(statistics_per_prawn['Avg_Length']),\n",
    "    \"Overall Median Length\": np.median(statistics_per_prawn['Avg_Length']),\n",
    "    \"Overall Uncertainty\": np.std(statistics_per_prawn['Avg_Length']) / np.sqrt(len(statistics_per_prawn))\n",
    "}\n",
    "\n",
    "# Display the scale statistics\n",
    "print(\"Scale Statistics:\")\n",
    "print(scales_data.groupby('Image_Name')['Length'].agg(\n",
    "    Avg_Scale_Length=np.mean,\n",
    "    Std_Scale_Length=np.std,\n",
    "    Median_Scale_Length=np.median\n",
    "))\n",
    "\n",
    "# Display the statistics for each prawn within each image\n",
    "print(\"\\nPrawn Length Statistics:\")\n",
    "print(statistics_per_prawn)\n",
    "\n",
    "# Display overall statistics\n",
    "print(\"\\nOverall Statistics:\")\n",
    "for key, value in overall_statistics.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "# Optionally, save the statistics to a new Excel file\n",
    "statistics_per_prawn.to_excel(\"prawn_length_statistics_per_image.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SN_1', 'Label', 'Area_1', 'Mean_1', 'Min_1', 'Max_1', 'BX_1', 'BY_1',\n",
      "       'Width_1', 'Height_1', 'Angle_1', 'Slice_1', 'Length_1', 'PrawnID',\n",
      "       'SN_2', 'Area_2', 'Mean_2', 'Min_2', 'Max_2', 'BX_2', 'BY_2', 'Width_2',\n",
      "       'Height_2', 'Angle_2', 'Slice_2', 'Length_2', 'SN_3', 'Area_3',\n",
      "       'Mean_3', 'Min_3', 'Max_3', 'BX_3', 'BY_3', 'Width_3', 'Height_3',\n",
      "       'Angle_3', 'Slice_3', 'Length_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the three Excel files into DataFrames\n",
    "file_1_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/1_Carapace.xlsx\"\n",
    "file_2_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/2_Carapace.xlsx\"\n",
    "file_3_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/3_Carapace.xlsx\"\n",
    "\n",
    "data_1 = pd.read_excel(file_1_path)\n",
    "data_2 = pd.read_excel(file_2_path)\n",
    "data_3 = pd.read_excel(file_3_path)\n",
    "\n",
    "# Function to extract the pixel-to-mm scale from the second row of each image\n",
    "def extract_scale(data):\n",
    "    return data.groupby('Label').apply(lambda x: x.iloc[1]['Length'] / 10).reset_index(name='Scale')\n",
    "\n",
    "# Extract scales for each dataset\n",
    "scale_1 = extract_scale(data_1)\n",
    "scale_2 = extract_scale(data_2)\n",
    "scale_3 = extract_scale(data_3)\n",
    "\n",
    "# Function to adjust bounding box coordinates based on the pixel-to-mm scale\n",
    "def adjust_bounding_box(data, scale):\n",
    "    data = data.copy()\n",
    "    prawn_data = data.groupby('Label').apply(lambda x: x.iloc[2:]).reset_index(drop=True)  # Skip the first two rows\n",
    "    prawn_data['BX'] *= scale\n",
    "    prawn_data['BY'] *= scale\n",
    "    prawn_data['Width'] *= scale\n",
    "    prawn_data['Height'] *= scale\n",
    "    return prawn_data\n",
    "\n",
    "# Adjust bounding boxes in each dataset for each image separately\n",
    "def process_image(image_label, data_1, data_2, data_3, scale_1, scale_2, scale_3, tolerance):\n",
    "    img_data_1 = data_1[data_1['Label'] == image_label]\n",
    "    img_data_2 = data_2[data_2['Label'] == image_label]\n",
    "    img_data_3 = data_3[data_3['Label'] == image_label]\n",
    "\n",
    "    scale_val_1 = scale_1[scale_1['Label'] == image_label]['Scale'].values[0]\n",
    "    scale_val_2 = scale_2[scale_2['Label'] == image_label]['Scale'].values[0]\n",
    "    scale_val_3 = scale_3[scale_3['Label'] == image_label]['Scale'].values[0]\n",
    "\n",
    "    prawn_data_1 = adjust_bounding_box(img_data_1, scale_val_1)\n",
    "    prawn_data_2 = adjust_bounding_box(img_data_2, scale_val_2)\n",
    "    prawn_data_3 = adjust_bounding_box(img_data_3, scale_val_3)\n",
    "\n",
    "    # Assign PrawnID by comparing bounding boxes with tolerance within the same image\n",
    "    prawn_data_1['PrawnID'] = None\n",
    "    prawn_data_2['PrawnID'] = None\n",
    "    prawn_data_3['PrawnID'] = None\n",
    "\n",
    "    for i, row1 in prawn_data_1.iterrows():\n",
    "        bx1, by1, width1, height1 = row1['BX'], row1['BY'], row1['Width'], row1['Height']\n",
    "        area1 = width1 * height1\n",
    "\n",
    "        for j, row2 in prawn_data_2.iterrows():\n",
    "            bx2, by2, width2, height2 = row2['BX'], row2['BY'], row2['Width'], row2['Height']\n",
    "            area2 = width2 * height2\n",
    "\n",
    "            if abs(area1 - area2) <= (tolerance * area1) and abs(bx1 - bx2) <= tolerance and abs(by1 - by2) <= tolerance:\n",
    "                prawn_data_1.at[i, 'PrawnID'] = f\"{image_label}_{i}\"\n",
    "                prawn_data_2.at[j, 'PrawnID'] = f\"{image_label}_{i}\"\n",
    "                break\n",
    "\n",
    "        for k, row3 in prawn_data_3.iterrows():\n",
    "            bx3, by3, width3, height3 = row3['BX'], row3['BY'], row3['Width'], row3['Height']\n",
    "            area3 = width3 * height3\n",
    "\n",
    "            if abs(area1 - area3) <= (tolerance * area1) and abs(bx1 - bx3) <= tolerance and abs(by1 - by3) <= tolerance:\n",
    "                prawn_data_1.at[i, 'PrawnID'] = f\"{image_label}_{i}\"\n",
    "                prawn_data_3.at[k, 'PrawnID'] = f\"{image_label}_{i}\"\n",
    "                break\n",
    "\n",
    "    return prawn_data_1, prawn_data_2, prawn_data_3\n",
    "\n",
    "# Set tolerance for bounding box comparison\n",
    "tolerance = 15 # Adjust based on acceptable variance\n",
    "\n",
    "# Process each image separately\n",
    "processed_data_1 = []\n",
    "processed_data_2 = []\n",
    "processed_data_3 = []\n",
    "\n",
    "for image_label in data_1['Label'].unique():\n",
    "    img_data_1, img_data_2, img_data_3 = process_image(image_label, data_1, data_2, data_3, scale_1, scale_2, scale_3, tolerance)\n",
    "    processed_data_1.append(img_data_1)\n",
    "    processed_data_2.append(img_data_2)\n",
    "    processed_data_3.append(img_data_3)\n",
    "\n",
    "# Combine the processed data into DataFrames\n",
    "final_data_1 = pd.concat(processed_data_1, ignore_index=True)\n",
    "final_data_2 = pd.concat(processed_data_2, ignore_index=True)\n",
    "final_data_3 = pd.concat(processed_data_3, ignore_index=True)\n",
    "\n",
    "# Merge datasets on PrawnID and Label to combine the measurements across the three files\n",
    "# Merge datasets on PrawnID and Label to combine the measurements across the three files\n",
    "merged_data = final_data_1.merge(final_data_2, on=['Label', 'PrawnID'], suffixes=('_1', '_2'))\n",
    "\n",
    "# Rename the columns in final_data_3 to ensure they have a unique suffix\n",
    "final_data_3 = final_data_3.rename(columns={\n",
    "    'SN': 'SN_3',\n",
    "    'Area': 'Area_3',\n",
    "    'Mean': 'Mean_3',\n",
    "    'Min': 'Min_3',\n",
    "    'Max': 'Max_3',\n",
    "    'BX': 'BX_3',\n",
    "    'BY': 'BY_3',\n",
    "    'Width': 'Width_3',\n",
    "    'Height': 'Height_3',\n",
    "    'Angle': 'Angle_3',\n",
    "    'Slice': 'Slice_3',\n",
    "    'Length': 'Length_3'\n",
    "})\n",
    "\n",
    "\n",
    "merged_data = merged_data.merge(final_data_3, on=['Label', 'PrawnID'])\n",
    "# Check if the length columns are present\n",
    "print(merged_data.columns)\n",
    "\n",
    "\n",
    "# Calculate statistics for each prawn\n",
    "def calculate_statistics(merged_data):\n",
    "    # Calculate mean, standard deviation, and uncertainty\n",
    "    merged_data['Mean_Length'] = merged_data[['Length_1', 'Length_2', 'Length_3']].mean(axis=1)\n",
    "    merged_data['Std_Dev_Length'] = merged_data[['Length_1', 'Length_2', 'Length_3']].std(axis=1)\n",
    "    merged_data['Uncertainty'] = merged_data['Std_Dev_Length'] / (3 ** 0.5)\n",
    "    return merged_data\n",
    "\n",
    "# Apply the statistics calculation\n",
    "final_data = calculate_statistics(merged_data)\n",
    "\n",
    "# Concat the bounding box and angle information for verification\n",
    "final_data['BoundingBox_1'] = final_data[['BX_1', 'BY_1', 'Width_1', 'Height_1']].apply(tuple, axis=1)\n",
    "final_data['BoundingBox_2'] = final_data[['BX_2', 'BY_2', 'Width_2', 'Height_2']].apply(tuple, axis=1)\n",
    "final_data['BoundingBox_3'] = final_data[['BX_3', 'BY_3', 'Width_3', 'Height_3']].apply(tuple, axis=1)\n",
    "\n",
    "final_data['Angle_1'] = final_data['Angle_1']\n",
    "final_data['Angle_2'] = final_data['Angle_2']\n",
    "final_data['Angle_3'] = final_data['Angle_3']\n",
    "\n",
    "# Display the final dataframe with PrawnID, calculated statistics, bounding boxes, and angles\n",
    "final_data[['Label', 'PrawnID', 'Mean_Length', 'Std_Dev_Length', 'Uncertainty', \n",
    "            'BoundingBox_1', 'BoundingBox_2', 'BoundingBox_3', \n",
    "            'Angle_1', 'Angle_2', 'Angle_3']]\n",
    "#save the final data to a new Excel file\n",
    "final_data.to_excel(\"final_data.xlsx\", index=False)\n",
    "# Display the final dataframe with PrawnID, calculated statistics, bounding boxes, and angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspect visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning ID for prawn 1 in image carapace:undistorted_GX010067_33_625.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/carapace/undistorted_GX010067_33_625.jpg_gamma.jpg\n",
      "Assigning ID for prawn 2 in image carapace:undistorted_GX010067_33_625.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/carapace/undistorted_GX010067_33_625.jpg_gamma.jpg\n",
      "Assigning ID for prawn 3 in image carapace:undistorted_GX010067_33_625.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/carapace/undistorted_GX010067_33_625.jpg_gamma.jpg\n",
      "Assigning ID for prawn 4 in image carapace:undistorted_GX010067_33_625.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/carapace/undistorted_GX010067_33_625.jpg_gamma.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "# Load the three Excel files into DataFrames\n",
    "file_1_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/1_Carapace.xlsx\"\n",
    "file_2_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/2_Carapace.xlsx\"\n",
    "file_3_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/3_Carapace.xlsx\"\n",
    "\n",
    "data_1 = pd.read_excel(file_1_path)\n",
    "data_2 = pd.read_excel(file_2_path)\n",
    "data_3 = pd.read_excel(file_3_path)\n",
    "\n",
    "# Function to extract the pixel-to-mm scale from the second row of each image\n",
    "def extract_scale(data):\n",
    "    return data.groupby('Label').apply(lambda x: x.iloc[1]['Length'] / 10).reset_index(name='Scale')\n",
    "\n",
    "# Extract scales for each dataset\n",
    "scale_1 = extract_scale(data_1)\n",
    "scale_2 = extract_scale(data_2)\n",
    "scale_3 = extract_scale(data_3)\n",
    "\n",
    "# Function to adjust bounding box coordinates based on the pixel-to-mm scale\n",
    "def adjust_bounding_box(data, scale):\n",
    "    data = data.copy()\n",
    "    data['BX'] *= scale\n",
    "    data['BY'] *= scale\n",
    "    data['Width'] *= scale\n",
    "    data['Height'] *= scale\n",
    "    return data\n",
    "\n",
    "# Function to draw bounding boxes and lines\n",
    "def draw_bounding_boxes_and_lines(image, prawn_data, color, thickness=2):\n",
    "    bx = int(prawn_data['BX'])\n",
    "    by = int(prawn_data['BY'])\n",
    "    width = int(prawn_data['Width'])\n",
    "    height = int(prawn_data['Height'])\n",
    "    angle = prawn_data['Angle']\n",
    "\n",
    "    if angle >= 0 and angle < 45 or angle >= 135 and angle <= 180:\n",
    "        start_point = (bx, by + height)\n",
    "        end_point = (bx + width, by)\n",
    "    else:\n",
    "        start_point = (bx, by)\n",
    "        end_point = (bx + width, by + height)\n",
    "\n",
    "    # Draw the rectangle (bounding box)\n",
    "    image = cv2.rectangle(image, (bx, by), (bx + width, by + height), color, thickness)\n",
    "    \n",
    "    # Draw the line\n",
    "    image = cv2.line(image, start_point, end_point, color, thickness)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Function to find the closest matching bounding box based on location\n",
    "def find_closest_bbox(bbox, prawn_data):\n",
    "    \"\"\"Finds the closest matching bounding box in prawn_data to the given bbox.\"\"\"\n",
    "    bx1, by1, width1, height1 = bbox\n",
    "\n",
    "    def distance(row):\n",
    "        bx2, by2, width2, height2 = row['BX'], row['BY'], row['Width'], row['Height']\n",
    "        return np.sqrt((bx1 - bx2)**2 + (by1 - by2)**2)\n",
    "\n",
    "    closest_row = prawn_data.iloc[prawn_data.apply(distance, axis=1).argmin()]\n",
    "    return closest_row\n",
    "\n",
    "# Function to manually assign PrawnIDs by visual inspection\n",
    "def assign_prawn_ids_manually(image_label, data_1, data_2, data_3, scale_1, scale_2, scale_3):\n",
    "    img_data_1 = data_1[data_1['Label'] == image_label]\n",
    "    img_data_2 = data_2[data_2['Label'] == image_label]\n",
    "    img_data_3 = data_3[data_3['Label'] == image_label]\n",
    "\n",
    "    scale_val_1 = scale_1[scale_1['Label'] == image_label]['Scale'].values[0]\n",
    "    scale_val_2 = scale_2[scale_2['Label'] == image_label]['Scale'].values[0]\n",
    "    scale_val_3 = scale_3[scale_3['Label'] == image_label]['Scale'].values[0]\n",
    "\n",
    "    # Adjust the bounding boxes according to the scale\n",
    "    img_data_1 = adjust_bounding_box(img_data_1, scale_val_1)\n",
    "    img_data_2 = adjust_bounding_box(img_data_2, scale_val_2)\n",
    "    img_data_3 = adjust_bounding_box(img_data_3, scale_val_3)\n",
    "\n",
    "    prawn_ids = []\n",
    "    prawn_id_counter =0\n",
    "\n",
    "    for i, row1 in img_data_1.iloc[2:].iterrows():\n",
    "        print(f\"Assigning ID for prawn {prawn_id_counter+1} in image {image_label}\")\n",
    "\n",
    "\n",
    "        file_name = image_label.split(\":\")[1] if \":\" in image_label else image_label\n",
    "\n",
    "        file_root, file_ext = os.path.splitext(file_name)\n",
    "      \n",
    "    # Append a .jpg extension if it’s missing\n",
    "      \n",
    "\n",
    "        file_name = f\"{file_root}{file_ext}.jpg\"\n",
    "\n",
    "       # Load the image (assuming you have a way to load it based on the label)\n",
    "        image_path = f\"C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/carapace/{file_name}\"  \n",
    "        \n",
    "        print(image_path)   \n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Draw bounding boxes and lines from dataset 1\n",
    "        image = draw_bounding_boxes_and_lines(image, row1, (255, 0, 0))  # Blue for file 1\n",
    "        \n",
    "        # Find the closest matching bounding boxes in the other datasets\n",
    "        closest_bbox_2 = find_closest_bbox((row1['BX'], row1['BY'], row1['Width'], row1['Height']), img_data_2)\n",
    "        closest_bbox_3 = find_closest_bbox((row1['BX'], row1['BY'], row1['Width'], row1['Height']), img_data_3)\n",
    "\n",
    "        # Draw bounding boxes and lines from dataset 2 and 3\n",
    "        image = draw_bounding_boxes_and_lines(image, closest_bbox_2, (0, 255, 0))  # Green for file 2\n",
    "        image = draw_bounding_boxes_and_lines(image, closest_bbox_3, (0, 0, 255))  # Red for file 3\n",
    "\n",
    "\n",
    "        # Display the image using Matplotlib\n",
    "    \n",
    "\n",
    "        # Manually input the ID\n",
    "        prawn_id = f\"Prawn_{prawn_id_counter}\"\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        prawn_id_counter += 1\n",
    "     \n",
    "\n",
    "        # Assign PrawnID to closest matches in all datasets\n",
    "        img_data_1.at[i, 'PrawnID'] = prawn_id\n",
    "        img_data_2.at[closest_bbox_2.name, 'PrawnID'] = prawn_id\n",
    "        img_data_3.at[closest_bbox_3.name, 'PrawnID'] = prawn_id\n",
    "\n",
    "    return img_data_1, img_data_2, img_data_3\n",
    "\n",
    "# Process each image and assign PrawnIDs manually\n",
    "processed_data_1 = []\n",
    "processed_data_2 = []\n",
    "processed_data_3 = []\n",
    "\n",
    "for image_label in data_1['Label'].unique():\n",
    "    img_data_1, img_data_2, img_data_3 = assign_prawn_ids_manually(image_label, data_1, data_2, data_3, scale_1, scale_2, scale_3)\n",
    "    processed_data_1.append(img_data_1)\n",
    "    processed_data_2.append(img_data_2)\n",
    "    processed_data_3.append(img_data_3)\n",
    "\n",
    "# Combine the processed data into DataFrames\n",
    "final_data_1 = pd.concat(processed_data_1, ignore_index=True)\n",
    "final_data_2 = pd.concat(processed_data_2, ignore_index=True)\n",
    "final_data_3 = pd.concat(processed_data_3, ignore_index=True)\n",
    "\n",
    "# Save the final data with manually assigned PrawnIDs\n",
    "final_data_1.to_excel(\"final_data_1_with_prawn_ids.xlsx\", index=False)\n",
    "final_data_2.to_excel(\"final_data_2_with_prawn_ids.xlsx\", index=False)\n",
    "final_data_3.to_excel(\"final_data_3_with_prawn_ids.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10/59 [00:00<00:00, 89.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 47/59 [00:00<00:00, 116.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:00<00:00, 106.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the three Excel files into DataFrames\n",
    "file_1_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/1_Full_body.xlsx\"\n",
    "file_2_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/2_Full_body.xlsx\"\n",
    "file_3_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\compile 3 files/3_Full_body.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "data_1 = pd.read_excel(file_1_path)\n",
    "data_2 = pd.read_excel(file_2_path)\n",
    "data_3 = pd.read_excel(file_3_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract the pixel-to-mm scale from the second row of each image\n",
    "def extract_scale(data):\n",
    "    return data.groupby('Label').apply(lambda x: x.iloc[1]['Length'] / 10).reset_index(name='Scale')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract scales for each dataset\n",
    "scale_1 = extract_scale(data_1)\n",
    "scale_2 = extract_scale(data_2)\n",
    "scale_3 = extract_scale(data_3)\n",
    "\n",
    "# Function to adjust bounding box coordinates based on the pixel-to-mm scale\n",
    "def adjust_bounding_box(data, scale):\n",
    "    data = data.copy()\n",
    "    data['BX'] *= scale\n",
    "    data['BY'] *= scale\n",
    "    data['Width'] *= scale\n",
    "    data['Height'] *= scale\n",
    "    return data\n",
    "\n",
    "# Function to draw bounding boxes and lines\n",
    "def draw_bounding_boxes_and_lines(image, prawn_data, color, thickness=2):\n",
    "    bx = int(prawn_data['BX'])\n",
    "    by = int(prawn_data['BY'])\n",
    "    width = int(prawn_data['Width'])\n",
    "    height = int(prawn_data['Height'])\n",
    "    angle = prawn_data['Angle']\n",
    "\n",
    "    if angle >= 0 and angle < 45 or angle >= 135 and angle <= 180:\n",
    "        start_point = (bx, by + height)\n",
    "        end_point = (bx + width, by)\n",
    "    else:\n",
    "        start_point = (bx, by)\n",
    "        end_point = (bx + width, by + height)\n",
    "\n",
    "    # Draw the rectangle (bounding box)\n",
    "    image = cv2.rectangle(image, (bx, by), (bx + width, by + height), color, thickness)\n",
    "    \n",
    "    # Draw the line\n",
    "    image = cv2.line(image, start_point, end_point, color, thickness)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Function to find the closest matching bounding box based on location\n",
    "def find_closest_bbox(bbox, prawn_data):\n",
    "    \"\"\"Finds the closest matching bounding box in prawn_data to the given bbox, ignoring the first two rows.\"\"\"\n",
    "    bx1, by1, width1, height1 = bbox\n",
    "\n",
    "    # Ignore the first two rows (scale and pixel reference)\n",
    "    prawn_data_filtered = prawn_data.iloc[2:]\n",
    "\n",
    "    def distance(row):\n",
    "        bx2, by2, width2, height2 = row['BX'], row['BY'], row['Width'], row['Height']\n",
    "        return np.sqrt((bx1 - bx2)**2 + (by1 - by2)**2)\n",
    "\n",
    "    closest_row = prawn_data_filtered.iloc[prawn_data_filtered.apply(distance, axis=1).argmin()]\n",
    "    return closest_row\n",
    "\n",
    "# Function to manually assign PrawnIDs by visual inspection\n",
    "def assign_prawn_ids_manually(image_label, data_1, data_2, data_3, scale_1, scale_2, scale_3):\n",
    "    img_data_1 = data_1[data_1['Label'] == image_label]\n",
    "    img_data_2 = data_2[data_2['Label'] == image_label]\n",
    "    img_data_3 = data_3[data_3['Label'] == image_label]\n",
    "\n",
    "    scale_val_1 = scale_1[scale_1['Label'] == image_label]['Scale'].values[0]\n",
    "    scale_val_2 = scale_2[scale_2['Label'] == image_label]['Scale'].values[0]\n",
    "    scale_val_3 = scale_3[scale_3['Label'] == image_label]['Scale'].values[0]\n",
    "\n",
    "    # Adjust the bounding boxes according to the scale\n",
    "    img_data_1 = adjust_bounding_box(img_data_1, scale_val_1)\n",
    "    img_data_2 = adjust_bounding_box(img_data_2, scale_val_2)\n",
    "    img_data_3 = adjust_bounding_box(img_data_3, scale_val_3)\n",
    "\n",
    "    prawn_ids = []\n",
    "    prawn_id_counter =0\n",
    "\n",
    "    for i, row1 in img_data_1.iloc[2:].iterrows():\n",
    "        \n",
    "      \n",
    "     # Blue for file 1\n",
    "        \n",
    "        # Find the closest matching bounding boxes in the other datasets\n",
    "        closest_bbox_2 = find_closest_bbox((row1['BX'], row1['BY'], row1['Width'], row1['Height']), img_data_2)\n",
    "        closest_bbox_3 = find_closest_bbox((row1['BX'], row1['BY'], row1['Width'], row1['Height']), img_data_3)\n",
    "\n",
    "        # Draw bounding boxes and lines from dataset 2 and 3\n",
    "       \n",
    "\n",
    "\n",
    "        # Display the image using Matplotlib\n",
    "    \n",
    "\n",
    "        # Manually input the ID\n",
    "        prawn_id = f\"Prawn_{prawn_id_counter}\"\n",
    "        print(prawn_id_counter)     \n",
    "\n",
    "\n",
    "        prawn_id_counter += 1\n",
    "     \n",
    "\n",
    "        # Assign PrawnID to closest matches in all datasets\n",
    "        img_data_1.at[i, 'PrawnID'] = prawn_id\n",
    "        img_data_2.at[closest_bbox_2.name, 'PrawnID'] = prawn_id\n",
    "        img_data_3.at[closest_bbox_3.name, 'PrawnID'] = prawn_id\n",
    "\n",
    "    return img_data_1, img_data_2, img_data_3\n",
    "\n",
    "# Process each image and assign PrawnIDs manually\n",
    "processed_data_1 = []\n",
    "processed_data_2 = []\n",
    "processed_data_3 = []\n",
    "\n",
    "for image_label in tqdm(data_1['Label'].unique()):\n",
    "    img_data_1, img_data_2, img_data_3 = assign_prawn_ids_manually(image_label, data_1, data_2, data_3, scale_1, scale_2, scale_3)\n",
    "    processed_data_1.append(img_data_1)\n",
    "    processed_data_2.append(img_data_2)\n",
    "    processed_data_3.append(img_data_3)\n",
    "\n",
    "# Combine the processed data into DataFrames\n",
    "final_data_1 = pd.concat(processed_data_1, ignore_index=True)\n",
    "final_data_2 = pd.concat(processed_data_2, ignore_index=True)\n",
    "final_data_3 = pd.concat(processed_data_3, ignore_index=True)\n",
    "\n",
    "# Save the final data with manually assigned PrawnIDs\n",
    "final_data_1.to_excel(\"final_full_data_1_with_prawn_ids.xlsx\", index=False)\n",
    "final_data_2.to_excel(\"final_full_data_2_with_prawn_ids.xlsx\", index=False)\n",
    "final_data_3.to_excel(\"final_full_data_3_with_prawn_ids.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image full body:'\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/'.jpg\n",
      "Saved visualization to output_images_2\\output_'.png\n",
      "Processing image full body:undistorted_GX010067_33_625.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010067_33_625.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010067_33_625.png\n",
      "Processing image full body:undistorted_GX010068_26_666.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010068_26_666.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010068_26_666.png\n",
      "Processing image full body:undistorted_GX010068_27_795.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010068_27_795.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010068_27_795.png\n",
      "Processing image full body:undistorted_GX010069_19_191.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010069_19_191.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010069_19_191.png\n",
      "Processing image full body:undistorted_GX010071_22_444.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010071_22_444.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010071_22_444.png\n",
      "Processing image full body:undistorted_GX010073_42_695.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010073_42_695.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010073_42_695.png\n",
      "Processing image full body:undistorted_GX010073_55_1014.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010073_55_1014.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010073_55_1014.png\n",
      "Processing image full body:undistorted_GX010080_157_2283.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010080_157_2283.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010080_157_2283.png\n",
      "Processing image full body:undistorted_GX010080_193_2640.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010080_193_2640.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010080_193_2640.png\n",
      "Processing image full body:undistorted_GX010080_212_2954.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010080_212_2954.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010080_212_2954.png\n",
      "Processing image full body:undistorted_GX010080_237_3366.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010080_237_3366.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010080_237_3366.png\n",
      "Processing image full body:undistorted_GX010080_238_3392.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010080_238_3392.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010080_238_3392.png\n",
      "Processing image full body:undistorted_GX010082_8_59.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010082_8_59.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010082_8_59.png\n",
      "Processing image full body:undistorted_GX010082_53_796.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010082_53_796.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010082_53_796.png\n",
      "Processing image full body:undistorted_GX010082_65_1070.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010082_65_1070.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010082_65_1070.png\n",
      "Processing image full body:undistorted_GX010082_236_3137.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010082_236_3137.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010082_236_3137.png\n",
      "Processing image full body:undistorted_GX010084_30_344.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010084_30_344.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010084_30_344.png\n",
      "Processing image full body:undistorted_GX010084_130_1748.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010084_130_1748.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010084_130_1748.png\n",
      "Processing image full body:undistorted_GX010085_261_3610.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010085_261_3610.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010085_261_3610.png\n",
      "Processing image full body:undistorted_GX010088_209_2904.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010088_209_2904.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010088_209_2904.png\n",
      "Processing image full body:undistorted_GX010088_262_3712.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010088_262_3712.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010088_262_3712.png\n",
      "Processing image full body:undistorted_GX010088_327_4364.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010088_327_4364.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010088_327_4364.png\n",
      "Processing image full body:undistorted_GX010089_11_236.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010089_11_236.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010089_11_236.png\n",
      "Processing image full body:undistorted_GX010090_132_1852.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010090_132_1852.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010090_132_1852.png\n",
      "Processing image full body:undistorted_GX010091_5_149.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010091_5_149.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010091_5_149.png\n",
      "Processing image full body:undistorted_GX010094_9_129.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010094_9_129.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010094_9_129.png\n",
      "Processing image full body:undistorted_GX010094_24_430.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010094_24_430.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010094_24_430.png\n",
      "Processing image full body:undistorted_GX010094_93_1258.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010094_93_1258.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010094_93_1258.png\n",
      "Processing image full body:undistorted_GX010094_163_2312.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010094_163_2312.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010094_163_2312.png\n",
      "Processing image full body:undistorted_GX010097_32_528.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010097_32_528.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010097_32_528.png\n",
      "Processing image full body:undistorted_GX010097_60_1080.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010097_60_1080.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010097_60_1080.png\n",
      "Processing image full body:undistorted_GX010097_105_1651.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010097_105_1651.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010097_105_1651.png\n",
      "Processing image full body:undistorted_GX010097_124_1870.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010097_124_1870.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010097_124_1870.png\n",
      "Processing image full body:undistorted_GX010101_12_173.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010101_12_173.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010101_12_173.png\n",
      "Processing image full body:undistorted_GX010102_9_165.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010102_9_165.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010102_9_165.png\n",
      "Processing image full body:undistorted_GX010102_50_1396.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010102_50_1396.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010102_50_1396.png\n",
      "Processing image full body:undistorted_GX010105_28_816.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010105_28_816.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010105_28_816.png\n",
      "Processing image full body:undistorted_GX010152_36_378.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010152_36_378.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010152_36_378.png\n",
      "Processing image full body:undistorted_GX010156_53_629.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010156_53_629.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010156_53_629.png\n",
      "Processing image full body:undistorted_GX010157_160_2259\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010157_160_2259.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010157_160_2259.png\n",
      "Processing image full body:undistorted_GX010157_173_2554.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010157_173_2554.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010157_173_2554.png\n",
      "Processing image full body:undistorted_GX010157_174_2582.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010157_174_2582.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010157_174_2582.png\n",
      "Processing image full body:undistorted_GX010157_177_2665.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010157_177_2665.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010157_177_2665.png\n",
      "Processing image full body:undistorted_GX010157_272_4575.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010157_272_4575.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010157_272_4575.png\n",
      "Processing image full body:undistorted_GX010157_293_5205.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010157_293_5205.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010157_293_5205.png\n",
      "Processing image full body:undistorted_GX010161_54_676.jpg_gamma.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010161_54_676.jpg_gamma.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010161_54_676.jpg_gamma.png\n",
      "Processing image full body:undistorted_GX010162_72_927.jpg_gamma.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010162_72_927.jpg_gamma.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010162_72_927.jpg_gamma.png\n",
      "Processing image full body:undistorted_GX010162_153_2230.jpg_gamma.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010162_153_2230.jpg_gamma.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010162_153_2230.jpg_gamma.png\n",
      "Processing image full body:undistorted_GX010162_163_2433.jpg_gamma.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010162_163_2433.jpg_gamma.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010162_163_2433.jpg_gamma.png\n",
      "Processing image full body:undistorted_GX010162_250_4175.jpg_gamma.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010162_250_4175.jpg_gamma.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010162_250_4175.jpg_gamma.png\n",
      "Processing image full body:undistorted_GX010171_115_1483.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010171_115_1483.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010171_115_1483.png\n",
      "Processing image full body:undistorted_GX010174_62_790.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010174_62_790.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010174_62_790.png\n",
      "Processing image full body:undistorted_GX010175_266_3372.jpg_gamma.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010175_266_3372.jpg_gamma.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010175_266_3372.jpg_gamma.png\n",
      "Processing image full body:undistorted_GX010177_232_3047\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010177_232_3047.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010177_232_3047.png\n",
      "Processing image full body:undistorted_GX010178_172_3604.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010178_172_3604.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010178_172_3604.png\n",
      "Processing image full body:undistorted_GX010183_37_685.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010183_37_685.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010183_37_685.png\n",
      "Processing image full body:undistorted_GX010183_80_1633.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010183_80_1633.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010183_80_1633.png\n",
      "Processing image full body:undistorted_GX010183_128_2852.jpg_gamma\n",
      "C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/undistorted_GX010183_128_2852.jpg_gamma.jpg\n",
      "Saved visualization to output_images_2\\output_undistorted_GX010183_128_2852.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the processed data from the Excel files\n",
    "final_data_1 = pd.read_excel(\"final_full_data_1_with_prawn_ids.xlsx\")\n",
    "final_data_2 = pd.read_excel(\"final_full_data_2_with_prawn_ids.xlsx\")\n",
    "final_data_3 = pd.read_excel(\"final_full_data_3_with_prawn_ids.xlsx\")\n",
    "\n",
    "# Function to draw bounding boxes with PrawnIDs on an image\n",
    "def draw_bounding_boxes_with_ids(image, prawn_data, color):\n",
    "    for _, row in prawn_data.iloc[2:].iterrows():\n",
    "        bx, by, width, height = int(row['BX']), int(row['BY']), int(row['Width']), int(row['Height'])\n",
    "        prawn_id = str(row['PrawnID'])\n",
    "        \n",
    "        cv2.rectangle(image, (bx, by), (bx + width, by + height), color, 2)\n",
    "        cv2.putText(image, prawn_id, (bx, by - 10), cv2.FONT_HERSHEY_SIMPLEX, 3, color, 2)\n",
    "    return image\n",
    "\n",
    "# Iterate through each unique image label\n",
    "for image_label in final_data_1['Label'].unique():\n",
    "    print(f\"Processing image {image_label}\")\n",
    "    \n",
    "    # Extract relevant data for the current image\n",
    "    img_data_1 = final_data_1[final_data_1['Label'] == image_label]\n",
    "    img_data_2 = final_data_2[final_data_2['Label'] == image_label]\n",
    "    img_data_3 = final_data_3[final_data_3['Label'] == image_label]\n",
    "\n",
    "    # Extract the file name from the label (remove prefix)\n",
    "    file_name = image_label.split(\":\")[1] if \":\" in image_label else image_label\n",
    "\n",
    "    file_root, file_ext = os.path.splitext(file_name)\n",
    "      \n",
    "    # Append a .jpg extension if it’s missing\n",
    "    file_name = f\"{file_root}{file_ext}.jpg\"\n",
    "\n",
    "    # Load the image (assuming you have a way to load it based on the label)\n",
    "    image_path = f\"C:/Users/gbo10/OneDrive/measurement_paper_images/images used for imageJ/check/stabilized/shai/measurements/1/full body/{file_name}\"  \n",
    "    \n",
    "    print(image_path)   \n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Verify if the image was successfully loaded\n",
    "    if image is None or image.size == 0:\n",
    "        print(f\"Error: Could not load image from {image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Draw bounding boxes with PrawnIDs from all three datasets\n",
    "    image = draw_bounding_boxes_with_ids(image, img_data_1, (255, 0, 0))  # Blue for dataset 1\n",
    "    image = draw_bounding_boxes_with_ids(image, img_data_2, (0, 255, 0))  # Green for dataset 2\n",
    "    image = draw_bounding_boxes_with_ids(image, img_data_3, (0, 0, 255))  # Red for dataset 3\n",
    "\n",
    "    # Save the image with bounding boxes\n",
    "    output_dir = \"output_images_2\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    output_path = os.path.join(output_dir, f\"output_{file_root}.png\")\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Saved visualization to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in merged_data:\n",
      "Index(['SN_1', 'Label', 'Area_1', 'Mean_1', 'Min_1', 'Max_1', 'BX_1', 'BY_1',\n",
      "       'Width_1', 'Height_1', 'Angle_1', 'Slice_1', 'Length_1', 'PrawnID',\n",
      "       'SN_2', 'Area_2', 'Mean_2', 'Min_2', 'Max_2', 'BX_2', 'BY_2', 'Width_2',\n",
      "       'Height_2', 'Angle_2', 'Slice_2', 'Length_2', 'SN_3', 'Area_3',\n",
      "       'Mean_3', 'Min_3', 'Max_3', 'BX_3', 'BY_3', 'Width_3', 'Height_3',\n",
      "       'Angle_3', 'Slice_3', 'Length_3'],\n",
      "      dtype='object')\n",
      "\n",
      "Sample of merged_data:\n",
      "   SN_1                                           Label  Area_1   Mean_1  \\\n",
      "0     1  carapace:undistorted_GX010067_33_625.jpg_gamma    1.81  212.428   \n",
      "1     1  carapace:undistorted_GX010067_33_625.jpg_gamma    1.81  212.428   \n",
      "2     1  carapace:undistorted_GX010067_33_625.jpg_gamma    1.81  212.428   \n",
      "3     1  carapace:undistorted_GX010067_33_625.jpg_gamma    1.81  212.428   \n",
      "4     2  carapace:undistorted_GX010067_33_625.jpg_gamma    1.81  212.428   \n",
      "\n",
      "     Min_1    Max_1         BX_1         BY_1    Width_1   Height_1  ...  \\\n",
      "0  195.333  215.333  3022.013263  1890.008021  54.999529  17.997273  ...   \n",
      "1  195.333  215.333  3022.013263  1890.008021  54.999529  17.997273  ...   \n",
      "2  195.333  215.333  3022.013263  1890.008021  54.999529  17.997273  ...   \n",
      "3  195.333  215.333  3022.013263  1890.008021  54.999529  17.997273  ...   \n",
      "4  195.333  215.333  3022.013263  1890.008021  54.999529  17.997273  ...   \n",
      "\n",
      "    Mean_3    Min_3    Max_3         BX_3         BY_3    Width_3   Height_3  \\\n",
      "0  209.528  188.667  214.333  2527.018234  1073.009359  18.999049  56.001767   \n",
      "1  209.528  188.667  214.333  2527.018234  1073.009359  18.999049  56.001767   \n",
      "2  209.528  188.667  214.333  2527.018234  1073.009359  18.999049  56.001767   \n",
      "3  209.528  188.667  214.333  2527.018234  1073.009359  18.999049  56.001767   \n",
      "4  209.528  188.667  214.333  2527.018234  1073.009359  18.999049  56.001767   \n",
      "\n",
      "   Angle_3  Slice_3  Length_3  \n",
      "0 -108.122        1    10.000  \n",
      "1 -108.122        1    57.871  \n",
      "2 -108.122        1    10.000  \n",
      "3 -108.122        1    57.871  \n",
      "4 -108.122        1    10.000  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "\n",
      "Valid PrawnIDs (appear exactly 3 times for each Label):\n",
      "Series([], Name: PrawnID, dtype: object)\n",
      "\n",
      "Filtered data:\n",
      "Empty DataFrame\n",
      "Columns: [SN_1, Label, Area_1, Mean_1, Min_1, Max_1, BX_1, BY_1, Width_1, Height_1, Angle_1, Slice_1, Length_1, PrawnID, SN_2, Area_2, Mean_2, Min_2, Max_2, BX_2, BY_2, Width_2, Height_2, Angle_2, Slice_2, Length_2, SN_3, Area_3, Mean_3, Min_3, Max_3, BX_3, BY_3, Width_3, Height_3, Angle_3, Slice_3, Length_3]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 38 columns]\n",
      "Warning: No PrawnIDs found that appear exactly 3 times for each Label.\n",
      "\n",
      "Columns in final_statistics:\n",
      "Index(['SN_1', 'Label', 'Area_1', 'Mean_1', 'Min_1', 'Max_1', 'BX_1', 'BY_1',\n",
      "       'Width_1', 'Height_1', 'Angle_1', 'Slice_1', 'Length_1', 'PrawnID',\n",
      "       'SN_2', 'Area_2', 'Mean_2', 'Min_2', 'Max_2', 'BX_2', 'BY_2', 'Width_2',\n",
      "       'Height_2', 'Angle_2', 'Slice_2', 'Length_2', 'SN_3', 'Area_3',\n",
      "       'Mean_3', 'Min_3', 'Max_3', 'BX_3', 'BY_3', 'Width_3', 'Height_3',\n",
      "       'Angle_3', 'Slice_3', 'Length_3'],\n",
      "      dtype='object')\n",
      "\n",
      "Statistics for length columns:\n",
      "Length_1:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: Length_1, dtype: float64\n",
      "\n",
      "Length_2:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: Length_2, dtype: float64\n",
      "\n",
      "Length_3:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: Length_3, dtype: float64\n",
      "\n",
      "Warning: No valid length measurements found.\n",
      "\n",
      "Data saved to final_statistics_with_prawn_ids_and_uncertainty.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the processed data from the Excel files\n",
    "final_data_1 = pd.read_excel(\"final_full_data_1_with_prawn_ids.xlsx\")\n",
    "final_data_2 = pd.read_excel(\"final_full_data_2_with_prawn_ids.xlsx\")\n",
    "final_data_3 = pd.read_excel(\"final_full_data_3_with_prawn_ids.xlsx\")\n",
    "\n",
    "# Merge the datasets on PrawnID and Label\n",
    "merged_data = pd.merge(final_data_1, final_data_2, on=['PrawnID', 'Label'], suffixes=('_1', '_2'))\n",
    "merged_data = pd.merge(merged_data, final_data_3, on=['PrawnID', 'Label'], suffixes=('', '_3'))\n",
    "\n",
    "# Now, rename the columns from final_data_3 that didn't get a suffix\n",
    "columns_to_rename = [col for col in merged_data.columns if not col.endswith(('_1', '_2', '_3')) and col not in ['PrawnID', 'Label']]\n",
    "rename_dict = {col: f\"{col}_3\" for col in columns_to_rename}\n",
    "merged_data = merged_data.rename(columns=rename_dict)\n",
    "\n",
    "# Print column names and a sample of the merged data to verify\n",
    "print(\"Columns in merged_data:\")\n",
    "print(merged_data.columns)\n",
    "print(\"\\nSample of merged_data:\")\n",
    "print(merged_data.head())\n",
    "\n",
    "# Count occurrences of each PrawnID within each Label\n",
    "prawn_counts = merged_data.groupby('Label')['PrawnID'].value_counts()\n",
    "\n",
    "# Get PrawnIDs that appear exactly 3 times for each Label\n",
    "valid_prawn_ids = prawn_counts[prawn_counts == 3].reset_index(level='PrawnID')['PrawnID']\n",
    "\n",
    "print(\"\\nValid PrawnIDs (appear exactly 3 times for each Label):\")\n",
    "print(valid_prawn_ids)\n",
    "\n",
    "# Filter the merged data to keep only valid PrawnIDs\n",
    "filtered_data = merged_data[merged_data.set_index(['Label', 'PrawnID']).index.isin(valid_prawn_ids.index)]\n",
    "\n",
    "print(\"\\nFiltered data:\")\n",
    "print(filtered_data)\n",
    "\n",
    "# Check if filtered_data is empty\n",
    "if filtered_data.empty:\n",
    "    print(\"Warning: No PrawnIDs found that appear exactly 3 times for each Label.\")\n",
    "else:\n",
    "    print(f\"Filtered data contains {len(filtered_data)} rows with PrawnIDs that appear exactly 3 times for each Label.\")\n",
    "\n",
    "# Function to calculate statistics for each PrawnID within each Label\n",
    "def calculate_statistics(group):\n",
    "    label = group['Label'].iloc[0]\n",
    "    prawn_id = group['PrawnID'].iloc[0]\n",
    "    \n",
    "    # Calculate statistics for lengths\n",
    "    lengths = group[['Length_1', 'Length_2', 'Length_3']]\n",
    "    avg_length = lengths.mean().mean()\n",
    "    std_length = lengths.std().mean()\n",
    "    \n",
    "    result = {\n",
    "        'Label': label,\n",
    "        'PrawnID': prawn_id,\n",
    "        'Avg_Length': avg_length,\n",
    "        'Std_Length': std_length,\n",
    "        'Uncertainty': std_length / (3 ** 0.5) if std_length != 0 else 0,\n",
    "        'BoundingBox_1': (group['BX_1'].iloc[0], group['BY_1'].iloc[0], group['Width_1'].iloc[0], group['Height_1'].iloc[0]),\n",
    "        'BoundingBox_2': (group['BX_2'].iloc[0], group['BY_2'].iloc[0], group['Width_2'].iloc[0], group['Height_2'].iloc[0]),\n",
    "        'BoundingBox_3': (group['BX_3'].iloc[0], group['BY_3'].iloc[0], group['Width_3'].iloc[0], group['Height_3'].iloc[0]),\n",
    "        'Angle_1': group['Angle_1'].iloc[0],\n",
    "        'Angle_2': group['Angle_2'].iloc[0],\n",
    "        'Angle_3': group['Angle_3'].iloc[0],   \n",
    "        'Length_1': group['Length_1'].iloc[0],\n",
    "        'Length_2': group['Length_2'].iloc[0],\n",
    "        'Length_3': group['Length_3'].iloc[0]\n",
    "    }\n",
    "    return pd.Series(result)\n",
    "\n",
    "# Apply the statistics calculation to each PrawnID within each Label\n",
    "final_statistics = filtered_data.groupby(['Label', 'PrawnID']).apply(calculate_statistics).reset_index(drop=True)\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"\\nColumns in final_statistics:\")\n",
    "print(final_statistics.columns)\n",
    "\n",
    "# Print some statistics about the length columns\n",
    "print(\"\\nStatistics for length columns:\")\n",
    "for col in ['Length_1', 'Length_2', 'Length_3']:\n",
    "    if col in final_statistics.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(final_statistics[col].describe())\n",
    "        print()\n",
    "\n",
    "# Calculate global uncertainty (across all measurements)\n",
    "all_lengths = pd.concat([final_statistics['Length_1'], final_statistics['Length_2'], final_statistics['Length_3']])\n",
    "all_lengths = all_lengths[all_lengths.notna()]  # Remove NaN values\n",
    "\n",
    "if len(all_lengths) > 0:\n",
    "    global_std = all_lengths.std()\n",
    "    global_uncertainty = global_std / (len(all_lengths) ** 0.5)\n",
    "else:\n",
    "    print(\"Warning: No valid length measurements found.\")\n",
    "    global_uncertainty = np.nan\n",
    "\n",
    "# Add global uncertainty to the dataframe\n",
    "final_statistics['Global_Uncertainty'] = global_uncertainty\n",
    "\n",
    "# Save the final statistics to an Excel file\n",
    "final_statistics.to_excel(\"final_full_statistics_with_prawn_ids_and_uncertainty.xlsx\", index=False)\n",
    "\n",
    "print(\"\\nData saved to final_statistics_with_prawn_ids_and_uncertainty.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid (Label, PrawnID) pairs: 191\n",
      "\n",
      "Sample of merged_data:\n",
      "   SN_1        Label  Area_1  Mean_1  Min_1    Max_1         BX_1       BY_1  \\\n",
      "0     1  full body:'   2.089   227.3  222.0  228.452  3634.979177  818.99494   \n",
      "1     1  full body:'   2.089   227.3  222.0  228.452  3634.979177  818.99494   \n",
      "2     1  full body:'   2.089   227.3  222.0  228.452  3634.979177  818.99494   \n",
      "3     1  full body:'   2.089   227.3  222.0  228.452  3634.979177  818.99494   \n",
      "4     2  full body:'   2.089   227.3  222.0  228.452  3634.979177  818.99494   \n",
      "\n",
      "     Width_1   Height_1  ...  Mean_3  Min_3    Max_3        BX_3        BY_3  \\\n",
      "0  37.999502  33.000083  ...  226.69  215.0  228.006  3475.97393  962.994157   \n",
      "1  37.999502  33.000083  ...  226.69  215.0  228.006  3475.97393  962.994157   \n",
      "2  37.999502  33.000083  ...  226.69  215.0  228.006  3475.97393  962.994157   \n",
      "3  37.999502  33.000083  ...  226.69  215.0  228.006  3475.97393  962.994157   \n",
      "4  37.999502  33.000083  ...  226.69  215.0  228.006  3475.97393  962.994157   \n",
      "\n",
      "     Width_3   Height_3  Angle_3  Slice_3  Length_3  \n",
      "0  40.998313  39.001087   43.531        1    10.000  \n",
      "1  40.998313  39.001087   43.531        1    55.172  \n",
      "2  40.998313  39.001087   43.531        1    10.000  \n",
      "3  40.998313  39.001087   43.531        1    55.172  \n",
      "4  40.998313  39.001087   43.531        1    10.000  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Scale_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Scale_1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries(result)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Apply the statistics calculation to each row\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m final_statistics \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalculate_statistics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample of final_statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_statistics\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10033\u001b[0m )\n\u001b[1;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:965\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 965\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:981\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    980\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    982\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    983\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    984\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    985\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[11], line 57\u001b[0m, in \u001b[0;36mcalculate_statistics\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     39\u001b[0m avg_length \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(lengths)\n\u001b[0;32m     40\u001b[0m std_length \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(lengths)\n\u001b[0;32m     42\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrawnID\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrawnID\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg_Length\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_length,\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStd_Length\u001b[39m\u001b[38;5;124m'\u001b[39m: std_length,\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUncertainty\u001b[39m\u001b[38;5;124m'\u001b[39m: std_length \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m std_length \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength_1\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength_1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength_2\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength_2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength_3\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength_3\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoundingBox_1\u001b[39m\u001b[38;5;124m'\u001b[39m: (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBX_1\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBY_1\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWidth_1\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeight_1\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoundingBox_2\u001b[39m\u001b[38;5;124m'\u001b[39m: (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBX_2\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBY_2\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWidth_2\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeight_2\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoundingBox_3\u001b[39m\u001b[38;5;124m'\u001b[39m: (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBX_3\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBY_3\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWidth_3\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeight_3\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAngle_1\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAngle_1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAngle_2\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAngle_2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAngle_3\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAngle_3\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScale_1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScale_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,  \n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScale_2\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScale_2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScale_3\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScale_3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     60\u001b[0m }\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries(result)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Scale_1'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the processed data from the Excel files\n",
    "final_data_1 = pd.read_excel(\"final_full_data_1_with_prawn_ids.xlsx\")\n",
    "final_data_2 = pd.read_excel(\"final_full_data_2_with_prawn_ids.xlsx\")\n",
    "final_data_3 = pd.read_excel(\"final_full_data_3_with_prawn_ids.xlsx\")\n",
    "\n",
    "# Create a set of (Label, PrawnID) tuples for each dataset\n",
    "pairs_1 = set(zip(final_data_1['Label'], final_data_1['PrawnID']))\n",
    "pairs_2 = set(zip(final_data_2['Label'], final_data_2['PrawnID']))\n",
    "pairs_3 = set(zip(final_data_3['Label'], final_data_3['PrawnID']))\n",
    "\n",
    "# Find (Label, PrawnID) pairs that appear in all three datasets\n",
    "valid_pairs = pairs_1.intersection(pairs_2, pairs_3)\n",
    "\n",
    "print(f\"Number of valid (Label, PrawnID) pairs: {len(valid_pairs)}\")\n",
    "\n",
    "# Filter each dataset to keep only valid (Label, PrawnID) pairs\n",
    "filtered_data_1 = final_data_1[final_data_1.set_index(['Label', 'PrawnID']).index.isin(valid_pairs)]\n",
    "filtered_data_2 = final_data_2[final_data_2.set_index(['Label', 'PrawnID']).index.isin(valid_pairs)]\n",
    "filtered_data_3 = final_data_3[final_data_3.set_index(['Label', 'PrawnID']).index.isin(valid_pairs)]\n",
    "\n",
    "# Merge the filtered datasets\n",
    "merged_data = pd.merge(filtered_data_1, filtered_data_2, on=['Label', 'PrawnID'], suffixes=('_1', '_2'))\n",
    "merged_data = pd.merge(merged_data, filtered_data_3, on=['Label', 'PrawnID'], suffixes=('', '_3'))\n",
    "\n",
    "# Rename columns from final_data_3 that didn't get a suffix\n",
    "columns_to_rename = [col for col in merged_data.columns if not col.endswith(('_1', '_2', '_3')) and col not in ['Label', 'PrawnID']]\n",
    "rename_dict = {col: f\"{col}_3\" for col in columns_to_rename}\n",
    "merged_data = merged_data.rename(columns=rename_dict)\n",
    "\n",
    "print(\"\\nSample of merged_data:\")\n",
    "print(merged_data.head())\n",
    "\n",
    "# Function to calculate statistics for each (Label, PrawnID) pair\n",
    "def calculate_statistics(row):\n",
    "    lengths = [row['Length_1'], row['Length_2'], row['Length_3']]\n",
    "    avg_length = np.mean(lengths)\n",
    "    std_length = np.std(lengths)\n",
    "    \n",
    "    result = {\n",
    "        'Label': row['Label'],\n",
    "        'PrawnID': row['PrawnID'],\n",
    "        'Avg_Length': avg_length,\n",
    "        'Std_Length': std_length,\n",
    "        'Uncertainty': std_length / (3 ** 0.5) if std_length != 0 else 0,\n",
    "        'Length_1': row['Length_1'],\n",
    "        'Length_2': row['Length_2'],\n",
    "        'Length_3': row['Length_3'],\n",
    "        'BoundingBox_1': (row['BX_1'], row['BY_1'], row['Width_1'], row['Height_1']),\n",
    "        'BoundingBox_2': (row['BX_2'], row['BY_2'], row['Width_2'], row['Height_2']),\n",
    "        'BoundingBox_3': (row['BX_3'], row['BY_3'], row['Width_3'], row['Height_3']),\n",
    "        'Angle_1': row['Angle_1'],\n",
    "        'Angle_2': row['Angle_2'],\n",
    "        'Angle_3': row['Angle_3'],\n",
    "        'Scale_1': row['Scale_1'],  \n",
    "        'Scale_2': row['Scale_2'],\n",
    "        'Scale_3': row['Scale_3']\n",
    "    }\n",
    "    return pd.Series(result)\n",
    "\n",
    "# Apply the statistics calculation to each row\n",
    "final_statistics = merged_data.apply(calculate_statistics, axis=1)\n",
    "\n",
    "print(\"\\nSample of final_statistics:\")\n",
    "print(final_statistics.head())\n",
    "\n",
    "# Calculate global uncertainty (across all measurements)\n",
    "all_lengths = pd.concat([final_statistics['Length_1'], final_statistics['Length_2'], final_statistics['Length_3']])\n",
    "all_lengths = all_lengths[all_lengths.notna()]  # Remove NaN values\n",
    "\n",
    "if len(all_lengths) > 0:\n",
    "    global_std = all_lengths.std()\n",
    "    global_uncertainty = global_std / (len(all_lengths) ** 0.5)\n",
    "else:\n",
    "    print(\"Warning: No valid length measurements found.\")\n",
    "    global_uncertainty = np.nan\n",
    "\n",
    "# Add global uncertainty to the dataframe\n",
    "final_statistics['Global_Uncertainty'] = global_uncertainty\n",
    "\n",
    "# Save the final statistics to an Excel file\n",
    "final_statistics.to_excel(\"final_full_statistics_with_prawn_ids_and_uncertainty.xlsx\", index=False)\n",
    "\n",
    "print(\"\\nData saved to final_statistics_with_prawn_ids_and_uncertainty.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
