{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assuming your DataFrame df contains columns:\n",
    "# ['Pond', 'WaterHeight', 'GroundTruth', 'PoseModel1', 'PoseModel2', 'SAMPrediction', 'X_Coordinate', 'Y_Coordinate']\n",
    "\n",
    "# Function to calculate Prawn-Level Metrics\n",
    "def calculate_metrics(df):\n",
    "    metrics = {}\n",
    "    for model in ['PoseModel1', 'PoseModel2', 'SAMPrediction']:\n",
    "        mae = mean_absolute_error(df['GroundTruth'], df[model])\n",
    "        rmse = np.sqrt(mean_squared_error(df['GroundTruth'], df[model]))\n",
    "        mpe = np.mean(np.abs((df['GroundTruth'] - df[model]) / df['GroundTruth'])) * 100\n",
    "        metrics[model] = {'MAE': mae, 'RMSE': rmse, 'MPE': mpe}\n",
    "    return metrics\n",
    "\n",
    "# Function to aggregate metrics by pond\n",
    "def aggregate_metrics_by_pond(df):\n",
    "    pond_metrics = {}\n",
    "    for pond in df['Pond'].unique():\n",
    "        pond_df = df[df['Pond'] == pond]\n",
    "        pond_metrics[pond] = calculate_metrics(pond_df)\n",
    "    return pond_metrics\n",
    "\n",
    "# Function to analyze height vs error\n",
    "def height_vs_error_analysis(df):\n",
    "    for model in ['PoseModel1', 'PoseModel2', 'SAMPrediction']:\n",
    "        df[f'{model}_Error'] = np.abs(df['GroundTruth'] - df[model])\n",
    "        sns.scatterplot(data=df, x='WaterHeight', y=f'{model}_Error', hue='Pond')\n",
    "        plt.title(f'Height vs Error for {model}')\n",
    "        plt.xlabel('Water Height')\n",
    "        plt.ylabel(f'{model} Error')\n",
    "        plt.show()\n",
    "\n",
    "# Function to calculate metrics for molt validation\n",
    "def calculate_molt_validation_metrics(molt_df):\n",
    "    molt_metrics = {}\n",
    "    for model in ['PoseModel1', 'PoseModel2', 'SAMPrediction']:\n",
    "        mae = mean_absolute_error(molt_df['GroundTruth'], molt_df[model])\n",
    "        rmse = np.sqrt(mean_squared_error(molt_df['GroundTruth'], molt_df[model]))\n",
    "        mpe = np.mean(np.abs((molt_df['GroundTruth'] - molt_df[model]) / molt_df['GroundTruth'])) * 100\n",
    "        molt_metrics[model] = {'MAE': mae, 'RMSE': rmse, 'MPE': mpe}\n",
    "    molt_metrics['Location'] = molt_df[['X_Coordinate', 'Y_Coordinate']].values\n",
    "    return molt_metrics\n",
    "\n",
    "# Function for side-by-side model comparisons\n",
    "def side_by_side_comparisons(df):\n",
    "    for pond in df['Pond'].unique():\n",
    "        pond_df = df[df['Pond'] == pond]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=pond_df['GroundTruth'], y=pond_df['PoseModel1'], label='Pose Model 1')\n",
    "        sns.scatterplot(x=pond_df['GroundTruth'], y=pond_df['PoseModel2'], label='Pose Model 2')\n",
    "        sns.scatterplot(x=pond_df['GroundTruth'], y=pond_df['SAMPrediction'], label='SAM Prediction')\n",
    "        plt.plot([df['GroundTruth'].min(), df['GroundTruth'].max()],\n",
    "                 [df['GroundTruth'].min(), df['GroundTruth'].max()], 'k--', lw=2)\n",
    "        plt.title(f'Comparison of Models in Pond {pond}')\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Predictions')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Function to analyze model performance across ponds\n",
    "def model_performance_across_ponds(df):\n",
    "    for model in ['PoseModel1', 'PoseModel2', 'SAMPrediction']:\n",
    "        sns.boxplot(x='Pond', y=model, data=df)\n",
    "        plt.title(f'{model} Performance Across Ponds')\n",
    "        plt.xlabel('Pond')\n",
    "        plt.ylabel('Prediction Error')\n",
    "        plt.show()\n",
    "\n",
    "# Function for correlation and regression analysis\n",
    "def correlation_regression_analysis(df):\n",
    "    correlation_results = {}\n",
    "    for pond in df['Pond'].unique():\n",
    "        pond_df = df[df['Pond'] == pond]\n",
    "        for model in ['PoseModel1', 'PoseModel2', 'SAMPrediction']:\n",
    "            corr, _ = pearsonr(pond_df['GroundTruth'], pond_df[model])\n",
    "            correlation_results[(pond, model)] = corr\n",
    "            sns.regplot(x=pond_df['GroundTruth'], y=pond_df[model])\n",
    "            plt.title(f'Regression Analysis: {model} in Pond {pond}')\n",
    "            plt.xlabel('Ground Truth')\n",
    "            plt.ylabel(model)\n",
    "            plt.show()\n",
    "    return correlation_results\n",
    "\n",
    "# Example usage:\n",
    "# prawn_metrics = calculate_metrics(df)\n",
    "# pond_metrics = aggregate_metrics_by_pond(df)\n",
    "# height_vs_error_analysis(df)\n",
    "# molt_validation_metrics = calculate_molt_validation_metrics(molt_df)\n",
    "# side_by_side_comparisons(df)\n",
    "# model_performance_across_ponds(df)\n",
    "# correlation_results = correlation_regression_analysis(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
