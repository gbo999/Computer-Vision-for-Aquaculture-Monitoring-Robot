{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mobile_sam import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"C:/Users/gbo10/OneDrive/measurement_paper_images/sam_experiments/mobile_sam.pt\"\n",
    "model_type = \"vit_t\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "sam.eval()\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\to colab\\test with seperation\\images\\right round\\GX010068_26_666-jpg_gamma_jpg.rf.d9631e54ca955ee3b93f9ad9bf8b3c5d.jpg: 640x640 5 prawns, 2301.3ms\n",
      "Speed: 8.2ms preprocess, 2301.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\detect\\predict\u001b[0m\n",
      "1 label saved to c:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\detect\\predict\\labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'prawn'}\n",
       " obb: None\n",
       " orig_img: array([[[130,  91, 153],\n",
       "         [123,  86, 132],\n",
       "         [118,  84,  95],\n",
       "         ...,\n",
       "         [146, 137,  99],\n",
       "         [140, 132,  95],\n",
       "         [136, 128,  91]],\n",
       " \n",
       "        [[129,  90, 152],\n",
       "         [123,  87, 131],\n",
       "         [117,  83,  94],\n",
       "         ...,\n",
       "         [145, 136,  98],\n",
       "         [139, 131,  94],\n",
       "         [133, 125,  88]],\n",
       " \n",
       "        [[130,  89, 150],\n",
       "         [124,  85, 130],\n",
       "         [118,  82,  94],\n",
       "         ...,\n",
       "         [144, 138,  97],\n",
       "         [138, 131,  92],\n",
       "         [131, 124,  85]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 54,  29,  49],\n",
       "         [ 50,  28,  47],\n",
       "         [ 48,  25,  47],\n",
       "         ...,\n",
       "         [ 85,  75,  57],\n",
       "         [ 84,  74,  57],\n",
       "         [ 84,  74,  56]],\n",
       " \n",
       "        [[ 54,  26,  49],\n",
       "         [ 52,  26,  50],\n",
       "         [ 52,  26,  50],\n",
       "         ...,\n",
       "         [ 82,  75,  55],\n",
       "         [ 82,  75,  56],\n",
       "         [ 81,  74,  54]],\n",
       " \n",
       "        [[ 53,  24,  49],\n",
       "         [ 54,  25,  50],\n",
       "         [ 53,  26,  52],\n",
       "         ...,\n",
       "         [ 82,  75,  55],\n",
       "         [ 81,  74,  54],\n",
       "         [ 80,  73,  53]]], dtype=uint8)\n",
       " orig_shape: (640, 640)\n",
       " path: 'C:\\\\Users\\\\gbo10\\\\OneDrive\\\\measurement_paper_images\\\\to colab\\\\test with seperation\\\\images\\\\right round\\\\GX010068_26_666-jpg_gamma_jpg.rf.d9631e54ca955ee3b93f9ad9bf8b3c5d.jpg'\n",
       " probs: None\n",
       " save_dir: 'c:\\\\Users\\\\gbo10\\\\Videos\\\\data-science\\\\Research-counting-algorithms\\\\runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 8.165121078491211, 'inference': 2301.274061203003, 'postprocess': 5.029201507568359}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import RTDETR\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = RTDETR(\"C:/Users/gbo10/OneDrive/measurement_paper_images/sam_experiments/right_round_best.pt\")\n",
    "\n",
    "# Run inference on 'bus.jpg' with arguments\n",
    "model.predict(\"C:/Users/gbo10/OneDrive/measurement_paper_images/to colab/test with seperation/images/right round/GX010068_26_666-jpg_gamma_jpg.rf.d9631e54ca955ee3b93f9ad9bf8b3c5d.jpg\", save=True,save_txt=True, imgsz=640, conf=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_box = np.array([146 433 238 587])\n",
      "input_box = np.array([311 343 377 520])\n",
      "input_box = np.array([363 507 464 603])\n",
      "input_box = np.array([ 60 348 104 631])\n",
      "input_box = np.array([ 47 549 169 640])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def yolo_to_box(yolo_line, img_width, img_height):\n",
    "    # YOLO format: class x_center y_center width height (normalized)\n",
    "    yolo_data = yolo_line.strip().split()\n",
    "    x_center, y_center, width, height = map(float, yolo_data[1:])\n",
    "\n",
    "    # Convert to pixel coordinates\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "\n",
    "    # Calculate top-left and bottom-right coordinates\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "\n",
    "    return np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "def convert_yolo_to_boxes(yolo_file, img_width, img_height):\n",
    "    boxes = []\n",
    "    with open(yolo_file, 'r') as file:\n",
    "        for line in file:\n",
    "            box = yolo_to_box(line, img_width, img_height)\n",
    "            boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "# Example usage\n",
    "yolo_file = 'C:/Users/gbo10/Videos/data-science/Research-counting-algorithms/runs/detect/predict/labels/GX010068_26_666-jpg_gamma_jpg.rf.d9631e54ca955ee3b93f9ad9bf8b3c5d.txt'\n",
    "img_width = 640  # Image width\n",
    "img_height = 640  # Image height\n",
    "\n",
    "boxes = convert_yolo_to_boxes(yolo_file, img_width, img_height)\n",
    "for box in boxes:\n",
    "    print(f\"input_box = np.array({box})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('C:/Users/gbo10/OneDrive/measurement_paper_images/to colab/test with seperation/images/right round/GX010068_26_666-jpg_gamma_jpg.rf.d9631e54ca955ee3b93f9ad9bf8b3c5d.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'boxes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m masks, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoint_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoint_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'boxes'"
     ]
    }
   ],
   "source": [
    "for input_box in boxes:\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_box[None, :  ],\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    show_mask(masks[0], plt.gca())\n",
    "    show_box(input_box, plt.gca())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
