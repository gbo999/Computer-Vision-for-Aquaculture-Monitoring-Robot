{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load the keypoints to fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils as fou\n",
    "import os\n",
    "\n",
    "# Assuming you have a list of image paths and corresponding TXT file paths\n",
    "image_paths = [...]  # List of image file paths\n",
    "txt_paths = [...]  # List of corresponding TXT file paths for each image\n",
    "\n",
    "# Create a FiftyOne dataset\n",
    "dataset = fo.Dataset(\"prawn_pose_estimation\")\n",
    "def parse_pose_estimation(txt_file):\n",
    "    \"\"\"\n",
    "    Parse the pose estimation data from a TXT file.\n",
    "\n",
    "    Parameters:\n",
    "    txt_file (str): Path to the TXT file.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of pose estimation data (each line parsed into a list of values).\n",
    "    \"\"\"\n",
    "    pose_estimations = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            pose_estimations.append([float(x) for x in line.strip().split()])\n",
    "    return pose_estimations\n",
    "\n",
    "# Loop over images and corresponding TXT files\n",
    "for img_path, txt_path in zip(image_paths, txt_paths):\n",
    "    # Parse the pose estimation data from the TXT file\n",
    "    pose_estimations = parse_pose_estimation(txt_path)\n",
    "    \n",
    "    # List to hold all detections and keypoints for the current image\n",
    "    detections = []\n",
    "    \n",
    "    for pose in pose_estimations:\n",
    "        # Extract and scale bounding box and keypoints\n",
    "        x_center_scaled = pose[1] * 5312\n",
    "        y_center_scaled = pose[2] * 2988\n",
    "        width_scaled = pose[3] * 5312\n",
    "        height_scaled = pose[4] * 2988\n",
    "\n",
    "        keypoints = []\n",
    "        for i in range(5, len(pose), 3):\n",
    "            x_kp_scaled = pose[i] * 5312\n",
    "            y_kp_scaled = pose[i + 1] * 2988\n",
    "            confidence = pose[i + 2]\n",
    "            keypoints.append(fo.Keypoint(point=[x_kp_scaled, y_kp_scaled], confidence=confidence))\n",
    "\n",
    "        bounding_box = [\n",
    "            (x_center_scaled - width_scaled / 2) / 5312,\n",
    "            (y_center_scaled - height_scaled / 2) / 2988,\n",
    "            width_scaled / 5312,\n",
    "            height_scaled / 2988\n",
    "        ]\n",
    "\n",
    "        # Create a detection object\n",
    "        detection = fo.Detection(label=\"prawn\", bounding_box=bounding_box, keypoints=keypoints)\n",
    "        detections.append(detection)\n",
    "\n",
    "    # Create a sample for FiftyOne\n",
    "    sample = fo.Sample(filepath=img_path)\n",
    "    sample[\"detections\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "# Launch the FiftyOne app to visualize\n",
    "session = fo.launch_app(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. load metadata from excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fiftyone as fo\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'your_file_path.xlsx'  # Replace with your actual file path\n",
    "metadata_df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it's loaded correctly\n",
    "print(metadata_df.head())\n",
    "\n",
    "\n",
    "# Assuming you have a list of image paths\n",
    "image_paths = [...]  # List of image file paths\n",
    "\n",
    "# Create a FiftyOne dataset\n",
    "dataset = fo.Dataset(\"prawn_metadata\")\n",
    "\n",
    "# Loop through each image and add metadata\n",
    "for img_path in image_paths:\n",
    "    # Extract the filename without extension\n",
    "    filename = img_path.split('/')[-1].replace('.jpg', '')  # Adjust if not .jpg\n",
    "    \n",
    "    # Match the filename with the metadata\n",
    "    metadata_row = metadata_df[metadata_df['file name'] == filename]\n",
    "    \n",
    "    if not metadata_row.empty:\n",
    "        metadata = metadata_row.iloc[0].to_dict()\n",
    "        \n",
    "        # Create a sample and attach metadata\n",
    "        sample = fo.Sample(filepath=img_path)\n",
    "        \n",
    "        # Add each metadata field to the sample\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'file name':  # Exclude the file name itself\n",
    "                sample[key] = value\n",
    "        \n",
    "        # Add the sample to the dataset\n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "# Launch the FiftyOne app to visualize\n",
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.bounding rectangle between detection and imagej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load ImageJ bounding rectangles from a CSV file\n",
    "imagej_bboxes_df = pd.read_csv('imagej_bboxes.csv')\n",
    "\n",
    "# Example structure of `imagej_bboxes_df`:\n",
    "# Columns: ['file name', 'x1', 'y1', 'x2', 'y2']\n",
    "def calculate_bbox_distance(detection_bbox, imagej_bbox):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between the centers of two bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    detection_bbox (tuple): (x1, y1, x2, y2) coordinates of the detection bounding box.\n",
    "    imagej_bbox (tuple): (x1, y1, x2, y2) coordinates of the ImageJ bounding box.\n",
    "\n",
    "    Returns:\n",
    "    float: Euclidean distance between the centers of the bounding boxes.\n",
    "    \"\"\"\n",
    "    # Calculate centers\n",
    "    detection_center = ((detection_bbox[0] + detection_bbox[2]) / 2, (detection_bbox[1] + detection_bbox[3]) / 2)\n",
    "    imagej_center = ((imagej_bbox[0] + imagej_bbox[2]) / 2, (imagej_bbox[1] + imagej_bbox[3]) / 2)\n",
    "    \n",
    "    # Calculate Euclidean distance between centers\n",
    "    distance = np.sqrt((detection_center[0] - imagej_center[0])**2 + (detection_center[1] - imagej_center[1])**2)\n",
    "    return distance\n",
    "for img_path in image_paths:\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "    # Assume `detections` is a list of bounding boxes from the model\n",
    "    detection_bboxes = [...]  # Replace with actual detection bounding boxes\n",
    "    \n",
    "    # Match with ImageJ bounding box\n",
    "    imagej_bbox = imagej_bboxes_df[imagej_bboxes_df['file name'] == filename].iloc[0]\n",
    "    imagej_bbox = (imagej_bbox['x1'], imagej_bbox['y1'], imagej_bbox['x2'], imagej_bbox['y2'])\n",
    "    \n",
    "    distances = []\n",
    "    for detection_bbox in detection_bboxes:\n",
    "        distance = calculate_bbox_distance(detection_bbox, imagej_bbox)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    # Create a FiftyOne sample and add metadata\n",
    "    sample = fo.Sample(filepath=img_path)\n",
    "    sample['bbox_distances'] = distances  # Store all distances for this image\n",
    "    \n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "# Launch FiftyOne app to visualize\n",
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. calculate length based on pinhole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_real_width(focal_length, distance_to_object, width_in_pixels, pixel_size):\n",
    "    \"\"\"\n",
    "    Calculate the real-life width of an object.\n",
    "\n",
    "    Parameters:\n",
    "    focal_length (float): Focal length of the camera lens in millimeters (mm).\n",
    "    distance_to_object (float): Distance from the camera to the object in millimeters (mm).\n",
    "    width_in_pixels (int): Width of the object in pixels on the image sensor.\n",
    "    pixel_size (float): Size of a pixel on the image sensor in millimeters (mm).\n",
    "\n",
    "    Returns:\n",
    "    float: Real-life width of the object in centimeters (cm).\n",
    "    \"\"\"\n",
    "    # Calculate the width of the object in the image sensor plane in millimeters\n",
    "    width_in_sensor = width_in_pixels * pixel_size\n",
    "\n",
    "    # Calculate the real-life width of the object using the similar triangles principle\n",
    "    real_width_mm = (width_in_sensor * distance_to_object) / focal_length\n",
    "\n",
    "    # Convert the width from millimeters to centimeters\n",
    "    real_width_cm = real_width_mm / 10.0\n",
    "\n",
    "    return real_width_cm\n",
    "\n",
    "def calculate_euclidean_distance(keypoint1, keypoint2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    keypoint1, keypoint2: Tuples of (x, y) coordinates of the keypoints in pixels.\n",
    "\n",
    "    Returns:\n",
    "    float: Euclidean distance between the keypoints in pixels.\n",
    "    \"\"\"\n",
    "    return np.sqrt((keypoint1[0] - keypoint2[0])**2 + (keypoint1[1] - keypoint2[1])**2)\n",
    "\n",
    "# Example of how to integrate this with your FiftyOne dataset\n",
    "for img_path in image_paths:\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    metadata_row = metadata_df[metadata_df['file name'] == filename]\n",
    "    \n",
    "    if not metadata_row.empty:\n",
    "        metadata = metadata_row.iloc[0].to_dict()\n",
    "        sample = fo.Sample(filepath=img_path)\n",
    "        \n",
    "        # Add metadata\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'file name':\n",
    "                sample[key] = value\n",
    "        \n",
    "        # Calculate the real height or width if keypoints and necessary metadata are available\n",
    "        if 'height(mm)' in metadata:\n",
    "            height_mm = metadata['height(mm)']\n",
    "            focal_length = 6.82  # Example focal length in mm\n",
    "            pixel_size = 0.0014  # Example pixel size in mm\n",
    "            keypoints = [...]  # Replace with actual keypoints from your data\n",
    "            \n",
    "            if len(keypoints) >= 2:\n",
    "                # Calculate the Euclidean distance in pixels\n",
    "                euclidean_distance_pixels = calculate_euclidean_distance(keypoints[0], keypoints[1])\n",
    "                \n",
    "                # Calculate the real width/height in centimeters\n",
    "                real_width_cm = calculate_real_width(focal_length, height_mm, euclidean_distance_pixels, pixel_size)\n",
    "                \n",
    "                # Attach the calculated real width/height to the sample\n",
    "                sample[\"real_width_cm\"] = real_width_cm\n",
    "        \n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# complete maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = fo.list_datasets()\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    if dataset_name.startswith('prawn_combined_dataset'):\n",
    "        fo.delete_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the existing filtered data with PrawnIDs\n",
    "filtered_data_file_path = r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\src\\measurement\\ImageJ\\Filtered_Data.csv'  # Replace with your actual file path\n",
    "filtered_df = pd.read_csv(filtered_data_file_path)\n",
    "\n",
    "# Load the additional metadata from another Excel file\n",
    "metadata_file_path = r\"C:\\Users\\gbo10\\OneDrive\\research\\thesis and paper\\test images.xlsx\"  # Replace with your actual file path\n",
    "metadata_df = pd.read_excel(metadata_file_path)\n",
    "\n",
    "# Function to parse pose estimation data from a TXT file\n",
    "def parse_pose_estimation(txt_file):\n",
    "    pose_estimations = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            pose_estimations.append([float(x) for x in line.strip().split()])\n",
    "    return pose_estimations\n",
    "\n",
    "# Function to calculate Euclidean distance between keypoints\n",
    "def calculate_euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "# Function to calculate the real-life width/height\n",
    "def calculate_real_width(focal_length, distance_to_object, width_in_pixels, pixel_size):\n",
    "    width_in_sensor = width_in_pixels * pixel_size\n",
    "    real_width_mm = (width_in_sensor * distance_to_object) / focal_length\n",
    "    return real_width_mm\n",
    "def extract_identifier_from_gt(filename):\n",
    "    return filename.split('-')[0]\n",
    "\n",
    "folder_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\car'  # Replace with the folder containing the images and TXT files\n",
    "\n",
    "# Assuming you have the following lists\n",
    "image_paths = [os.path.join(folder_path, image) for image in os.listdir(folder_path) if image.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "\n",
    "prediction_folder_path=r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict19\\labels\"\n",
    "\n",
    "txt_paths = [os.path.join(prediction_folder_path, txt) for txt in os.listdir(prediction_folder_path) if txt.endswith('.txt')]\n",
    "# Create a FiftyOne dataset\n",
    "ground_truth_paths = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\labels\"  # Replace with the folder containing the ground truth TXT files\n",
    "\n",
    "ground_truth_paths_text = [os.path.join(ground_truth_paths, txt) for txt in os.listdir(ground_truth_paths) if txt.endswith('.txt')]\n",
    "\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "# List all datasets in FiftyOne\n",
    "\n",
    "# Loop through the list and delete datasets that match the condition\n",
    "\n",
    "\n",
    "#delete all datasets\n",
    "\n",
    "\n",
    "# Create a new dataset\n",
    "\n",
    "dataset = fo.Dataset(\"prawn_combined_dataset22\", overwrite=True)\n",
    "\n",
    "dataset.default_skeleton = fo.KeypointSkeleton(\n",
    "    labels=[\n",
    "        \"start_carapace\",\n",
    "        \"eyes\",\n",
    "    ],\n",
    "    edges=[\n",
    "        [0, 1],  # Connect keypoint 0 to keypoint 1  # Connect keypoint 1 to keypoint 2\n",
    "        # Add more connections as needed\n",
    "    ],\n",
    ")\n",
    "# Loop over images and corresponding TXT files\n",
    "for img_path in image_paths:\n",
    "\n",
    "    # Extract the relevant part for matching\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0] \n",
    "    parts = filename.split('_')\n",
    "    relevant_part = f\"{parts[1][-3:]}_{parts[3].split('.')[0]}\"\n",
    "     # e.g., undistorted_GX010152_36_378.jpg_gamma\n",
    "    identifier = filename.replace('undistorted_', '').replace('.jpg_gamma', '')  # Extract the identifier from the filename\n",
    "\n",
    "\n",
    "    # Construct the paths to the prediction and ground truth files\n",
    "    prediction_txt_path = os.path.join(prediction_folder_path, f\"{filename}.txt\")\n",
    "\n",
    "    # Match ground truth based on the extracted identifier\n",
    "    ground_truth_txt_path = None\n",
    "    for gt_file in ground_truth_paths_text:\n",
    "        b= extract_identifier_from_gt(os.path.basename(gt_file))\n",
    "        if b == identifier:\n",
    "            ground_truth_txt_path = gt_file\n",
    "\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Parse the pose estimation data from the TXT file\n",
    "    pose_estimations = parse_pose_estimation(prediction_txt_path)\n",
    "    \n",
    "    ground_truths = parse_pose_estimation(ground_truth_txt_path)\n",
    "\n",
    "    \n",
    "    keypoints_list = []  # List to store keypoints\n",
    "    detections = []  # List to store detection bounding boxes\n",
    "    ground_truth_keypoints_list = []\n",
    "    ground_truth_detection_list = []\n",
    "\n",
    "    for pose in pose_estimations:\n",
    "        if len(pose)==11:        \n",
    "\n",
    "            x1_rel = pose[1] \n",
    "            y1_rel = pose[2] \n",
    "            width_rel = pose[3] \n",
    "            height_rel = pose[4] \n",
    "\n",
    "            \n",
    "            #center to top left\n",
    "            x1_rel = x1_rel - width_rel/2\n",
    "            y1_rel = y1_rel - height_rel/2\n",
    "        # Calc\n",
    "\n",
    "\n",
    "\n",
    "            keypoints = []\n",
    "            confidences = []\n",
    "            for i in range(5, len(pose), 3):\n",
    "                x_kp_scaled = pose[i] \n",
    "                y_kp_scaled = pose[i + 1] \n",
    "                confidence = pose[i + 2]\n",
    "                keypoints.append([x_kp_scaled, y_kp_scaled])\n",
    "            \n",
    "\n",
    "            confidences = [float(pose[i+2]) for i in range(5, len(pose), 3)]\n",
    "\n",
    "\n",
    "         \n",
    "                \n",
    "            keypoint=fo.Keypoint(points=keypoints, confidence=None)\n",
    "            keypoints_list.append(keypoint)\n",
    "\n",
    "\n",
    "            keypoints_dict= {'point1':keypoints[0],'point2':keypoints[1]}\n",
    "            detections.append(fo.Detection\n",
    "                              (label=\"prawn\", bounding_box=[x1_rel, y1_rel, width_rel, height_rel],\n",
    "                               attributes={'keypoints':keypoints_dict}))\n",
    "            \n",
    "    for gt_pose in ground_truths:\n",
    "            x1_rel = gt_pose[1]\n",
    "            y1_rel = gt_pose[2]\n",
    "            width_rel = gt_pose[3]\n",
    "            height_rel = gt_pose[4]\n",
    "\n",
    "            # Convert center to top-left\n",
    "            x1_rel = x1_rel - width_rel / 2\n",
    "            y1_rel = y1_rel - height_rel / 2\n",
    "\n",
    "            keypoints = []\n",
    "            for i in range(5, len(gt_pose), 3):\n",
    "                x_kp_scaled = gt_pose[i]\n",
    "                y_kp_scaled = gt_pose[i + 1]\n",
    "                keypoints.append([x_kp_scaled, y_kp_scaled])\n",
    "\n",
    "            ground_truth_keypoints_list.append(fo.Keypoint(points=keypoints))\n",
    "            ground_truth_detection_list.append(fo.Detection(label=\"prawn_truth\", bounding_box=[x1_rel, y1_rel, width_rel, height_rel]))\n",
    "\n",
    "    sample = fo.Sample(filepath=img_path)\n",
    "    \n",
    "    sample[\"ground_truth_keypoints\"] = fo.Keypoints(keypoints=ground_truth_keypoints_list)\n",
    "\n",
    "\n",
    "    sample[\"ground_truth_detections\"] = fo.Detections(detections=detections)   \n",
    "    # Add keypoints as a separate field\n",
    "    sample[\"keypoints\"] = fo.Keypoints(keypoints=keypoints_list)\n",
    "   \n",
    "    sample[\"detections_predictions\"] = fo.Detections(detections=detections)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Match the filename with the corresponding rows in the filtered data\n",
    "    matching_rows = filtered_df[filtered_df['Label'] == f'carapace:{filename}']\n",
    "    \n",
    "    # Add metadata from the additional Excel file\n",
    "    metadata_row = metadata_df[metadata_df['file name'] == relevant_part]\n",
    "\n",
    "    if not metadata_row.empty:\n",
    "        metadata = metadata_row.iloc[0].to_dict() \n",
    "        for key, value in metadata.items():\n",
    "            if key != 'file name': \n",
    "                sample[key] = value\n",
    "            \n",
    "                        \n",
    "    else:\n",
    "        print(f\"No metadata found for {relevant_part}\")\n",
    "    true_detections=[]\n",
    "\n",
    "    for _, row in matching_rows.iterrows():\n",
    "        prawn_id = row['PrawnID']\n",
    "        prawn_bbox = ast.literal_eval(row['BoundingBox_1'])  # Replace with the correct column name\n",
    "\n",
    "        # Convert the tuple elements to floats\n",
    "        prawn_bbox = tuple(float(coord) for coord in prawn_bbox)# Replace with correct bounding box columns\n",
    "        \n",
    "        prawn_normalized_bbox=[prawn_bbox[0]/5312, prawn_bbox[1]/2988, prawn_bbox[2]/5312, prawn_bbox[3]/2988]\n",
    "\n",
    "        # Convert PrawnID bounding box to absolute coordinates\n",
    "        \n",
    "        \n",
    "        true_detections.append(fo.Detection(label=\"prawn_true\", bounding_box=prawn_normalized_bbox))\n",
    "\n",
    "        prawn_point=(prawn_bbox[0]/5312, prawn_bbox[1]/2988)\n",
    "        \n",
    "\n",
    "        # Find the closest detection bounding box within the same image (Label)\n",
    "        min_distance = float('inf')\n",
    "        closest_detection = None\n",
    "        \n",
    "        for detection_bbox in sample[\"detections_predictions\"].detections:\n",
    "            detection_bbox_coords = detection_bbox.bounding_box\n",
    "            \n",
    "            det_point=(detection_bbox_coords[0], detection_bbox_coords[1])\n",
    "            \n",
    "            distance = calculate_euclidean_distance(prawn_point, det_point)\n",
    "            \n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_detection = detection_bbox\n",
    "        \n",
    "        if closest_detection is not None:\n",
    "            height_mm = sample['heigtht(mm)']  # Use appropriate column for height in mm\n",
    "            focal_length = 24.22\n",
    "            pixel_size = 0.00716844 \n",
    "\n",
    "                \n",
    "\n",
    "            keypoints_dict2 = closest_detection.attributes[\"keypoints\"]\n",
    "\n",
    "            keypoints1 = [keypoints_dict2['point1'], keypoints_dict2['point2']]    \n",
    "\n",
    "            keypoint1_scaled = [keypoints1[0][0] * 5312, keypoints1[0][1] * 2988]\n",
    "            keypoint2_scaled = [keypoints1[1][0] * 5312, keypoints1[1][1] * 2988]\n",
    "\n",
    "            euclidean_distance_pixels = calculate_euclidean_distance(keypoint1_scaled, keypoint2_scaled)\n",
    "            real_length_cm = calculate_real_width(focal_length, height_mm, euclidean_distance_pixels, pixel_size)\n",
    "            \n",
    "\n",
    "            # Update the filtered_df with the calculated lengths\n",
    "            filtered_df.loc[(filtered_df['Label'] == f'carapace:{filename}') & (filtered_df['PrawnID'] == prawn_id), 'RealLength(cm)'] = real_length_cm\n",
    "            \n",
    "            # Add floating text near the keypoints in FiftyOne\n",
    "\n",
    "            true_length=filtered_df.loc[(filtered_df['Label'] == f'carapace:{filename}') & (filtered_df['PrawnID'] == prawn_id), 'Avg_Length'].values[0]\n",
    "\n",
    "\n",
    "            closest_detection_label =f'MPError: {abs(real_length_cm - true_length) / true_length * 100:.2f}%, true length: {true_length:.2f}cm, pred length: {real_length_cm:.2f}cm'\n",
    "            \n",
    "            closest_detection.label=closest_detection_label\n",
    "\n",
    "\n",
    "            #if mpe>25: tag sample\n",
    "            if abs(real_length_cm - true_length) / true_length * 100 > 25:\n",
    "                if \"MPE>25\" not in sample.tags:\n",
    "                    sample.tags.append(\"MPE>25\")\n",
    "            \n",
    "    # Assuming 'sample' is your FiftyOne sample object\n",
    "        \n",
    "    sample[\"true_detections\"] = fo.Detections(detections=true_detections)            \n",
    "\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "ground_truth_count=0\n",
    "\n",
    "for sample in dataset:\n",
    "    ground_truth_count+=len(sample[\"ground_truth_detections\"].detections)\n",
    "\n",
    "print('gt',ground_truth_count)\n",
    "\n",
    "detections_predictions_count=0\n",
    "\n",
    "for sample in dataset:\n",
    "    detections_predictions_count+=len(sample[\"detections_predictions\"].detections)\n",
    "\n",
    "print('pred',detections_predictions_count)\n",
    "\n",
    "\n",
    "# Save the updated filtered data with the calculated lengths\n",
    "# filtered_df.to_excel('updated_filtered_data_with_lengths.xlsx', index=False)\n",
    "\n",
    "# Launch the FiftyOne app to visualize\n",
    "\n",
    "# Define a custom view\n",
    "\n",
    "# Configure the view to show your custom attributes\n",
    "\n",
    "\n",
    "session = fo.launch_app(dataset, port=5151)\n",
    "\n",
    "\n",
    "# Launch the app with the custom view\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x1024 1617.6ms\n",
      "Speed: 23.0ms preprocess, 1617.6ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1602.6ms\n",
      "Speed: 18.6ms preprocess, 1602.6ms inference, 8.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1428.8ms\n",
      "Speed: 19.0ms preprocess, 1428.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1512.6ms\n",
      "Speed: 22.0ms preprocess, 1512.6ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1498.9ms\n",
      "Speed: 23.9ms preprocess, 1498.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1508.2ms\n",
      "Speed: 22.5ms preprocess, 1508.2ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1320.1ms\n",
      "Speed: 19.0ms preprocess, 1320.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1388.6ms\n",
      "Speed: 20.0ms preprocess, 1388.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1483.9ms\n",
      "Speed: 21.0ms preprocess, 1483.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1378.4ms\n",
      "Speed: 23.0ms preprocess, 1378.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1368.6ms\n",
      "Speed: 19.0ms preprocess, 1368.6ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1379.5ms\n",
      "Speed: 22.0ms preprocess, 1379.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1412.8ms\n",
      "Speed: 18.0ms preprocess, 1412.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1406.2ms\n",
      "Speed: 20.0ms preprocess, 1406.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1392.4ms\n",
      "Speed: 21.0ms preprocess, 1392.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1465.3ms\n",
      "Speed: 19.5ms preprocess, 1465.3ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1636.5ms\n",
      "Speed: 20.0ms preprocess, 1636.5ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1630.4ms\n",
      "Speed: 20.5ms preprocess, 1630.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1476.4ms\n",
      "Speed: 22.0ms preprocess, 1476.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1429.6ms\n",
      "Speed: 20.0ms preprocess, 1429.6ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1601.3ms\n",
      "Speed: 28.0ms preprocess, 1601.3ms inference, 7.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1433.9ms\n",
      "Speed: 17.0ms preprocess, 1433.9ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1392.4ms\n",
      "Speed: 20.5ms preprocess, 1392.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1430.1ms\n",
      "Speed: 18.0ms preprocess, 1430.1ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1431.5ms\n",
      "Speed: 18.0ms preprocess, 1431.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1405.5ms\n",
      "Speed: 17.5ms preprocess, 1405.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1522.7ms\n",
      "Speed: 18.0ms preprocess, 1522.7ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1660.8ms\n",
      "Speed: 17.5ms preprocess, 1660.8ms inference, 12.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1594.5ms\n",
      "Speed: 20.0ms preprocess, 1594.5ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1505.3ms\n",
      "Speed: 17.0ms preprocess, 1505.3ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Client is not connected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 174\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Save dataset\u001b[39;00m\n\u001b[0;32m    173\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m--> 174\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5153\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:196\u001b[0m, in \u001b[0;36mlaunch_app\u001b[1;34m(dataset, view, spaces, color_scheme, plots, port, address, remote, desktop, browser, height, auto, config)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Launches the FiftyOne App.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mNote that only one App instance can be opened at a time. If this method is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    a :class:`Session`\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _session  \u001b[38;5;66;03m# pylint: disable=global-statement\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m _session \u001b[38;5;241m=\u001b[39m \u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_scheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_scheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesktop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesktop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrowser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _session\u001b[38;5;241m.\u001b[39mremote:\n\u001b[0;32m    213\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(_REMOTE_INSTRUCTIONS\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mformat(_session\u001b[38;5;241m.\u001b[39mserver_port))\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:435\u001b[0m, in \u001b[0;36mSession.__init__\u001b[1;34m(self, dataset, view, view_name, spaces, color_scheme, plots, port, address, remote, desktop, browser, height, auto, config)\u001b[0m\n\u001b[0;32m    432\u001b[0m _register_session(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_height\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser \u001b[38;5;241m=\u001b[39m browser\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremote:\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:256\u001b[0m, in \u001b[0;36mupdate_state.<locals>.decorator.<locals>.wrapper\u001b[1;34m(session, *args, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_show \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n\u001b[0;32m    255\u001b[0m     session\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[1;32m--> 256\u001b[0m result \u001b[38;5;241m=\u001b[39m func(session, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    257\u001b[0m session\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend_event(StateUpdate(state\u001b[38;5;241m=\u001b[39msession\u001b[38;5;241m.\u001b[39m_state))\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_show \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:1043\u001b[0m, in \u001b[0;36mSession.show\u001b[1;34m(self, height)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesktop:\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m_reload()\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:1144\u001b[0m, in \u001b[0;36mSession.freeze\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1141\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly notebook sessions can be frozen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDeactivateNotebookCell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplots\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\client.py:152\u001b[0m, in \u001b[0;36mClient.send_event\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient is not connected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_event(event)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_event(event)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Client is not connected"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import SAM\n",
    "from math import sqrt\n",
    "from random import randint, shuffle\n",
    "import fiftyone as fo\n",
    "import fiftyone.core.labels as fol\n",
    "import os\n",
    "\n",
    "# Helper Classes and Functions for Circle Calculation\n",
    "class Point:\n",
    "    def __init__(self, X=0, Y=0) -> None:\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "class Circle:\n",
    "    def __init__(self, c=Point(), r=0) -> None:    \n",
    "        self.C = c\n",
    "        self.R = r\n",
    "\n",
    "def dist(a, b):\n",
    "    return sqrt((a.X - b.X) ** 2 + (a.Y - b.Y) ** 2)\n",
    "\n",
    "def is_inside(c, p):\n",
    "    return dist(c.C, p) <= c.R\n",
    "\n",
    "def get_circle_center(bx, by, cx, cy):\n",
    "    B = bx * bx + by * by\n",
    "    C = cx * cx + cy * cy\n",
    "    D = bx * cy - by * cx\n",
    "    return Point((cy * B - by * C) / (2 * D), (bx * C - cx * B) / (2 * D))\n",
    "\n",
    "def circle_from1(A, B):\n",
    "    C = Point((A.X + B.X) / 2.0, (A.Y + B.Y) / 2.0)\n",
    "    return Circle(C, dist(A, B) / 2.0)\n",
    "\n",
    "def circle_from2(A, B, C):\n",
    "    I = get_circle_center(B.X - A.X, B.Y - A.Y, C.X - A.X, C.Y - A.Y)\n",
    "    I.X += A.X\n",
    "    I.Y += A.Y\n",
    "    return Circle(I, dist(I, A))\n",
    "\n",
    "def is_valid_circle(c, P):\n",
    "    for p in P:\n",
    "        if not is_inside(c, p):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def min_circle_trivial(P):\n",
    "    assert len(P) <= 3\n",
    "    if not P:\n",
    "        return Circle()\n",
    "    elif len(P) == 1:\n",
    "        return Circle(P[0], 0)\n",
    "    elif len(P) == 2:\n",
    "        return circle_from1(P[0], P[1])\n",
    "    for i in range(3):\n",
    "        for j in range(i + 1, 3):\n",
    "            c = circle_from1(P[i], P[j])\n",
    "            if is_valid_circle(c, P):\n",
    "                return c\n",
    "    return circle_from2(P[0], P[1], P[2])\n",
    "\n",
    "def welzl_helper(P, R, n):\n",
    "    if n == 0 or len(R) == 3:\n",
    "        return min_circle_trivial(R)\n",
    "    idx = randint(0, n - 1)\n",
    "    p = P[idx]\n",
    "    P[idx], P[n - 1] = P[n - 1], P[idx]\n",
    "    d = welzl_helper(P, R.copy(), n - 1)\n",
    "    if is_inside(d, p):\n",
    "        return d\n",
    "    R.append(p)\n",
    "    return welzl_helper(P, R.copy(), n - 1)\n",
    "\n",
    "def welzl(P):\n",
    "    P_copy = P.copy()\n",
    "    shuffle(P_copy)\n",
    "    return welzl_helper(P_copy, [], len(P_copy))\n",
    "def yolo_key_to_box(yolo_line, img_width=640, img_height=640):\n",
    "    # YOLO format: class x_center y_center width height (normalized)\n",
    "    yolo_data = yolo_line.strip().split()\n",
    "    x_center, y_center, width, height = map(float, yolo_data[1:5])\n",
    "\n",
    "\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "\n",
    "    # Calculate top-left and bottom-right coordinates\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "\n",
    "    return np.array([ x_min, y_min, x_max, y_max])\n",
    "\n",
    "def convert_yolo_key_to_boxes(yolo_file):\n",
    "    boxes = []\n",
    "    with open(yolo_file, 'r') as file:\n",
    "        for line in file:\n",
    "            box = yolo_key_to_box(line)\n",
    "            boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "def find_matching_yolo_file(core_name, yolo_dir):\n",
    "    for yolo_file in os.listdir(yolo_dir):\n",
    "        if core_name in yolo_file:  # Check if the core name is part of the YOLO file name\n",
    "            return os.path.join(yolo_dir, yolo_file)\n",
    "    return None \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize FiftyOne dataset\n",
    "dataset = fo.Dataset(\"prawn_segmentation\", overwrite=True)\n",
    "\n",
    "# Load SAM model\n",
    "model = SAM(r'mobile_sam.pt')\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\"\n",
    "yolo_dir = r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict19\\labels\"\n",
    "\n",
    "# Iterate over images\n",
    "for image_name in os.listdir(image_dir):\n",
    "    if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "       \n",
    "        core_name = image_name.split('.')[0]\n",
    "        # Create the corresponding YOLO file path\n",
    "\n",
    "                \n",
    "\n",
    "        yolo_file = find_matching_yolo_file(core_name, yolo_dir)\n",
    "        \n",
    "        # Convert YOLO annotations to bounding boxes\n",
    "        boxes = convert_yolo_key_to_boxes(yolo_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Perform segmentation\n",
    "        \n",
    "        # Initialize list to store segmentations\n",
    "        all_polygons = []\n",
    "        all_attributes = []\n",
    "\n",
    "        \n",
    "        # Process each mask individually\n",
    "        for xy in results[0].masks.xy:\n",
    "                 \n",
    "    # Normalize points to [0, 1] range\n",
    "                normalized_points = [(float(p[0]) / image.shape[1], float(p[1]) / image.shape[0]) for p in xy]\n",
    "                all_polygons.append(normalized_points)\n",
    "                points = [Point(p[0], p[1]) for p in xy]\n",
    "                mec = welzl(points)\n",
    "                diameter = mec.R * 2 \n",
    "                all_attributes.append({'diameter': diameter})\n",
    "                # Convert contour to a format suitable for FiftyOne\n",
    "                segmentation = fol.Polyline(\n",
    "                        points=all_polygons,\n",
    "                        closed=True,\n",
    "                        filled=True,\n",
    "                        attributes=all_attributes\n",
    "\n",
    "                        \n",
    "                    )\n",
    "        \n",
    "        # Add image and segmentation to FiftyOne dataset\n",
    "        sample = fo.Sample(filepath=image_path)\n",
    "        sample[\"segmentations\"] =fol.Polylines(polylines=[segmentation])\n",
    "\n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "# Save dataset\n",
    "dataset.save()\n",
    "session = fo.launch_app(dataset, port=5153)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x1024 5168.0ms\n",
      "Speed: 54.0ms preprocess, 5168.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict44\u001b[0m\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h46m36s160.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h46m36s160_segmentations.txt\n",
      "\n",
      "0: 1024x1024 8094.8ms\n",
      "Speed: 17.0ms preprocess, 8094.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict44\u001b[0m\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h46m54s892.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h46m54s892_segmentations.txt\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h46m54s892.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h46m54s892_segmentations.txt\n",
      "\n",
      "0: 1024x1024 8889.7ms\n",
      "Speed: 23.0ms preprocess, 8889.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict44\u001b[0m\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h47m01s074.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h47m01s074_segmentations.txt\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h47m01s074.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h47m01s074_segmentations.txt\n",
      "\n",
      "0: 1024x1024 8669.5ms\n",
      "Speed: 24.0ms preprocess, 8669.5ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict44\u001b[0m\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h47m15s479.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h47m15s479_segmentations.txt\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h47m15s479.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h47m15s479_segmentations.txt\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h47m15s479.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h47m15s479_segmentations.txt\n",
      "\n",
      "0: 1024x1024 10360.7ms\n",
      "Speed: 41.5ms preprocess, 10360.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict44\u001b[0m\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h49m34s599.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h49m34s599_segmentations.txt\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h49m34s599.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h49m34s599_segmentations.txt\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h49m34s599.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h49m34s599_segmentations.txt\n",
      "\n",
      "0: 1024x1024 7822.1ms\n",
      "Speed: 20.0ms preprocess, 7822.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict44\u001b[0m\n",
      "Saved segmentation for undistorted_vlcsnap-2024-09-19-21h49m47s447.png to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\\undistorted_vlcsnap-2024-09-19-21h49m47s447_segmentations.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import SAM\n",
    "\n",
    "# YOLO format conversion\n",
    "def yolo_key_to_box(yolo_line, img_width=640, img_height=640):\n",
    "    yolo_data = yolo_line.strip().split()\n",
    "    x_center, y_center, width, height = map(float, yolo_data[1:5])\n",
    "\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "\n",
    "    return np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "def convert_yolo_key_to_boxes(yolo_file):\n",
    "    boxes = []\n",
    "    with open(yolo_file, 'r') as file:\n",
    "        for line in file:\n",
    "            box = yolo_key_to_box(line)\n",
    "            boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "def find_matching_yolo_file(core_name, yolo_dir):\n",
    "    for yolo_file in os.listdir(yolo_dir):\n",
    "        if core_name in yolo_file:  # Check if the core name is part of the YOLO file name\n",
    "            return os.path.join(yolo_dir, yolo_file)\n",
    "    return None \n",
    "\n",
    "# Initialize SAM model\n",
    "model = SAM(r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\fifty_one\\measurements\\sam2_b.pt')\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized\"\n",
    "yolo_dir = r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict39\\labels\"\n",
    "# Iterate over images\n",
    "for image_name in os.listdir(image_dir):\n",
    "    if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "        core_name = image_name.split('.')[0]\n",
    "        \n",
    "        # Create the corresponding YOLO file path\n",
    "        yolo_file = find_matching_yolo_file(core_name, yolo_dir)\n",
    "        if yolo_file is None:\n",
    "            print(f\"No matching YOLO file found for {image_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert YOLO annotations to bounding boxes\n",
    "        boxes = convert_yolo_key_to_boxes(yolo_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Perform segmentation\n",
    "        results = model(image, imgsz=640,save=True, bboxes=np.array(boxes))\n",
    "        \n",
    "        # Prepare a .txt file to save the segmentations\n",
    "        output_txt_path = os.path.join(image_dir, f\"{core_name}_segmentations.txt\")\n",
    "        \n",
    "        with open(output_txt_path, 'w') as f:\n",
    "            # Process each mask individually\n",
    "            for xy in results[0].masks.xy:\n",
    "                # Convert the xy coordinates into a string format for saving\n",
    "                xy_str = \" \".join(map(str, xy.flatten().tolist()))  # Flatten and convert to string\n",
    "                f.write(xy_str + \"\\n\")  # Write each xy as a line\n",
    "                print(f\"Saved segmentation for {image_name} to {output_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image with filled segmentations to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\\undistorted_GX010091_5_149_with_segmentations.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_filled_segmentations_with_opacity(image, xy_coordinates, color=(0, 255, 0), opacity=0.2):\n",
    "    \"\"\"\n",
    "    Draws and fills the segmentation polygons on the image with a specified opacity.\n",
    "    xy_coordinates: List of lists where each inner list represents a segmentation polygon \n",
    "                    (each polygon is a list of [x, y] pairs).\n",
    "    color: The color to fill the polygons (in BGR format).\n",
    "    opacity: A float value between 0 and 1 that determines the transparency of the fill.\n",
    "    \"\"\"\n",
    "    # Create an overlay image to draw on\n",
    "    overlay = image.copy()\n",
    "    \n",
    "    # Draw filled polygons on the overlay\n",
    "    for polygon in xy_coordinates:\n",
    "        polygon = np.array(polygon, np.int32)  # Convert to NumPy array of int32\n",
    "        polygon = polygon.reshape((-1, 1, 2))  # Reshape for OpenCV\n",
    "        cv2.fillPoly(overlay, [polygon], color=color)  # Fill the polygon on the overlay\n",
    "    \n",
    "    # Blend the overlay with the original image\n",
    "    cv2.addWeighted(overlay, opacity, image, 1 - opacity, 0, image)\n",
    "    return image\n",
    "\n",
    "def load_segmentations_from_txt(txt_file):\n",
    "    \"\"\"\n",
    "    Loads segmentation polygons from a .txt file.\n",
    "    Each line in the file contains space-separated coordinates: x1 y1 x2 y2 ... xn yn.\n",
    "    \"\"\"\n",
    "    segmentations = []\n",
    "    with open(txt_file, 'r') as file:\n",
    "        for line in file:\n",
    "            coords = list(map(float, line.strip().split()))  # Read all floats from the line\n",
    "            polygon = [(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]  # Convert into (x, y) pairs\n",
    "            segmentations.append(polygon)\n",
    "    return segmentations\n",
    "\n",
    "# Specify the image and corresponding segmentation file\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\"\n",
    "image_name = \"undistorted_GX010091_5_149.jpg_gamma.jpg\"  # Replace with the name of your image\n",
    "image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "core_name = image_name.split('.')[0]\n",
    "\n",
    "# Path to the corresponding segmentations .txt file\n",
    "txt_file_path = os.path.join(image_dir, f\"{core_name}_segmentations.txt\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the segmentation file exists\n",
    "if os.path.exists(txt_file_path):\n",
    "    # Load segmentations from the .txt file\n",
    "    segmentations = load_segmentations_from_txt(txt_file_path)\n",
    "    \n",
    "    # Draw and fill the segmentations on the image\n",
    "    image_with_segmentations = draw_filled_segmentations_with_opacity(image, segmentations)\n",
    "    \n",
    "    # Save or display the result\n",
    "    output_image_path = os.path.join(image_dir, f\"{core_name}_with_segmentations.png\")\n",
    "    cv2.imwrite(output_image_path, image_with_segmentations)\n",
    "    print(f\"Saved image with filled segmentations to {output_image_path}\")\n",
    "    \n",
    "    # Optionally, display the image (for local testing)\n",
    "    # cv2.imshow('Segmented Image', image_with_segmentations)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(f\"No segmentation file found for {image_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try skeletonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import skeletonize\n",
    "import networkx as nx\n",
    "\n",
    "def load_multiple_coords_from_txt(txt_file):\n",
    "    \"\"\"\n",
    "    Loads multiple (x, y) coordinates sets from a .txt file.\n",
    "    Each line in the file contains space-separated coordinates: x1 y1 x2 y2 ... xn yn.\n",
    "    Each line represents a different segmentation.\n",
    "    \"\"\"\n",
    "    segmentations = []\n",
    "    with open(txt_file, 'r') as file:\n",
    "        for line in file:\n",
    "            coords = list(map(float, line.strip().split()))  # Read all floats from the line\n",
    "            polygon = [(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]   # Convert into (y, x) pairs\n",
    "            segmentations.append(polygon)\n",
    "    return segmentations\n",
    "\n",
    "def create_filled_binary_mask(coords, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Creates a binary mask with a filled polygon from a single segmentation's (y, x) coordinates.\n",
    "    \"\"\"\n",
    "    binary_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "    polygon = np.array(coords, np.int32)  # Convert to NumPy array of int32\n",
    "    polygon = polygon.reshape((-1, 1, 2))  # Reshape for OpenCV fillPoly\n",
    "    cv2.fillPoly(binary_mask, [polygon], color=1) \n",
    "    \n",
    "     # Fill the polygon on the binary mask\n",
    "    return binary_mask\n",
    "\n",
    "def skeletonize_mask(binary_mask,i):\n",
    "    \"\"\"\n",
    "    Skeletonizes the binary mask using skimage's skeletonize function.\n",
    "    \"\"\"\n",
    "    skeleton = skeletonize(binary_mask)\n",
    "    cv2.imwrite(f'skeleton{i}.png', skeleton.astype(np.uint8) * 255)\n",
    "\n",
    "    return skeleton\n",
    "\n",
    "    \n",
    "\n",
    "def draw_skeleton(image, skeleton_coords, color=(0, 255, 0), thickness=1):\n",
    "    \"\"\"\n",
    "    Draws the skeleton on the image.\n",
    "    skeleton_coords: List of (y, x) points representing the skeleton.\n",
    "    color: The color to draw the skeleton (in BGR format).\n",
    "    thickness: The thickness of the lines representing the skeleton.\n",
    "    \"\"\"\n",
    "    for i in range(len(skeleton_coords) - 1):\n",
    "        start_point = (skeleton_coords[i][1], skeleton_coords[i][0])  # (x, y)\n",
    "        end_point = (skeleton_coords[i+1][1], skeleton_coords[i+1][0])  # (x, y)\n",
    "        cv2.line(image, start_point, end_point, color=color, thickness=thickness)\n",
    "    return image\n",
    "\n",
    "def draw_longest_path(image, longest_path, color=(0, 0, 255), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws the longest path on the image in a different color.\n",
    "    longest_path: List of (y, x) points representing the longest path.\n",
    "    color: The color to draw the longest path (in BGR format).\n",
    "    thickness: The thickness of the lines representing the longest path.\n",
    "    \"\"\"\n",
    "    for i in range(len(longest_path) - 1):\n",
    "        start_point = (longest_path[i][1], longest_path[i][0])  # (x, y)\n",
    "        end_point = (longest_path[i+1][1], longest_path[i+1][0])  # (x, y)\n",
    "        cv2.line(image, start_point, end_point, color=color, thickness=thickness)\n",
    "    return image\n",
    "\n",
    "def find_longest_path(skeleton_coords):\n",
    "    \"\"\"\n",
    "    Finds the longest path in the skeleton using graph analysis.\n",
    "    skeleton_coords: List of (y, x) points representing the skeleton.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for y, x in skeleton_coords:\n",
    "        G.add_node((y, x))  # Add node at each skeleton point\n",
    "        for dy in [-1, 0, 1]:\n",
    "            for dx in [-1, 0, 1]:\n",
    "                if dy == 0 and dx == 0:\n",
    "                    continue  # Skip the current point itself\n",
    "                neighbor = (y + dy, x + dx)\n",
    "                if neighbor in G.nodes:\n",
    "                    G.add_edge((y, x), neighbor)\n",
    "\n",
    "    connected_components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "    largest_component = max(connected_components, key=len)\n",
    "    endpoints = [node for node, degree in dict(largest_component.degree()).items() if degree == 1]\n",
    "\n",
    "    max_length = 0\n",
    "    longest_path = []\n",
    "\n",
    "    for source in endpoints:\n",
    "        lengths = nx.single_source_shortest_path_length(largest_component, source)\n",
    "        farthest_node = max(lengths, key=lengths.get)\n",
    "        length = lengths[farthest_node]\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            longest_path = nx.shortest_path(largest_component, source, farthest_node)\n",
    "\n",
    "    return longest_path\n",
    "\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right_resized\"\n",
    "image_name = \"undistorted_GX010067_33_625.jpg_gamma.jpg\"  # Replace with the name of your image\n",
    "image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "core_name = image_name.split('.')[0]\n",
    "\n",
    "# Path to the corresponding segmentations .txt file\n",
    "txt_file_path = os.path.join(image_dir, f\"{core_name}_segmentations.txt\")  # Replace with your .txt file path\n",
    "\n",
    "# Load image and segmentations\n",
    "image = cv2.imread(image_path)\n",
    "segmentations = load_multiple_coords_from_txt(txt_file_path)\n",
    "img_height, img_width = image.shape[:2]\n",
    "\n",
    "i=0\n",
    "# Process each segmentation individually\n",
    "for coords in segmentations:\n",
    "   \n",
    "    binary_mask = create_filled_binary_mask(coords, img_height, img_width)\n",
    "    skeleton = skeletonize_mask(binary_mask,i)\n",
    "    skeleton_coords = np.column_stack(np.nonzero(skeleton))\n",
    "    longest_path = find_longest_path(skeleton_coords)\n",
    "    \n",
    "    # Draw skeleton and longest path\n",
    "    image = draw_skeleton(image, skeleton_coords)\n",
    "    image = draw_longest_path(image, longest_path)\n",
    "    \n",
    "# Save or display the final output image as needed\n",
    "cv2.imwrite('output_image_with_skeleton_and_longest_path.png', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right_resized\"\n",
    "image_name = \"undistorted_GX010067_33_625.jpg_gamma.jpg\"  # Replace with the name of your image\n",
    "image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "core_name = image_name.split('.')[0]\n",
    "\n",
    "# Path to the corresponding segmentations .txt file\n",
    "txt_file_path = os.path.join(image_dir, f\"{core_name}_segmentations.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010152_36_378-jpg_gamma_jpg.rf.dde16353cb779b5cc6dabcd884e15547.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010152_36_378.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010155_18_219-jpg_gamma_jpg.rf.9057ffadeef9e786b3936806a3c9f36a.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010155_18_219.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010155_78_1170-jpg_gamma_jpg.rf.e004afd947d9bd621eb3843251abf362.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010155_78_1170.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010157_174_2582-jpg_gamma_jpg.rf.17f5d5766741fd481089300d1ad169b3.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010157_174_2582.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010157_177_2665-jpg_gamma_jpg.rf.0261ef6c41f32527ede8abb830f0c5b4.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010157_177_2665.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010161_136_2267-jpg_gamma-jpg_gamma_jpg.rf.d9954b4dcab4ac449c4210d47558d8cd.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010161_136_2267.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010161_54_676-jpg_gamma-jpg_gamma_jpg.rf.a76e2090f1504f2f3346505c8b6695c0.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010161_54_676.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010162_72_927-jpg_gamma_jpg.rf.1dc73de36406e10e543ee54034d31aa1.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010162_72_927.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010173_75_922-jpg_gamma_jpg.rf.9d2e8d784d1003aeb5785562ebf6b3ec.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010173_75_922.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010174_62_790-jpg_gamma_jpg.rf.751d4c5f73e63333ab2801a80cec7d5a.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010174_62_790.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010175_215_2644-jpg_gamma_jpg.rf.a5a6a089316911ffa24b620e9bd4b526.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010175_215_2644.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010175_266_3372-jpg_gamma-jpg_gamma_jpg.rf.382de27143dbd418f1c511227a90dc1e.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010175_266_3372.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010175_82_852-jpg_gamma_jpg.rf.c5143bb908a8a828b1ee6a1b774b8359.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010175_82_852.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010178_172_3604-jpg_gamma_jpg.rf.965ab8e85dd18756b57af8e5eefcd1fb.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010178_172_3604.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010178_189_3987-jpg_gamma_jpg.rf.673f87b9ad4ed71b37228f9e8391688d.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010178_189_3987.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010179_200_3927-jpg_gamma_jpg.rf.b3587f8a5710fe5c4f16e3e5ac4fb29f.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010179_200_3927.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010179_88_1697-jpg_gamma_jpg.rf.7d40fb8be30a514dac7b802056400240.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010179_88_1697.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010183_128_2852-jpg_gamma_jpg.rf.3fb084ffce7052947955101eb632c13f.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010183_128_2852.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010183_37_685-jpg_gamma_jpg.rf.bfcae27938d603211d8a7d9959bf39d2.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010183_37_685.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010183_80_1633-jpg_gamma_jpg.rf.40bb3ba57d7f48e08d3d9d1de5bf864c.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010183_80_1633.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010157_160_2259.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010157_160.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010157_68_883.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010157_68.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010169_121_1249.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010169_121.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010174_104_1236.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010174_104.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010177_232_3047.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010177_232.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010178_107_2118.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010178_107.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010178_114_2258.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010178_114.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010180_91_1563.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010180_91.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010180_99_1788.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010180_99.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010181_101_1542.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010181_101.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\"\n",
    "yolo_dir = r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict19\\labels\"\n",
    "\n",
    "# Helper function to extract the core image name\n",
    "def extract_core_image_name(image_name):\n",
    "    # Remove everything after the first hyphen\n",
    "    core_name = image_name.split('-')[0]\n",
    "    \n",
    "    # Split the core name by underscores, and capture the first three parts\n",
    "    parts = core_name.split('_')\n",
    "    \n",
    "    # Rebuild the core name using the first three parts\n",
    "    core_name = '_'.join(parts[:3])\n",
    "    \n",
    "    return core_name\n",
    "\n",
    "# Helper function to find the matching YOLO file\n",
    "def find_matching_yolo_file(core_name, yolo_dir):\n",
    "    for yolo_file in os.listdir(yolo_dir):\n",
    "        if core_name in yolo_file:  # Check if the core name is part of the YOLO file name\n",
    "            return os.path.join(yolo_dir, yolo_file)\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Helper function to rename images\n",
    "def rename_images(image_dir, yolo_dir):\n",
    "    for image_name in os.listdir(image_dir):\n",
    "        if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            \n",
    "            # Extract the core name from the image filename\n",
    "            core_name = extract_core_image_name(image_name)\n",
    "            \n",
    "            # Find the matching YOLO file\n",
    "            yolo_file_path = find_matching_yolo_file(core_name, yolo_dir)\n",
    "            \n",
    "            if yolo_file_path:\n",
    "                # Create the new image name using the core name\n",
    "                new_image_name = f\"undistorted_{core_name}.jpg\"\n",
    "                new_image_path = os.path.join(image_dir, new_image_name)\n",
    "                \n",
    "                # Rename the image\n",
    "                print(f\"Renaming {image_path} to {new_image_path}\")\n",
    "                os.rename(image_path, new_image_path)\n",
    "            else:\n",
    "                print(f\"No matching YOLO file for {image_name}, skipping...\")\n",
    "\n",
    "# Run the renaming process\n",
    "rename_images(image_dir, yolo_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # current carapace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\\carapace\\right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:01<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\\carapace\\left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 25.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\\carapace\\car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:00<00:00, 35.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ground truth found for undistorted_GX010161_136_2267.jpg_gamma.jpg_gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:00<00:00, 39.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ground truth found for undistorted_GX010169_121_1249\n",
      "No ground truth found for undistorted_GX010174_62_790.jpg_gamma\n",
      "No ground truth found for undistorted_GX010175_266_3372.jpg_gamma.jpg_gamma\n",
      "No ground truth found for undistorted_GX010175_82_852.jpg_gamma\n",
      "No ground truth found for undistorted_GX010177_232_3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:00<00:00, 52.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ground truth found for undistorted_GX010178_114_2258\n",
      "No ground truth found for undistorted_GX010178_172_3604.jpg_gamma\n",
      "No ground truth found for undistorted_GX010179_200_3927.jpg_gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 40.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5154/?notebook=True&subscription=acc58b7f-223f-452a-a831-cdff34d04533\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20bb7391910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main.py\n",
    "import fiftyone as fo\n",
    "import os\n",
    "from data_loader import load_data, create_dataset, process_images\n",
    "from utils import parse_pose_estimation, calculate_euclidean_distance, calculate_real_width, extract_identifier_from_gt\n",
    "# from measurements.metrics import  compute_dataset_map\n",
    "\n",
    "\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "# Reload the module\n",
    "reload(sys.modules['data_loader'])\n",
    "reload(sys.modules['utils'])\n",
    "\n",
    "\n",
    "# Paths to data files\n",
    "filtered_data_file_path = r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\src\\measurement\\ImageJ\\Filtered_Data.csv'\n",
    "metadata_file_path = r\"C:\\Users\\gbo10\\OneDrive\\research\\thesis and paper\\test images.xlsx\"\n",
    "right_folder_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right'\n",
    "right_prediction_folder_path = r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict35\\labels\"\n",
    "right_ground_truth_folder_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\current from roboflow -28.8-first sense\\measurement\\test\\test-right\\valid\\labels\"\n",
    "\n",
    "left_folder_path=r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left'\n",
    "left_prediction_folder_path=r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict36\\labels\"\n",
    "left_ground_truth_folder_path=r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\current from roboflow -28.8-first sense\\measurement\\test\\test-left\\valid\\labels\"\n",
    "\n",
    "car_folder_path=r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\car'\n",
    "car_prediction_folder_path=r'C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict38\\labels'\n",
    "car_ground_truth_folder_path=r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\current from roboflow -28.8-first sense\\measurement\\test\\test-square\\valid\\labels'\n",
    "\n",
    "\n",
    "# Load data and metadata\n",
    "filtered_df, metadata_df = load_data(filtered_data_file_path, metadata_file_path)\n",
    "\n",
    "# Create FiftyOne dataset\n",
    "dataset = create_dataset()\n",
    "\n",
    "for folder_path, prediction_folder_path, ground_truth_folder_path in zip([right_folder_path, left_folder_path, car_folder_path],\n",
    "                                                                         [right_prediction_folder_path, left_prediction_folder_path, car_prediction_folder_path],\n",
    "                                                                         [right_ground_truth_folder_path, left_ground_truth_folder_path, car_ground_truth_folder_path]):\n",
    "    # Get image paths\n",
    "    image_paths = [os.path.join(folder_path, img) for img in os.listdir(folder_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    # Get ground truth paths\n",
    "    ground_truth_paths = [os.path.join(ground_truth_folder_path, txt) for txt in os.listdir(ground_truth_folder_path) if txt.endswith('.txt')]\n",
    "\n",
    "    prediction_paths=[os.path.join(prediction_folder_path, txt) for txt in os.listdir(prediction_folder_path) if txt.endswith('.txt')]\n",
    "\n",
    "    \n",
    "    # Process images and predictions using segmentations and bounding boxes\n",
    "    pond_tag=folder_path.split('/')[-1]\n",
    "    print(pond_tag)\n",
    "    process_images(image_paths=image_paths,ground_truth_paths_text=ground_truth_paths,prediction_folder_path= prediction_folder_path,filtered_df= filtered_df,metadata_df= metadata_df,dataset= dataset,pond_type=pond_tag)\n",
    "\n",
    "# Process images and predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start the FiftyOne session\n",
    "session = fo.launch_app(dataset, port=5154)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\fifty_one', 'C:\\\\Users\\\\gbo10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\python39.zip', 'C:\\\\Users\\\\gbo10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\DLLs', 'C:\\\\Users\\\\gbo10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib', 'C:\\\\Users\\\\gbo10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv', '', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\fifty_one']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add swimming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36048\\591749776.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36048\\591749776.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(updated_df, swimming_df, threshold)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mupdated_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprawn_bbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BoundingBox_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Extract the bounding box of the prawn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Get the corresponding rows from swimming_df based on image Label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mmatching_swimming_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswimming_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mswimming_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Check each matching row in swimming_df to see if the bounding boxes are close enough\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswim_row\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatching_swimming_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Helper function to calculate Euclidean distance between two bounding box points\n",
    "def calculate_euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Function to check if the bounding boxes are close enough based on a distance threshold\n",
    "def find_closest_swimming_bbox(prawn_bbox, swimming_rows, threshold=50):\n",
    "    prawn_point = (prawn_bbox[0], prawn_bbox[1])  # Top-left corner of the prawn bounding box\n",
    "    min_distance = float('inf')\n",
    "    closest_swimming_bbox = None\n",
    "\n",
    "    # Iterate through all swimming bounding boxes\n",
    "    for _, swim_row in swimming_rows.iterrows():\n",
    "        swim_point = (swim_row['BX'], swim_row['BY'])  # Top-left corner of the swimming bounding box\n",
    "\n",
    "        # Calculate the Euclidean distance\n",
    "        distance = calculate_euclidean_distance(prawn_point, swim_point)\n",
    "\n",
    "        # If the distance is smaller than the current minimum, update the closest match\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_swimming_bbox = swim_row\n",
    "\n",
    "    # Return True if a close enough match is found, otherwise False\n",
    "    return min_distance < threshold\n",
    "\n",
    "# Add the swimming status to the updated dataframe\n",
    "def add_swimming_status(updated_df, swimming_df, threshold=50):\n",
    "    # Initialize the swimming column with default value \"not swimming\"\n",
    "    updated_df['swimming'] = 'not swimming'\n",
    "\n",
    "    # Iterate over each prawn in updated_df\n",
    "    for i, row in updated_df.iterrows():\n",
    "        prawn_label = row['Label'].split(':')[1]  # Extract the part after 'carapace:' from prawn label\n",
    "        prawn_bbox = ast.literal_eval(row['BoundingBox_1'])  # Convert bounding box string to a tuple\n",
    "\n",
    "        # Get corresponding rows from swimming_df with the same Label\n",
    "        matching_swimming_rows = swimming_df[swimming_df['Label'].apply(lambda x: x.split(':')[1]) == prawn_label]\n",
    "\n",
    "        # Check each matching row in swimming_df to see if the bounding boxes are close enough\n",
    "        if not matching_swimming_rows.empty:\n",
    "            if find_closest_swimming_bbox(prawn_bbox, matching_swimming_rows, threshold):\n",
    "                # If a close match is found, mark the prawn as swimming\n",
    "                updated_df.at[i, 'swimming'] = 'swimming'\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "# Load the updated dataframe and the swimming dataframe\n",
    "updated_df = pd.read_excel(r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\fifty_one\\measurements\\Updated_Filtered_Data_with_real_length.xlsx')\n",
    "swimming_df = pd.read_excel(r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\src\\measurement\\ImageJ\\Swimming.xlsx')\n",
    "\n",
    "# Add the swimming status column\n",
    "updated_df_with_swimming = add_swimming_status(updated_df, swimming_df)\n",
    "\n",
    "# Save the updated dataframe to a new Excel file\n",
    "updated_df_with_swimming.to_excel('Updated_Filtered_Data_with_Swimming.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from random import randint, shuffle\n",
    "import fiftyone as fo\n",
    "import fiftyone.core.labels as fol\n",
    "\n",
    "# Helper Classes and Functions for Circle Calculation\n",
    "class Point:\n",
    "    def __init__(self, X=0, Y=0) -> None:\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "class Circle:\n",
    "    def __init__(self, c=Point(), r=0) -> None:    \n",
    "        self.C = c\n",
    "        self.R = r\n",
    "\n",
    "def dist(a, b):\n",
    "    return sqrt((a.X - b.X) ** 2 + (a.Y - b.Y) ** 2)\n",
    "\n",
    "def is_inside(c, p):\n",
    "    return dist(c.C, p) <= c.R\n",
    "\n",
    "def get_circle_center(bx, by, cx, cy):\n",
    "    B = bx * bx + by * by\n",
    "    C = cx * cx + cy * cy\n",
    "    D = bx * cy - by * cx\n",
    "    return Point((cy * B - by * C) / (2 * D), (bx * C - cx * B) / (2 * D))\n",
    "\n",
    "def circle_from1(A, B):\n",
    "    C = Point((A.X + B.X) / 2.0, (A.Y + B.Y) / 2.0)\n",
    "    return Circle(C, dist(A, B) / 2.0)\n",
    "\n",
    "def circle_from2(A, B, C):\n",
    "    I = get_circle_center(B.X - A.X, B.Y - A.Y, C.X - A.X, C.Y - A.Y)\n",
    "    I.X += A.X\n",
    "    I.Y += A.Y\n",
    "    return Circle(I, dist(I, A))\n",
    "\n",
    "def is_valid_circle(c, P):\n",
    "    for p in P:\n",
    "        if not is_inside(c, p):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def min_circle_trivial(P):\n",
    "    assert len(P) <= 3\n",
    "    if not P:\n",
    "        return Circle()\n",
    "    elif len(P) == 1:\n",
    "        return Circle(P[0], 0)\n",
    "    elif len(P) == 2:\n",
    "        return circle_from1(P[0], P[1])\n",
    "    for i in range(3):\n",
    "        for j in range(i + 1, 3):\n",
    "            c = circle_from1(P[i], P[j])\n",
    "            if is_valid_circle(c, P):\n",
    "                return c\n",
    "    return circle_from2(P[0], P[1], P[2])\n",
    "\n",
    "def welzl_helper(P, R, n):\n",
    "    if n == 0 or len(R) == 3:\n",
    "        return min_circle_trivial(R)\n",
    "    idx = randint(0, n - 1)\n",
    "    p = P[idx]\n",
    "    P[idx], P[n - 1] = P[n - 1], P[idx]\n",
    "    d = welzl_helper(P, R.copy(), n - 1)\n",
    "    if is_inside(d, p):\n",
    "        return d\n",
    "    R.append(p)\n",
    "    return welzl_helper(P, R.copy(), n - 1)\n",
    "\n",
    "def welzl(P):\n",
    "    P_copy = P.copy()\n",
    "    shuffle(P_copy)\n",
    "    return welzl_helper(P_copy, [], len(P_copy))\n",
    "\n",
    "# Helper function to load segmentations from a .txt file\n",
    "def load_segmentations_from_txt(txt_file):\n",
    "    \"\"\"\n",
    "    Loads segmentation polygons from a .txt file without scaling the coordinates.\n",
    "    Each line in the file contains space-separated coordinates: x1 y1 x2 y2 ... xn yn.\n",
    "    Assumes coordinates are already scaled to 640x640.\n",
    "    \"\"\"\n",
    "    segmentations = []\n",
    "    with open(txt_file, 'r') as file:\n",
    "        for line in file:\n",
    "            coords = list(map(float, line.strip().split()))  # Read all floats from the line\n",
    "            polygon = [(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]  # Directly read the coordinates\n",
    "            segmentations.append(polygon)\n",
    "    return segmentations\n",
    "\n",
    "\n",
    "# Initialize FiftyOne dataset\n",
    "dataset = fo.Dataset(\"prawn_segmentation\", overwrite=True)\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "# Assuming image size is always 640x640 based on your .txt files\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\"\n",
    "segmentation_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\" # Directory with the segmentation .txt files\n",
    "\n",
    "# Iterate over images\n",
    "for image_name in os.listdir(image_dir):\n",
    "    if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "       \n",
    "        core_name = image_name.split('.')[0]\n",
    "        # Create the corresponding segmentation txt file path\n",
    "        segmentation_file = os.path.join(segmentation_dir, f\"{core_name}_segmentations.txt\")\n",
    "        \n",
    "        if not os.path.exists(segmentation_file):\n",
    "            print(f\"Segmentation file not found for {core_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Load image (assuming it's also 640x640)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Load segmentations from the .txt file (no scaling needed)\n",
    "        all_polygons = load_segmentations_from_txt(segmentation_file)\n",
    "        \n",
    "        # Initialize list to store segmentations and attributes\n",
    "        all_attributes = []\n",
    "        for polygon in all_polygons:\n",
    "            points = [Point(p[0], p[1]) for p in polygon]\n",
    "            mec = welzl(points)  # Compute the minimum enclosing circle\n",
    "            diameter = mec.R * 2  # Get the diameter of the circle\n",
    "            all_attributes.append({'diameter': diameter})\n",
    "        \n",
    "        # Create Fift   yOne polylines\n",
    "        segmentations = []\n",
    "\n",
    "        for polygon, diameter in zip(all_polygons, all_attributes):\n",
    "            normalized_points = [[(x / 640.0, y / 640.0) for x, y in polygon]]\n",
    "            \n",
    "            segmentation = fo.Polyline(\n",
    "                points=normalized_points,\n",
    "                closed=True,\n",
    "                filled=True,\n",
    "                diameter=diameter['diameter']\n",
    "            )\n",
    "            segmentations.append(segmentation)\n",
    "\n",
    "        \n",
    "        # Add image and segmentation to FiftyOne dataset\n",
    "        sample = fo.Sample(filepath=image_path)\n",
    "        sample[\"segmentations\"] = fol.Polylines(polylines=segmentations)\n",
    "\n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "# Save dataset and launch FiftyOne app\n",
    "dataset.save()\n",
    "session = fo.launch_app(dataset, port=5153)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:01<00:00, 15.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No segmentation file found for undistorted_GX010175_215_2644.jpg_gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved the updated filtered data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved the updated filtered data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:07<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved the updated filtered data.\n",
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Client is not connected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     process_images(image_paths, prediction_folder_path, filtered_df, metadata_df, dataset,pond_tag\u001b[38;5;241m=\u001b[39mpond_tag)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Start the FiftyOne session\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5153\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:196\u001b[0m, in \u001b[0;36mlaunch_app\u001b[1;34m(dataset, view, spaces, color_scheme, plots, port, address, remote, desktop, browser, height, auto, config)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Launches the FiftyOne App.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mNote that only one App instance can be opened at a time. If this method is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    a :class:`Session`\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _session  \u001b[38;5;66;03m# pylint: disable=global-statement\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m _session \u001b[38;5;241m=\u001b[39m \u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_scheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_scheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesktop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesktop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrowser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _session\u001b[38;5;241m.\u001b[39mremote:\n\u001b[0;32m    213\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(_REMOTE_INSTRUCTIONS\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mformat(_session\u001b[38;5;241m.\u001b[39mserver_port))\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:435\u001b[0m, in \u001b[0;36mSession.__init__\u001b[1;34m(self, dataset, view, view_name, spaces, color_scheme, plots, port, address, remote, desktop, browser, height, auto, config)\u001b[0m\n\u001b[0;32m    432\u001b[0m _register_session(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_height\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser \u001b[38;5;241m=\u001b[39m browser\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremote:\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:256\u001b[0m, in \u001b[0;36mupdate_state.<locals>.decorator.<locals>.wrapper\u001b[1;34m(session, *args, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_show \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n\u001b[0;32m    255\u001b[0m     session\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[1;32m--> 256\u001b[0m result \u001b[38;5;241m=\u001b[39m func(session, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    257\u001b[0m session\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend_event(StateUpdate(state\u001b[38;5;241m=\u001b[39msession\u001b[38;5;241m.\u001b[39m_state))\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_show \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:1043\u001b[0m, in \u001b[0;36mSession.show\u001b[1;34m(self, height)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesktop:\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m_reload()\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:1144\u001b[0m, in \u001b[0;36mSession.freeze\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1141\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly notebook sessions can be frozen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDeactivateNotebookCell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplots\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\client.py:152\u001b[0m, in \u001b[0;36mClient.send_event\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient is not connected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_event(event)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_event(event)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Client is not connected"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "from data_loader import load_data, create_dataset, process_images\n",
    "from utils import calculate_euclidean_distance, calculate_real_width, extract_identifier_from_gt\n",
    "from data_loader_body import load_data, create_dataset, process_images\n",
    "import MEC\n",
    "import skeletonization\n",
    "\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "# Reload the necessary modules\n",
    "reload(sys.modules['data_loader_body'])\n",
    "reload(sys.modules['utils'])\n",
    "reload(sys.modules['MEC'])\n",
    "reload(sys.modules['skeletonization'])\n",
    "\n",
    "# Paths to data files\n",
    "filtered_data_file_path = r\"C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\src\\measurement\\ImageJ\\final_full_statistics_with_prawn_ids_and_uncertainty - Copy.xlsx\"\n",
    "metadata_file_path = r\"C:\\Users\\gbo10\\OneDrive\\research\\thesis and paper\\test images.xlsx\"\n",
    "car_image_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\car'\n",
    "car_prediction_folder_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\car_resized\"\n",
    "\n",
    "left_image_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left'\n",
    "left_prediction_folder_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\"\n",
    "\n",
    "right_image_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right'\n",
    "right_prediction_folder_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right_resized\"\n",
    "\n",
    "# Load data and metadata\n",
    "filtered_df, metadata_df = load_data(filtered_data_file_path, metadata_file_path)\n",
    "# Create the FiftyOne dataset\n",
    "molt_image_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted\"\n",
    "molt_prediction=r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\molt-19-9\\unditorted_resized'\n",
    "\n",
    "\n",
    "dataset = create_dataset()\n",
    "\n",
    "\n",
    "for folder_path, prediction_folder_path in [(car_image_path, car_prediction_folder_path), (left_image_path, left_prediction_folder_path), (right_image_path, right_prediction_folder_path)]:\n",
    "    image_paths = [os.path.join(folder_path, image) for image in os.listdir(folder_path) if image.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "    prediction_paths_text = [os.path.join(prediction_folder_path, txt) for txt in os.listdir(prediction_folder_path) if txt.endswith('.txt')]\n",
    "\n",
    "    # Process images and predictions using segmentations and bounding boxes\n",
    "    pond_tag=folder_path.split('/')[-1]\n",
    "\n",
    "    process_images(image_paths, prediction_folder_path, filtered_df, metadata_df, dataset,pond_tag=pond_tag)\n",
    "\n",
    "# Start the FiftyOne session\n",
    "session = fo.launch_app(dataset, port=5153)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
