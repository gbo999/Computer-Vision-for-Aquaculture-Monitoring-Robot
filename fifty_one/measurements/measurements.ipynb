{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load the keypoints to fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils as fou\n",
    "import os\n",
    "\n",
    "# Assuming you have a list of image paths and corresponding TXT file paths\n",
    "image_paths = [...]  # List of image file paths\n",
    "txt_paths = [...]  # List of corresponding TXT file paths for each image\n",
    "\n",
    "# Create a FiftyOne dataset\n",
    "dataset = fo.Dataset(\"prawn_pose_estimation\")\n",
    "def parse_pose_estimation(txt_file):\n",
    "    \"\"\"\n",
    "    Parse the pose estimation data from a TXT file.\n",
    "\n",
    "    Parameters:\n",
    "    txt_file (str): Path to the TXT file.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of pose estimation data (each line parsed into a list of values).\n",
    "    \"\"\"\n",
    "    pose_estimations = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            pose_estimations.append([float(x) for x in line.strip().split()])\n",
    "    return pose_estimations\n",
    "\n",
    "# Loop over images and corresponding TXT files\n",
    "for img_path, txt_path in zip(image_paths, txt_paths):\n",
    "    # Parse the pose estimation data from the TXT file\n",
    "    pose_estimations = parse_pose_estimation(txt_path)\n",
    "    \n",
    "    # List to hold all detections and keypoints for the current image\n",
    "    detections = []\n",
    "    \n",
    "    for pose in pose_estimations:\n",
    "        # Extract and scale bounding box and keypoints\n",
    "        x_center_scaled = pose[1] * 5312\n",
    "        y_center_scaled = pose[2] * 2988\n",
    "        width_scaled = pose[3] * 5312\n",
    "        height_scaled = pose[4] * 2988\n",
    "\n",
    "        keypoints = []\n",
    "        for i in range(5, len(pose), 3):\n",
    "            x_kp_scaled = pose[i] * 5312\n",
    "            y_kp_scaled = pose[i + 1] * 2988\n",
    "            confidence = pose[i + 2]\n",
    "            keypoints.append(fo.Keypoint(point=[x_kp_scaled, y_kp_scaled], confidence=confidence))\n",
    "\n",
    "        bounding_box = [\n",
    "            (x_center_scaled - width_scaled / 2) / 5312,\n",
    "            (y_center_scaled - height_scaled / 2) / 2988,\n",
    "            width_scaled / 5312,\n",
    "            height_scaled / 2988\n",
    "        ]\n",
    "\n",
    "        # Create a detection object\n",
    "        detection = fo.Detection(label=\"prawn\", bounding_box=bounding_box, keypoints=keypoints)\n",
    "        detections.append(detection)\n",
    "\n",
    "    # Create a sample for FiftyOne\n",
    "    sample = fo.Sample(filepath=img_path)\n",
    "    sample[\"detections\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "# Launch the FiftyOne app to visualize\n",
    "session = fo.launch_app(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. load metadata from excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fiftyone as fo\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'your_file_path.xlsx'  # Replace with your actual file path\n",
    "metadata_df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it's loaded correctly\n",
    "print(metadata_df.head())\n",
    "\n",
    "\n",
    "# Assuming you have a list of image paths\n",
    "image_paths = [...]  # List of image file paths\n",
    "\n",
    "# Create a FiftyOne dataset\n",
    "dataset = fo.Dataset(\"prawn_metadata\")\n",
    "\n",
    "# Loop through each image and add metadata\n",
    "for img_path in image_paths:\n",
    "    # Extract the filename without extension\n",
    "    filename = img_path.split('/')[-1].replace('.jpg', '')  # Adjust if not .jpg\n",
    "    \n",
    "    # Match the filename with the metadata\n",
    "    metadata_row = metadata_df[metadata_df['file name'] == filename]\n",
    "    \n",
    "    if not metadata_row.empty:\n",
    "        metadata = metadata_row.iloc[0].to_dict()\n",
    "        \n",
    "        # Create a sample and attach metadata\n",
    "        sample = fo.Sample(filepath=img_path)\n",
    "        \n",
    "        # Add each metadata field to the sample\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'file name':  # Exclude the file name itself\n",
    "                sample[key] = value\n",
    "        \n",
    "        # Add the sample to the dataset\n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "# Launch the FiftyOne app to visualize\n",
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.bounding rectangle between detection and imagej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load ImageJ bounding rectangles from a CSV file\n",
    "imagej_bboxes_df = pd.read_csv('imagej_bboxes.csv')\n",
    "\n",
    "# Example structure of `imagej_bboxes_df`:\n",
    "# Columns: ['file name', 'x1', 'y1', 'x2', 'y2']\n",
    "def calculate_bbox_distance(detection_bbox, imagej_bbox):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between the centers of two bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    detection_bbox (tuple): (x1, y1, x2, y2) coordinates of the detection bounding box.\n",
    "    imagej_bbox (tuple): (x1, y1, x2, y2) coordinates of the ImageJ bounding box.\n",
    "\n",
    "    Returns:\n",
    "    float: Euclidean distance between the centers of the bounding boxes.\n",
    "    \"\"\"\n",
    "    # Calculate centers\n",
    "    detection_center = ((detection_bbox[0] + detection_bbox[2]) / 2, (detection_bbox[1] + detection_bbox[3]) / 2)\n",
    "    imagej_center = ((imagej_bbox[0] + imagej_bbox[2]) / 2, (imagej_bbox[1] + imagej_bbox[3]) / 2)\n",
    "    \n",
    "    # Calculate Euclidean distance between centers\n",
    "    distance = np.sqrt((detection_center[0] - imagej_center[0])**2 + (detection_center[1] - imagej_center[1])**2)\n",
    "    return distance\n",
    "for img_path in image_paths:\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "    # Assume `detections` is a list of bounding boxes from the model\n",
    "    detection_bboxes = [...]  # Replace with actual detection bounding boxes\n",
    "    \n",
    "    # Match with ImageJ bounding box\n",
    "    imagej_bbox = imagej_bboxes_df[imagej_bboxes_df['file name'] == filename].iloc[0]\n",
    "    imagej_bbox = (imagej_bbox['x1'], imagej_bbox['y1'], imagej_bbox['x2'], imagej_bbox['y2'])\n",
    "    \n",
    "    distances = []\n",
    "    for detection_bbox in detection_bboxes:\n",
    "        distance = calculate_bbox_distance(detection_bbox, imagej_bbox)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    # Create a FiftyOne sample and add metadata\n",
    "    sample = fo.Sample(filepath=img_path)\n",
    "    sample['bbox_distances'] = distances  # Store all distances for this image\n",
    "    \n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "# Launch FiftyOne app to visualize\n",
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. calculate length based on pinhole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_real_width(focal_length, distance_to_object, width_in_pixels, pixel_size):\n",
    "    \"\"\"\n",
    "    Calculate the real-life width of an object.\n",
    "\n",
    "    Parameters:\n",
    "    focal_length (float): Focal length of the camera lens in millimeters (mm).\n",
    "    distance_to_object (float): Distance from the camera to the object in millimeters (mm).\n",
    "    width_in_pixels (int): Width of the object in pixels on the image sensor.\n",
    "    pixel_size (float): Size of a pixel on the image sensor in millimeters (mm).\n",
    "\n",
    "    Returns:\n",
    "    float: Real-life width of the object in centimeters (cm).\n",
    "    \"\"\"\n",
    "    # Calculate the width of the object in the image sensor plane in millimeters\n",
    "    width_in_sensor = width_in_pixels * pixel_size\n",
    "\n",
    "    # Calculate the real-life width of the object using the similar triangles principle\n",
    "    real_width_mm = (width_in_sensor * distance_to_object) / focal_length\n",
    "\n",
    "    # Convert the width from millimeters to centimeters\n",
    "    real_width_cm = real_width_mm / 10.0\n",
    "\n",
    "    return real_width_cm\n",
    "\n",
    "def calculate_euclidean_distance(keypoint1, keypoint2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    keypoint1, keypoint2: Tuples of (x, y) coordinates of the keypoints in pixels.\n",
    "\n",
    "    Returns:\n",
    "    float: Euclidean distance between the keypoints in pixels.\n",
    "    \"\"\"\n",
    "    return np.sqrt((keypoint1[0] - keypoint2[0])**2 + (keypoint1[1] - keypoint2[1])**2)\n",
    "\n",
    "# Example of how to integrate this with your FiftyOne dataset\n",
    "for img_path in image_paths:\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    metadata_row = metadata_df[metadata_df['file name'] == filename]\n",
    "    \n",
    "    if not metadata_row.empty:\n",
    "        metadata = metadata_row.iloc[0].to_dict()\n",
    "        sample = fo.Sample(filepath=img_path)\n",
    "        \n",
    "        # Add metadata\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'file name':\n",
    "                sample[key] = value\n",
    "        \n",
    "        # Calculate the real height or width if keypoints and necessary metadata are available\n",
    "        if 'height(mm)' in metadata:\n",
    "            height_mm = metadata['height(mm)']\n",
    "            focal_length = 6.82  # Example focal length in mm\n",
    "            pixel_size = 0.0014  # Example pixel size in mm\n",
    "            keypoints = [...]  # Replace with actual keypoints from your data\n",
    "            \n",
    "            if len(keypoints) >= 2:\n",
    "                # Calculate the Euclidean distance in pixels\n",
    "                euclidean_distance_pixels = calculate_euclidean_distance(keypoints[0], keypoints[1])\n",
    "                \n",
    "                # Calculate the real width/height in centimeters\n",
    "                real_width_cm = calculate_real_width(focal_length, height_mm, euclidean_distance_pixels, pixel_size)\n",
    "                \n",
    "                # Attach the calculated real width/height to the sample\n",
    "                sample[\"real_width_cm\"] = real_width_cm\n",
    "        \n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# complete maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = fo.list_datasets()\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    if dataset_name.startswith('prawn_combined_dataset'):\n",
    "        fo.delete_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the existing filtered data with PrawnIDs\n",
    "filtered_data_file_path = r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\src\\measurement\\ImageJ\\Filtered_Data.csv'  # Replace with your actual file path\n",
    "filtered_df = pd.read_csv(filtered_data_file_path)\n",
    "\n",
    "# Load the additional metadata from another Excel file\n",
    "metadata_file_path = r\"C:\\Users\\gbo10\\OneDrive\\research\\thesis and paper\\test images.xlsx\"  # Replace with your actual file path\n",
    "metadata_df = pd.read_excel(metadata_file_path)\n",
    "\n",
    "# Function to parse pose estimation data from a TXT file\n",
    "def parse_pose_estimation(txt_file):\n",
    "    pose_estimations = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            pose_estimations.append([float(x) for x in line.strip().split()])\n",
    "    return pose_estimations\n",
    "\n",
    "# Function to calculate Euclidean distance between keypoints\n",
    "def calculate_euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "# Function to calculate the real-life width/height\n",
    "def calculate_real_width(focal_length, distance_to_object, width_in_pixels, pixel_size):\n",
    "    width_in_sensor = width_in_pixels * pixel_size\n",
    "    real_width_mm = (width_in_sensor * distance_to_object) / focal_length\n",
    "    return real_width_mm\n",
    "def extract_identifier_from_gt(filename):\n",
    "    return filename.split('-')[0]\n",
    "\n",
    "folder_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\car'  # Replace with the folder containing the images and TXT files\n",
    "\n",
    "# Assuming you have the following lists\n",
    "image_paths = [os.path.join(folder_path, image) for image in os.listdir(folder_path) if image.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "\n",
    "prediction_folder_path=r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict19\\labels\"\n",
    "\n",
    "txt_paths = [os.path.join(prediction_folder_path, txt) for txt in os.listdir(prediction_folder_path) if txt.endswith('.txt')]\n",
    "# Create a FiftyOne dataset\n",
    "ground_truth_paths = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\labels\"  # Replace with the folder containing the ground truth TXT files\n",
    "\n",
    "ground_truth_paths_text = [os.path.join(ground_truth_paths, txt) for txt in os.listdir(ground_truth_paths) if txt.endswith('.txt')]\n",
    "\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "# List all datasets in FiftyOne\n",
    "\n",
    "# Loop through the list and delete datasets that match the condition\n",
    "\n",
    "\n",
    "#delete all datasets\n",
    "\n",
    "\n",
    "# Create a new dataset\n",
    "\n",
    "dataset = fo.Dataset(\"prawn_combined_dataset22\", overwrite=True)\n",
    "\n",
    "dataset.default_skeleton = fo.KeypointSkeleton(\n",
    "    labels=[\n",
    "        \"start_carapace\",\n",
    "        \"eyes\",\n",
    "    ],\n",
    "    edges=[\n",
    "        [0, 1],  # Connect keypoint 0 to keypoint 1  # Connect keypoint 1 to keypoint 2\n",
    "        # Add more connections as needed\n",
    "    ],\n",
    ")\n",
    "# Loop over images and corresponding TXT files\n",
    "for img_path in image_paths:\n",
    "\n",
    "    # Extract the relevant part for matching\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0] \n",
    "    parts = filename.split('_')\n",
    "    relevant_part = f\"{parts[1][-3:]}_{parts[3].split('.')[0]}\"\n",
    "     # e.g., undistorted_GX010152_36_378.jpg_gamma\n",
    "    identifier = filename.replace('undistorted_', '').replace('.jpg_gamma', '')  # Extract the identifier from the filename\n",
    "\n",
    "\n",
    "    # Construct the paths to the prediction and ground truth files\n",
    "    prediction_txt_path = os.path.join(prediction_folder_path, f\"{filename}.txt\")\n",
    "\n",
    "    # Match ground truth based on the extracted identifier\n",
    "    ground_truth_txt_path = None\n",
    "    for gt_file in ground_truth_paths_text:\n",
    "        b= extract_identifier_from_gt(os.path.basename(gt_file))\n",
    "        if b == identifier:\n",
    "            ground_truth_txt_path = gt_file\n",
    "\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Parse the pose estimation data from the TXT file\n",
    "    pose_estimations = parse_pose_estimation(prediction_txt_path)\n",
    "    \n",
    "    ground_truths = parse_pose_estimation(ground_truth_txt_path)\n",
    "\n",
    "    \n",
    "    keypoints_list = []  # List to store keypoints\n",
    "    detections = []  # List to store detection bounding boxes\n",
    "    ground_truth_keypoints_list = []\n",
    "    ground_truth_detection_list = []\n",
    "\n",
    "    for pose in pose_estimations:\n",
    "        if len(pose)==11:        \n",
    "\n",
    "            x1_rel = pose[1] \n",
    "            y1_rel = pose[2] \n",
    "            width_rel = pose[3] \n",
    "            height_rel = pose[4] \n",
    "\n",
    "            \n",
    "            #center to top left\n",
    "            x1_rel = x1_rel - width_rel/2\n",
    "            y1_rel = y1_rel - height_rel/2\n",
    "        # Calc\n",
    "\n",
    "\n",
    "\n",
    "            keypoints = []\n",
    "            confidences = []\n",
    "            for i in range(5, len(pose), 3):\n",
    "                x_kp_scaled = pose[i] \n",
    "                y_kp_scaled = pose[i + 1] \n",
    "                confidence = pose[i + 2]\n",
    "                keypoints.append([x_kp_scaled, y_kp_scaled])\n",
    "            \n",
    "\n",
    "            confidences = [float(pose[i+2]) for i in range(5, len(pose), 3)]\n",
    "\n",
    "\n",
    "         \n",
    "                \n",
    "            keypoint=fo.Keypoint(points=keypoints, confidence=None)\n",
    "            keypoints_list.append(keypoint)\n",
    "\n",
    "\n",
    "            keypoints_dict= {'point1':keypoints[0],'point2':keypoints[1]}\n",
    "            detections.append(fo.Detection\n",
    "                              (label=\"prawn\", bounding_box=[x1_rel, y1_rel, width_rel, height_rel],\n",
    "                               attributes={'keypoints':keypoints_dict}))\n",
    "            \n",
    "    for gt_pose in ground_truths:\n",
    "            x1_rel = gt_pose[1]\n",
    "            y1_rel = gt_pose[2]\n",
    "            width_rel = gt_pose[3]\n",
    "            height_rel = gt_pose[4]\n",
    "\n",
    "            # Convert center to top-left\n",
    "            x1_rel = x1_rel - width_rel / 2\n",
    "            y1_rel = y1_rel - height_rel / 2\n",
    "\n",
    "            keypoints = []\n",
    "            for i in range(5, len(gt_pose), 3):\n",
    "                x_kp_scaled = gt_pose[i]\n",
    "                y_kp_scaled = gt_pose[i + 1]\n",
    "                keypoints.append([x_kp_scaled, y_kp_scaled])\n",
    "\n",
    "            ground_truth_keypoints_list.append(fo.Keypoint(points=keypoints))\n",
    "            ground_truth_detection_list.append(fo.Detection(label=\"prawn_truth\", bounding_box=[x1_rel, y1_rel, width_rel, height_rel]))\n",
    "\n",
    "    sample = fo.Sample(filepath=img_path)\n",
    "    \n",
    "    sample[\"ground_truth_keypoints\"] = fo.Keypoints(keypoints=ground_truth_keypoints_list)\n",
    "\n",
    "\n",
    "    sample[\"ground_truth_detections\"] = fo.Detections(detections=detections)   \n",
    "    # Add keypoints as a separate field\n",
    "    sample[\"keypoints\"] = fo.Keypoints(keypoints=keypoints_list)\n",
    "   \n",
    "    sample[\"detections_predictions\"] = fo.Detections(detections=detections)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Match the filename with the corresponding rows in the filtered data\n",
    "    matching_rows = filtered_df[filtered_df['Label'] == f'carapace:{filename}']\n",
    "    \n",
    "    # Add metadata from the additional Excel file\n",
    "    metadata_row = metadata_df[metadata_df['file name'] == relevant_part]\n",
    "\n",
    "    if not metadata_row.empty:\n",
    "        metadata = metadata_row.iloc[0].to_dict() \n",
    "        for key, value in metadata.items():\n",
    "            if key != 'file name': \n",
    "                sample[key] = value\n",
    "            \n",
    "                        \n",
    "    else:\n",
    "        print(f\"No metadata found for {relevant_part}\")\n",
    "    true_detections=[]\n",
    "\n",
    "    for _, row in matching_rows.iterrows():\n",
    "        prawn_id = row['PrawnID']\n",
    "        prawn_bbox = ast.literal_eval(row['BoundingBox_1'])  # Replace with the correct column name\n",
    "\n",
    "        # Convert the tuple elements to floats\n",
    "        prawn_bbox = tuple(float(coord) for coord in prawn_bbox)# Replace with correct bounding box columns\n",
    "        \n",
    "        prawn_normalized_bbox=[prawn_bbox[0]/5312, prawn_bbox[1]/2988, prawn_bbox[2]/5312, prawn_bbox[3]/2988]\n",
    "\n",
    "        # Convert PrawnID bounding box to absolute coordinates\n",
    "        \n",
    "        \n",
    "        true_detections.append(fo.Detection(label=\"prawn_true\", bounding_box=prawn_normalized_bbox))\n",
    "\n",
    "        prawn_point=(prawn_bbox[0]/5312, prawn_bbox[1]/2988)\n",
    "        \n",
    "\n",
    "        # Find the closest detection bounding box within the same image (Label)\n",
    "        min_distance = float('inf')\n",
    "        closest_detection = None\n",
    "        \n",
    "        for detection_bbox in sample[\"detections_predictions\"].detections:\n",
    "            detection_bbox_coords = detection_bbox.bounding_box\n",
    "            \n",
    "            det_point=(detection_bbox_coords[0], detection_bbox_coords[1])\n",
    "            \n",
    "            distance = calculate_euclidean_distance(prawn_point, det_point)\n",
    "            \n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_detection = detection_bbox\n",
    "        \n",
    "        if closest_detection is not None:\n",
    "            height_mm = sample['heigtht(mm)']  # Use appropriate column for height in mm\n",
    "            focal_length = 24.22\n",
    "            pixel_size = 0.00716844 \n",
    "\n",
    "                \n",
    "\n",
    "            keypoints_dict2 = closest_detection.attributes[\"keypoints\"]\n",
    "\n",
    "            keypoints1 = [keypoints_dict2['point1'], keypoints_dict2['point2']]    \n",
    "\n",
    "            keypoint1_scaled = [keypoints1[0][0] * 5312, keypoints1[0][1] * 2988]\n",
    "            keypoint2_scaled = [keypoints1[1][0] * 5312, keypoints1[1][1] * 2988]\n",
    "\n",
    "            euclidean_distance_pixels = calculate_euclidean_distance(keypoint1_scaled, keypoint2_scaled)\n",
    "            real_length_cm = calculate_real_width(focal_length, height_mm, euclidean_distance_pixels, pixel_size)\n",
    "            \n",
    "\n",
    "            # Update the filtered_df with the calculated lengths\n",
    "            filtered_df.loc[(filtered_df['Label'] == f'carapace:{filename}') & (filtered_df['PrawnID'] == prawn_id), 'RealLength(cm)'] = real_length_cm\n",
    "            \n",
    "            # Add floating text near the keypoints in FiftyOne\n",
    "\n",
    "            true_length=filtered_df.loc[(filtered_df['Label'] == f'carapace:{filename}') & (filtered_df['PrawnID'] == prawn_id), 'Avg_Length'].values[0]\n",
    "\n",
    "\n",
    "            closest_detection_label =f'MPError: {abs(real_length_cm - true_length) / true_length * 100:.2f}%, true length: {true_length:.2f}cm, pred length: {real_length_cm:.2f}cm'\n",
    "            \n",
    "            closest_detection.label=closest_detection_label\n",
    "\n",
    "\n",
    "            #if mpe>25: tag sample\n",
    "            if abs(real_length_cm - true_length) / true_length * 100 > 25:\n",
    "                if \"MPE>25\" not in sample.tags:\n",
    "                    sample.tags.append(\"MPE>25\")\n",
    "            \n",
    "    # Assuming 'sample' is your FiftyOne sample object\n",
    "        \n",
    "    sample[\"true_detections\"] = fo.Detections(detections=true_detections)            \n",
    "\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "ground_truth_count=0\n",
    "\n",
    "for sample in dataset:\n",
    "    ground_truth_count+=len(sample[\"ground_truth_detections\"].detections)\n",
    "\n",
    "print('gt',ground_truth_count)\n",
    "\n",
    "detections_predictions_count=0\n",
    "\n",
    "for sample in dataset:\n",
    "    detections_predictions_count+=len(sample[\"detections_predictions\"].detections)\n",
    "\n",
    "print('pred',detections_predictions_count)\n",
    "\n",
    "\n",
    "# Save the updated filtered data with the calculated lengths\n",
    "# filtered_df.to_excel('updated_filtered_data_with_lengths.xlsx', index=False)\n",
    "\n",
    "# Launch the FiftyOne app to visualize\n",
    "\n",
    "# Define a custom view\n",
    "\n",
    "# Configure the view to show your custom attributes\n",
    "\n",
    "\n",
    "session = fo.launch_app(dataset, port=5151)\n",
    "\n",
    "\n",
    "# Launch the app with the custom view\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x1024 1617.6ms\n",
      "Speed: 23.0ms preprocess, 1617.6ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1602.6ms\n",
      "Speed: 18.6ms preprocess, 1602.6ms inference, 8.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1428.8ms\n",
      "Speed: 19.0ms preprocess, 1428.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1512.6ms\n",
      "Speed: 22.0ms preprocess, 1512.6ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1498.9ms\n",
      "Speed: 23.9ms preprocess, 1498.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1508.2ms\n",
      "Speed: 22.5ms preprocess, 1508.2ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1320.1ms\n",
      "Speed: 19.0ms preprocess, 1320.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1388.6ms\n",
      "Speed: 20.0ms preprocess, 1388.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1483.9ms\n",
      "Speed: 21.0ms preprocess, 1483.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1378.4ms\n",
      "Speed: 23.0ms preprocess, 1378.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1368.6ms\n",
      "Speed: 19.0ms preprocess, 1368.6ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1379.5ms\n",
      "Speed: 22.0ms preprocess, 1379.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1412.8ms\n",
      "Speed: 18.0ms preprocess, 1412.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1406.2ms\n",
      "Speed: 20.0ms preprocess, 1406.2ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1392.4ms\n",
      "Speed: 21.0ms preprocess, 1392.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1465.3ms\n",
      "Speed: 19.5ms preprocess, 1465.3ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1636.5ms\n",
      "Speed: 20.0ms preprocess, 1636.5ms inference, 4.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1630.4ms\n",
      "Speed: 20.5ms preprocess, 1630.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1476.4ms\n",
      "Speed: 22.0ms preprocess, 1476.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1429.6ms\n",
      "Speed: 20.0ms preprocess, 1429.6ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1601.3ms\n",
      "Speed: 28.0ms preprocess, 1601.3ms inference, 7.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1433.9ms\n",
      "Speed: 17.0ms preprocess, 1433.9ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1392.4ms\n",
      "Speed: 20.5ms preprocess, 1392.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1430.1ms\n",
      "Speed: 18.0ms preprocess, 1430.1ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1431.5ms\n",
      "Speed: 18.0ms preprocess, 1431.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1405.5ms\n",
      "Speed: 17.5ms preprocess, 1405.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1522.7ms\n",
      "Speed: 18.0ms preprocess, 1522.7ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1660.8ms\n",
      "Speed: 17.5ms preprocess, 1660.8ms inference, 12.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1594.5ms\n",
      "Speed: 20.0ms preprocess, 1594.5ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1505.3ms\n",
      "Speed: 17.0ms preprocess, 1505.3ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Client is not connected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 174\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Save dataset\u001b[39;00m\n\u001b[0;32m    173\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m--> 174\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5153\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:196\u001b[0m, in \u001b[0;36mlaunch_app\u001b[1;34m(dataset, view, spaces, color_scheme, plots, port, address, remote, desktop, browser, height, auto, config)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Launches the FiftyOne App.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mNote that only one App instance can be opened at a time. If this method is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    a :class:`Session`\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _session  \u001b[38;5;66;03m# pylint: disable=global-statement\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m _session \u001b[38;5;241m=\u001b[39m \u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_scheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_scheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesktop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesktop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrowser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _session\u001b[38;5;241m.\u001b[39mremote:\n\u001b[0;32m    213\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(_REMOTE_INSTRUCTIONS\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mformat(_session\u001b[38;5;241m.\u001b[39mserver_port))\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:435\u001b[0m, in \u001b[0;36mSession.__init__\u001b[1;34m(self, dataset, view, view_name, spaces, color_scheme, plots, port, address, remote, desktop, browser, height, auto, config)\u001b[0m\n\u001b[0;32m    432\u001b[0m _register_session(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_height\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser \u001b[38;5;241m=\u001b[39m browser\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremote:\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:256\u001b[0m, in \u001b[0;36mupdate_state.<locals>.decorator.<locals>.wrapper\u001b[1;34m(session, *args, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_show \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n\u001b[0;32m    255\u001b[0m     session\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[1;32m--> 256\u001b[0m result \u001b[38;5;241m=\u001b[39m func(session, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    257\u001b[0m session\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend_event(StateUpdate(state\u001b[38;5;241m=\u001b[39msession\u001b[38;5;241m.\u001b[39m_state))\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_show \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39mauto \u001b[38;5;129;01mand\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context():\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:1043\u001b[0m, in \u001b[0;36mSession.show\u001b[1;34m(self, height)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m focx\u001b[38;5;241m.\u001b[39mis_notebook_context() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesktop:\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m_reload()\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\session.py:1144\u001b[0m, in \u001b[0;36mSession.freeze\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1141\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly notebook sessions can be frozen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDeactivateNotebookCell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplots\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\fiftyone\\core\\session\\client.py:152\u001b[0m, in \u001b[0;36mClient.send_event\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient is not connected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_event(event)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_event(event)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Client is not connected"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import SAM\n",
    "from math import sqrt\n",
    "from random import randint, shuffle\n",
    "import fiftyone as fo\n",
    "import fiftyone.core.labels as fol\n",
    "import os\n",
    "\n",
    "# Helper Classes and Functions for Circle Calculation\n",
    "class Point:\n",
    "    def __init__(self, X=0, Y=0) -> None:\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "class Circle:\n",
    "    def __init__(self, c=Point(), r=0) -> None:    \n",
    "        self.C = c\n",
    "        self.R = r\n",
    "\n",
    "def dist(a, b):\n",
    "    return sqrt((a.X - b.X) ** 2 + (a.Y - b.Y) ** 2)\n",
    "\n",
    "def is_inside(c, p):\n",
    "    return dist(c.C, p) <= c.R\n",
    "\n",
    "def get_circle_center(bx, by, cx, cy):\n",
    "    B = bx * bx + by * by\n",
    "    C = cx * cx + cy * cy\n",
    "    D = bx * cy - by * cx\n",
    "    return Point((cy * B - by * C) / (2 * D), (bx * C - cx * B) / (2 * D))\n",
    "\n",
    "def circle_from1(A, B):\n",
    "    C = Point((A.X + B.X) / 2.0, (A.Y + B.Y) / 2.0)\n",
    "    return Circle(C, dist(A, B) / 2.0)\n",
    "\n",
    "def circle_from2(A, B, C):\n",
    "    I = get_circle_center(B.X - A.X, B.Y - A.Y, C.X - A.X, C.Y - A.Y)\n",
    "    I.X += A.X\n",
    "    I.Y += A.Y\n",
    "    return Circle(I, dist(I, A))\n",
    "\n",
    "def is_valid_circle(c, P):\n",
    "    for p in P:\n",
    "        if not is_inside(c, p):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def min_circle_trivial(P):\n",
    "    assert len(P) <= 3\n",
    "    if not P:\n",
    "        return Circle()\n",
    "    elif len(P) == 1:\n",
    "        return Circle(P[0], 0)\n",
    "    elif len(P) == 2:\n",
    "        return circle_from1(P[0], P[1])\n",
    "    for i in range(3):\n",
    "        for j in range(i + 1, 3):\n",
    "            c = circle_from1(P[i], P[j])\n",
    "            if is_valid_circle(c, P):\n",
    "                return c\n",
    "    return circle_from2(P[0], P[1], P[2])\n",
    "\n",
    "def welzl_helper(P, R, n):\n",
    "    if n == 0 or len(R) == 3:\n",
    "        return min_circle_trivial(R)\n",
    "    idx = randint(0, n - 1)\n",
    "    p = P[idx]\n",
    "    P[idx], P[n - 1] = P[n - 1], P[idx]\n",
    "    d = welzl_helper(P, R.copy(), n - 1)\n",
    "    if is_inside(d, p):\n",
    "        return d\n",
    "    R.append(p)\n",
    "    return welzl_helper(P, R.copy(), n - 1)\n",
    "\n",
    "def welzl(P):\n",
    "    P_copy = P.copy()\n",
    "    shuffle(P_copy)\n",
    "    return welzl_helper(P_copy, [], len(P_copy))\n",
    "def yolo_key_to_box(yolo_line, img_width=640, img_height=640):\n",
    "    # YOLO format: class x_center y_center width height (normalized)\n",
    "    yolo_data = yolo_line.strip().split()\n",
    "    x_center, y_center, width, height = map(float, yolo_data[1:5])\n",
    "\n",
    "\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "\n",
    "    # Calculate top-left and bottom-right coordinates\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "\n",
    "    return np.array([ x_min, y_min, x_max, y_max])\n",
    "\n",
    "def convert_yolo_key_to_boxes(yolo_file):\n",
    "    boxes = []\n",
    "    with open(yolo_file, 'r') as file:\n",
    "        for line in file:\n",
    "            box = yolo_key_to_box(line)\n",
    "            boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "def find_matching_yolo_file(core_name, yolo_dir):\n",
    "    for yolo_file in os.listdir(yolo_dir):\n",
    "        if core_name in yolo_file:  # Check if the core name is part of the YOLO file name\n",
    "            return os.path.join(yolo_dir, yolo_file)\n",
    "    return None \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize FiftyOne dataset\n",
    "dataset = fo.Dataset(\"prawn_segmentation\", overwrite=True)\n",
    "\n",
    "# Load SAM model\n",
    "model = SAM(r'mobile_sam.pt')\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\"\n",
    "yolo_dir = r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict19\\labels\"\n",
    "\n",
    "# Iterate over images\n",
    "for image_name in os.listdir(image_dir):\n",
    "    if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "       \n",
    "        core_name = image_name.split('.')[0]\n",
    "        # Create the corresponding YOLO file path\n",
    "\n",
    "                \n",
    "\n",
    "        yolo_file = find_matching_yolo_file(core_name, yolo_dir)\n",
    "        \n",
    "        # Convert YOLO annotations to bounding boxes\n",
    "        boxes = convert_yolo_key_to_boxes(yolo_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Perform segmentation\n",
    "        \n",
    "        # Initialize list to store segmentations\n",
    "        all_polygons = []\n",
    "        all_attributes = []\n",
    "\n",
    "        \n",
    "        # Process each mask individually\n",
    "        for xy in results[0].masks.xy:\n",
    "                 \n",
    "    # Normalize points to [0, 1] range\n",
    "                normalized_points = [(float(p[0]) / image.shape[1], float(p[1]) / image.shape[0]) for p in xy]\n",
    "                all_polygons.append(normalized_points)\n",
    "                points = [Point(p[0], p[1]) for p in xy]\n",
    "                mec = welzl(points)\n",
    "                diameter = mec.R * 2 \n",
    "                all_attributes.append({'diameter': diameter})\n",
    "                # Convert contour to a format suitable for FiftyOne\n",
    "                segmentation = fol.Polyline(\n",
    "                        points=all_polygons,\n",
    "                        closed=True,\n",
    "                        filled=True,\n",
    "                        attributes=all_attributes\n",
    "\n",
    "                        \n",
    "                    )\n",
    "        \n",
    "        # Add image and segmentation to FiftyOne dataset\n",
    "        sample = fo.Sample(filepath=image_path)\n",
    "        sample[\"segmentations\"] =fol.Polylines(polylines=[segmentation])\n",
    "\n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "# Save dataset\n",
    "dataset.save()\n",
    "session = fo.launch_app(dataset, port=5153)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found matching YOLO file for undistorted_GX010191_100_1250.jpg\n",
      "\n",
      "0: 1024x1024 2663.3ms\n",
      "Speed: 27.1ms preprocess, 2663.3ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1/66 [00:05<05:38,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_100_1250.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_100_1250_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_100_1250.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_100_1250_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_100_1250.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_100_1250_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_100_1250.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_100_1250_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_101_1265.jpg\n",
      "\n",
      "0: 1024x1024 4262.6ms\n",
      "Speed: 33.6ms preprocess, 4262.6ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 2/66 [00:10<05:25,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_101_1265.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_101_1265_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_101_1265.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_101_1265_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_101_1265.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_101_1265_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_102_1277.jpg\n",
      "\n",
      "0: 1024x1024 3647.6ms\n",
      "Speed: 104.7ms preprocess, 3647.6ms inference, 17.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–         | 3/66 [00:14<04:48,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_102_1277.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_102_1277_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_102_1277.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_102_1277_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_103_1289.jpg\n",
      "\n",
      "0: 1024x1024 3991.4ms\n",
      "Speed: 48.6ms preprocess, 3991.4ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 4/66 [00:18<04:34,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_103_1289.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_103_1289_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_103_1289.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_103_1289_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_103_1289.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_103_1289_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_104_1301.jpg\n",
      "\n",
      "0: 1024x1024 2749.1ms\n",
      "Speed: 30.7ms preprocess, 2749.1ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 5/66 [00:21<03:56,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_104_1301.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_104_1301_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_104_1301.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_104_1301_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_104_1301.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_104_1301_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_105_1312.jpg\n",
      "\n",
      "0: 1024x1024 2489.7ms\n",
      "Speed: 15.1ms preprocess, 2489.7ms inference, 13.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 6/66 [00:23<03:27,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_105_1312.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_105_1312_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_107_1330.jpg\n",
      "\n",
      "0: 1024x1024 2470.1ms\n",
      "Speed: 129.8ms preprocess, 2470.1ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 7/66 [00:26<03:09,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_107_1330.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_107_1330_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_107_1330.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_107_1330_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_107_1330.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_107_1330_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_107_1330.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_107_1330_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_108_1337.jpg\n",
      "\n",
      "0: 1024x1024 2585.9ms\n",
      "Speed: 120.3ms preprocess, 2585.9ms inference, 14.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 8/66 [00:29<03:01,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_108_1337.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_108_1337_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_108_1337.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_108_1337_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_108_1337.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_108_1337_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_108_1337.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_108_1337_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_10_370.jpg\n",
      "\n",
      "0: 1024x1024 2532.8ms\n",
      "Speed: 24.7ms preprocess, 2532.8ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–Ž        | 9/66 [00:32<02:52,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_10_370.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_10_370_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_110_1355.jpg\n",
      "\n",
      "0: 1024x1024 3437.9ms\n",
      "Speed: 50.3ms preprocess, 3437.9ms inference, 12.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 10/66 [00:36<03:01,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_110_1355.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_110_1355_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_110_1355.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_110_1355_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_110_1355.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_110_1355_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_110_1355.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_110_1355_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_116_1399.jpg\n",
      "\n",
      "0: 1024x1024 2793.5ms\n",
      "Speed: 39.2ms preprocess, 2793.5ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 11/66 [00:39<02:53,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_116_1399.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_116_1399_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_116_1399.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_116_1399_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_116_1399.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_116_1399_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_118_1415.jpg\n",
      "\n",
      "0: 1024x1024 4875.2ms\n",
      "Speed: 15.1ms preprocess, 4875.2ms inference, 10.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 12/66 [00:44<03:33,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_118_1415.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_118_1415_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_118_1415.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_118_1415_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_118_1415.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_118_1415_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_118_1415.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_118_1415_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_11_391.jpg\n",
      "\n",
      "0: 1024x1024 5789.4ms\n",
      "Speed: 66.7ms preprocess, 5789.4ms inference, 20.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 13/66 [00:50<04:03,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_11_391.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_11_391_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_11_391.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_11_391_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_12_410.jpg\n",
      "\n",
      "0: 1024x1024 3720.1ms\n",
      "Speed: 49.8ms preprocess, 3720.1ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 14/66 [00:54<03:49,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_12_410.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_12_410_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_12_410.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_12_410_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_12_410.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_12_410_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_12_410.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_12_410_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_12_410.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_12_410_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_13_439.jpg\n",
      "\n",
      "0: 1024x1024 3178.7ms\n",
      "Speed: 35.2ms preprocess, 3178.7ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 15/66 [00:58<03:29,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_13_439.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_13_439_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_13_439.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_13_439_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_13_439.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_13_439_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_14_460.jpg\n",
      "\n",
      "0: 1024x1024 2964.0ms\n",
      "Speed: 35.9ms preprocess, 2964.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 16/66 [01:01<03:11,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_14_460.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_14_460_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_14_460.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_14_460_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_14_460.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_14_460_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_15_479.jpg\n",
      "\n",
      "0: 1024x1024 2967.6ms\n",
      "Speed: 59.8ms preprocess, 2967.6ms inference, 14.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 17/66 [01:04<02:59,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_15_479.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_15_479_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_15_479.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_15_479_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_16_495.jpg\n",
      "\n",
      "0: 1024x1024 4241.2ms\n",
      "Speed: 50.2ms preprocess, 4241.2ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 18/66 [01:09<03:08,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_16_495.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_16_495_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_16_495.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_16_495_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_16_495.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_16_495_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_16_495.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_16_495_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_16_495.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_16_495_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_17_509.jpg\n",
      "\n",
      "0: 1024x1024 2930.0ms\n",
      "Speed: 49.8ms preprocess, 2930.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 19/66 [01:12<02:53,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_17_509.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_17_509_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_17_509.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_17_509_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_17_509.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_17_509_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_18_521.jpg\n",
      "\n",
      "0: 1024x1024 2754.9ms\n",
      "Speed: 60.0ms preprocess, 2754.9ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 20/66 [01:15<02:39,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_18_521.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_18_521_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_19_537.jpg\n",
      "\n",
      "0: 1024x1024 2830.0ms\n",
      "Speed: 26.6ms preprocess, 2830.0ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 21/66 [01:18<02:29,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_19_537.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_19_537_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_26_199.jpg\n",
      "\n",
      "0: 1024x1024 3219.7ms\n",
      "Speed: 40.2ms preprocess, 3219.7ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 22/66 [01:21<02:27,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_26_199.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_26_199_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_26_199.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_26_199_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_26_199.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_26_199_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_26_199.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_26_199_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_26_199.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_26_199_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_26_199.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_26_199_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_27_215.jpg\n",
      "\n",
      "0: 1024x1024 2686.3ms\n",
      "Speed: 25.2ms preprocess, 2686.3ms inference, 16.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 23/66 [01:24<02:17,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_27_215.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_27_215_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_27_215.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_27_215_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_28_230.jpg\n",
      "\n",
      "0: 1024x1024 4380.1ms\n",
      "Speed: 19.6ms preprocess, 4380.1ms inference, 10.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 24/66 [01:29<02:33,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_28_230.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_28_230_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_28_230.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_28_230_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_28_230.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_28_230_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_28_230.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_28_230_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_28_230.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_28_230_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_28_230.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_28_230_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_28_230.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_28_230_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_29_257.jpg\n",
      "\n",
      "0: 1024x1024 2875.1ms\n",
      "Speed: 29.7ms preprocess, 2875.1ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 25/66 [01:32<02:22,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_29_257.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_29_257_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_29_257.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_29_257_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_29_257.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_29_257_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_29_257.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_29_257_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_30_275.jpg\n",
      "\n",
      "0: 1024x1024 2839.9ms\n",
      "Speed: 20.1ms preprocess, 2839.9ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 26/66 [01:35<02:12,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_30_275.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_30_275_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_30_275.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_30_275_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_31_283.jpg\n",
      "\n",
      "0: 1024x1024 2740.2ms\n",
      "Speed: 19.6ms preprocess, 2740.2ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/66 [01:38<02:04,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_31_283.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_31_283_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_31_283.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_31_283_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_31_283.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_31_283_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_32_1049.jpg\n",
      "\n",
      "0: 1024x1024 3138.9ms\n",
      "Speed: 25.1ms preprocess, 3138.9ms inference, 17.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/66 [01:41<02:03,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_32_1049.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_1049_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_32_1049.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_1049_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_32_1049.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_1049_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_32_1049.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_1049_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_32_1049.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_1049_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_32_305.jpg\n",
      "\n",
      "0: 1024x1024 2870.0ms\n",
      "Speed: 24.7ms preprocess, 2870.0ms inference, 10.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 29/66 [01:44<01:58,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_32_305.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_305_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_32_305.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_305_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_32_305.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_305_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_32_305.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_305_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_32_305.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_32_305_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_33_1087.jpg\n",
      "\n",
      "0: 1024x1024 3030.2ms\n",
      "Speed: 45.3ms preprocess, 3030.2ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 30/66 [01:47<01:55,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_33_1087.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_33_1087_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_33_1087.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_33_1087_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_33_1087.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_33_1087_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_33_331.jpg\n",
      "\n",
      "0: 1024x1024 2889.4ms\n",
      "Speed: 15.2ms preprocess, 2889.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 31/66 [01:51<01:50,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_33_331.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_33_331_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_33_331.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_33_331_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_33_331.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_33_331_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_33_331.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_33_331_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_33_331.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_33_331_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_34_1128.jpg\n",
      "\n",
      "0: 1024x1024 2849.9ms\n",
      "Speed: 20.1ms preprocess, 2849.9ms inference, 10.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 32/66 [01:54<01:46,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_34_1128.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_34_1128_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_34_1128.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_34_1128_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_34_1128.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_34_1128_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_34_1128.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_34_1128_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_34_1128.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_34_1128_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_34_350.jpg\n",
      "\n",
      "0: 1024x1024 2841.6ms\n",
      "Speed: 33.7ms preprocess, 2841.6ms inference, 15.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 33/66 [01:57<01:42,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_34_350.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_34_350_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_34_350.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_34_350_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_34_350.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_34_350_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_35_1167.jpg\n",
      "\n",
      "0: 1024x1024 3204.0ms\n",
      "Speed: 173.1ms preprocess, 3204.0ms inference, 23.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/66 [02:00<01:45,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_35_1167.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_35_1167_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_35_1167.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_35_1167_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_35_1167.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_35_1167_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_35_1167.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_35_1167_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_35_1167.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_35_1167_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_35_367.jpg\n",
      "\n",
      "0: 1024x1024 2892.3ms\n",
      "Speed: 55.9ms preprocess, 2892.3ms inference, 10.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 35/66 [02:03<01:40,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_35_367.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_35_367_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_35_367.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_35_367_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_36_1195.jpg\n",
      "\n",
      "0: 1024x1024 4143.7ms\n",
      "Speed: 44.8ms preprocess, 4143.7ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 36/66 [02:08<01:47,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_36_1195.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_36_1195_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_36_1195.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_36_1195_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_36_1195.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_36_1195_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_36_381.jpg\n",
      "\n",
      "0: 1024x1024 3062.7ms\n",
      "Speed: 31.6ms preprocess, 3062.7ms inference, 14.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 37/66 [02:11<01:41,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_36_381.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_36_381_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_36_381.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_36_381_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_36_381.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_36_381_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_37_1242.jpg\n",
      "\n",
      "0: 1024x1024 3290.8ms\n",
      "Speed: 19.4ms preprocess, 3290.8ms inference, 5.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 38/66 [02:15<01:36,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_37_1242.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_37_1242_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_37_1242.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_37_1242_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_37_392.jpg\n",
      "\n",
      "0: 1024x1024 3424.7ms\n",
      "Speed: 29.2ms preprocess, 3424.7ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 39/66 [02:18<01:34,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_37_392.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_37_392_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_37_392.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_37_392_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_38_1266.jpg\n",
      "\n",
      "0: 1024x1024 4212.2ms\n",
      "Speed: 39.1ms preprocess, 4212.2ms inference, 18.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 40/66 [02:23<01:40,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_38_1266.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_38_1266_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_38_1266.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_38_1266_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_38_1266.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_38_1266_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_38_404.jpg\n",
      "\n",
      "0: 1024x1024 3557.4ms\n",
      "Speed: 100.3ms preprocess, 3557.4ms inference, 14.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 41/66 [02:27<01:36,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_38_404.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_38_404_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_38_404.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_38_404_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_38_404.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_38_404_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_38_404.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_38_404_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_38_404.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_38_404_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_39_1284.jpg\n",
      "\n",
      "0: 1024x1024 3142.9ms\n",
      "Speed: 25.9ms preprocess, 3142.9ms inference, 6.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/66 [02:30<01:31,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_39_1284.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_39_1284_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_39_1284.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_39_1284_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_39_416.jpg\n",
      "\n",
      "0: 1024x1024 3738.8ms\n",
      "Speed: 270.2ms preprocess, 3738.8ms inference, 2.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 43/66 [02:35<01:31,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_39_416.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_39_416_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_41_445.jpg\n",
      "\n",
      "0: 1024x1024 3136.4ms\n",
      "Speed: 30.6ms preprocess, 3136.4ms inference, 7.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 44/66 [02:38<01:23,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_41_445.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_41_445_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_42_1339.jpg\n",
      "\n",
      "0: 1024x1024 3306.8ms\n",
      "Speed: 33.6ms preprocess, 3306.8ms inference, 13.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 45/66 [02:43<01:25,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_42_1339.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_1339_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_42_1339.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_1339_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_42_1339.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_1339_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_42_1339.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_1339_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_42_1339.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_1339_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_42_1339.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_1339_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_42_457.jpg\n",
      "\n",
      "0: 1024x1024 3699.2ms\n",
      "Speed: 73.8ms preprocess, 3699.2ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n",
      "Saved segmentation for undistorted_GX010191_42_457.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_457_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_42_457.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_457_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_42_457.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_457_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_42_457.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_42_457_segmentations.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 46/66 [02:47<01:21,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found matching YOLO file for undistorted_GX010191_43_1353.jpg\n",
      "\n",
      "0: 1024x1024 4496.5ms\n",
      "Speed: 29.9ms preprocess, 4496.5ms inference, 12.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 47/66 [02:52<01:22,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_43_1353.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_1353_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_1353.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_1353_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_1353.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_1353_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_1353.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_1353_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_1353.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_1353_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_1353.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_1353_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_1353.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_1353_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_1353.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_1353_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_43_468.jpg\n",
      "\n",
      "0: 1024x1024 3466.4ms\n",
      "Speed: 61.3ms preprocess, 3466.4ms inference, 10.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 48/66 [02:56<01:15,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_43_468.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_468_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_468.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_468_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_468.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_468_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_468.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_468_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_468.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_468_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_43_468.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_43_468_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_44_478.jpg\n",
      "\n",
      "0: 1024x1024 2754.4ms\n",
      "Speed: 23.5ms preprocess, 2754.4ms inference, 6.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/66 [02:59<01:05,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_44_478.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_44_478_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_44_478.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_44_478_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_45_489.jpg\n",
      "\n",
      "0: 1024x1024 3570.0ms\n",
      "Speed: 124.9ms preprocess, 3570.0ms inference, 15.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 50/66 [03:03<01:02,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_45_489.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_45_489_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_45_489.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_45_489_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_45_489.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_45_489_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_45_489.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_45_489_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_45_489.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_45_489_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_46_497.jpg\n",
      "\n",
      "0: 1024x1024 3016.5ms\n",
      "Speed: 28.2ms preprocess, 3016.5ms inference, 10.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 51/66 [03:06<00:55,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_46_497.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_46_497_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_46_497.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_46_497_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_46_497.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_46_497_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_46_497.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_46_497_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_46_497.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_46_497_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_46_497.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_46_497_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_46_497.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_46_497_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_47_507.jpg\n",
      "\n",
      "0: 1024x1024 3880.2ms\n",
      "Speed: 45.3ms preprocess, 3880.2ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 52/66 [03:10<00:53,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_47_507.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_47_507_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_47_507.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_47_507_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_47_507.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_47_507_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_51_535.jpg\n",
      "\n",
      "0: 1024x1024 2635.1ms\n",
      "Speed: 100.2ms preprocess, 2635.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n",
      "Saved segmentation for undistorted_GX010191_51_535.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_51_535_segmentations.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 53/66 [03:13<00:46,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found matching YOLO file for undistorted_GX010191_5_190.jpg\n",
      "\n",
      "0: 1024x1024 3155.0ms\n",
      "Speed: 75.0ms preprocess, 3155.0ms inference, 5.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 54/66 [03:17<00:42,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_5_190.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_5_190_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_5_190.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_5_190_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_5_190.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_5_190_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_6_219.jpg\n",
      "\n",
      "0: 1024x1024 3201.5ms\n",
      "Speed: 48.8ms preprocess, 3201.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 55/66 [03:20<00:38,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_6_219.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_6_219_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_6_219.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_6_219_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_6_219.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_6_219_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_6_219.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_6_219_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_88_1016.jpg\n",
      "\n",
      "0: 1024x1024 2745.7ms\n",
      "Speed: 48.8ms preprocess, 2745.7ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mc:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\segment\\predict51\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/66 [03:23<00:33,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for undistorted_GX010191_88_1016.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_88_1016_segmentations.txt\n",
      "Saved segmentation for undistorted_GX010191_88_1016.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\\undistorted_GX010191_88_1016_segmentations.txt\n",
      "Found matching YOLO file for undistorted_GX010191_8_309.jpg\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/66 [03:26<00:36,  3.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Perform segmentation\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m360\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msave(filename\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(segmenation_dir,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcore_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Prepare a .txt file to save the segmentations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\model.py:137\u001b[0m, in \u001b[0;36mSAM.__call__\u001b[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, bboxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    Performs segmentation prediction on the given image or video source.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        >>> print(f\"Detected {len(results[0].masks)} masks\")\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, bboxes, points, labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\model.py:111\u001b[0m, in \u001b[0;36mSAM.predict\u001b[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(overrides)\n\u001b[0;32m    110\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(bboxes\u001b[38;5;241m=\u001b[39mbboxes, points\u001b[38;5;241m=\u001b[39mpoints, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(source, stream, prompts\u001b[38;5;241m=\u001b[39mprompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\engine\\model.py:563\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:203\u001b[0m, in \u001b[0;36mPredictor.inference\u001b[1;34m(self, im, bboxes, points, labels, masks, multimask_output, *args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [bboxes, points, masks]):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:232\u001b[0m, in \u001b[0;36mPredictor.prompt_inference\u001b[1;34m(self, im, bboxes, points, labels, masks, multimask_output)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, im, bboxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multimask_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    Performs image segmentation inference based on input cues using SAM's specialized architecture.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m        >>> masks, scores, logits = predictor.prompt_inference(im, bboxes=bboxes)\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_im_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[0;32m    234\u001b[0m     src_shape, dst_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    235\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_all \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(dst_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m src_shape[\u001b[38;5;241m0\u001b[39m], dst_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m src_shape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:525\u001b[0m, in \u001b[0;36mPredictor.get_im_features\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_im_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, im):\n\u001b[0;32m    524\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extracts image features using the SAM model's image encoder for subsequent mask prediction.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\tiny_encoder.py:991\u001b[0m, in \u001b[0;36mTinyViT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs the forward pass through the TinyViT model, extracting features from the input image.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\tiny_encoder.py:983\u001b[0m, in \u001b[0;36mTinyViT.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_i, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m    982\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[1;32m--> 983\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m batch, _, channel \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    985\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, channel)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\tiny_encoder.py:757\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes input through TinyViT blocks and optional downsampling.\"\"\"\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 757\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(blk, x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\tiny_encoder.py:613\u001b[0m, in \u001b[0;36mTinyViTBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;66;03m# Window partition\u001b[39;00m\n\u001b[0;32m    608\u001b[0m x \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    609\u001b[0m     x\u001b[38;5;241m.\u001b[39mview(b, nH, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, nW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, c)\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(b \u001b[38;5;241m*\u001b[39m nH \u001b[38;5;241m*\u001b[39m nW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, c)\n\u001b[0;32m    612\u001b[0m )\n\u001b[1;32m--> 613\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;66;03m# Window reverse\u001b[39;00m\n\u001b[0;32m    616\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(b, nH, nW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, c)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(b, pH, pW, c)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\tiny_encoder.py:473\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    470\u001b[0m B, N, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# B, N, C\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# Normalization\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# (B, N, num_heads, d)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2542\u001b[0m     )\n\u001b[1;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import SAM\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# YOLO format conversion\n",
    "def yolo_key_to_box(yolo_line, img_width=640, img_height=360):\n",
    "    yolo_data = yolo_line.strip().split()\n",
    "    x_center, y_center, width, height = map(float, yolo_data[1:5])\n",
    "\n",
    "    x_center *= img_width\n",
    "    y_center *= img_height\n",
    "    width *= img_width\n",
    "    height *= img_height\n",
    "\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "\n",
    "    return np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "def convert_yolo_key_to_boxes(yolo_file):\n",
    "    boxes = []\n",
    "    with open(yolo_file, 'r') as file:\n",
    "        for line in file:\n",
    "            box = yolo_key_to_box(line)\n",
    "            boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "def find_matching_yolo_file(core_name, yolo_dir):\n",
    "    for yolo_file in os.listdir(yolo_dir):\n",
    "        if core_name in yolo_file:  # Check if the core name is part of the YOLO file name\n",
    "            return os.path.join(yolo_dir, yolo_file)\n",
    "    return None \n",
    "\n",
    "# Initialize SAM model\n",
    "model = SAM(r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\fifty_one\\measurements\\mobile_sam.pt')\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\org_images\"\n",
    "yolo_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\txt\"\n",
    "segmenation_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\molt\\image processed\\good\\640360\\right\\segmentations\"\n",
    "\n",
    "# Iterate over images\n",
    "for image_name in tqdm(os.listdir(image_dir)):\n",
    "    if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "        core_name = image_name.split('.')[0]\n",
    "        \n",
    "        # Create the corresponding YOLO file path\n",
    "        yolo_file = find_matching_yolo_file(core_name, yolo_dir)\n",
    "        if yolo_file is None:\n",
    "            print(f\"No matching YOLO file found for {image_name}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Found matching YOLO file for {image_name}\")\n",
    "        # Convert YOLO annotations to bounding boxes\n",
    "        boxes = convert_yolo_key_to_boxes(yolo_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Perform segmentation\n",
    "        results = model(image, imgsz=(640,360),save=True, bboxes=np.array(boxes))\n",
    "        \n",
    "        results[0].save(filename=os.path.join(segmenation_dir,f'{core_name}.jpg'))\n",
    "        # Prepare a .txt file to save the segmentations\n",
    "        output_txt_path = os.path.join(image_dir, f\"{core_name}_segmentations.txt\")\n",
    "        \n",
    "        with open(output_txt_path, 'w') as f:\n",
    "            # Process each mask individually\n",
    "            for xy in results[0].masks.xy:\n",
    "                # Convert the xy coordinates into a string format for saving\n",
    "                xy_str = \" \".join(map(str, xy.flatten().tolist()))  # Flatten and convert to string\n",
    "                f.write(xy_str + \"\\n\")  # Write each xy as a line\n",
    "                print(f\"Saved segmentation for {image_name} to {output_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image with filled segmentations to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\\undistorted_GX010091_5_149_with_segmentations.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_filled_segmentations_with_opacity(image, xy_coordinates, color=(0, 255, 0), opacity=0.2):\n",
    "    \"\"\"\n",
    "    Draws and fills the segmentation polygons on the image with a specified opacity.\n",
    "    xy_coordinates: List of lists where each inner list represents a segmentation polygon \n",
    "                    (each polygon is a list of [x, y] pairs).\n",
    "    color: The color to fill the polygons (in BGR format).\n",
    "    opacity: A float value between 0 and 1 that determines the transparency of the fill.\n",
    "    \"\"\"\n",
    "    # Create an overlay image to draw on\n",
    "    overlay = image.copy()\n",
    "    \n",
    "    # Draw filled polygons on the overlay\n",
    "    for polygon in xy_coordinates:\n",
    "        polygon = np.array(polygon, np.int32)  # Convert to NumPy array of int32\n",
    "        polygon = polygon.reshape((-1, 1, 2))  # Reshape for OpenCV\n",
    "        cv2.fillPoly(overlay, [polygon], color=color)  # Fill the polygon on the overlay\n",
    "    \n",
    "    # Blend the overlay with the original image\n",
    "    cv2.addWeighted(overlay, opacity, image, 1 - opacity, 0, image)\n",
    "    return image\n",
    "\n",
    "def load_segmentations_from_txt(txt_file):\n",
    "    \"\"\"\n",
    "    Loads segmentation polygons from a .txt file.\n",
    "    Each line in the file contains space-separated coordinates: x1 y1 x2 y2 ... xn yn.\n",
    "    \"\"\"\n",
    "    segmentations = []\n",
    "    with open(txt_file, 'r') as file:\n",
    "        for line in file:\n",
    "            coords = list(map(float, line.strip().split()))  # Read all floats from the line\n",
    "            polygon = [(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]  # Convert into (x, y) pairs\n",
    "            segmentations.append(polygon)\n",
    "    return segmentations\n",
    "\n",
    "# Specify the image and corresponding segmentation file\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\"\n",
    "image_name = \"undistorted_GX010091_5_149.jpg_gamma.jpg\"  # Replace with the name of your image\n",
    "image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "core_name = image_name.split('.')[0]\n",
    "\n",
    "# Path to the corresponding segmentations .txt file\n",
    "txt_file_path = os.path.join(image_dir, f\"{core_name}_segmentations.txt\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the segmentation file exists\n",
    "if os.path.exists(txt_file_path):\n",
    "    # Load segmentations from the .txt file\n",
    "    segmentations = load_segmentations_from_txt(txt_file_path)\n",
    "    \n",
    "    # Draw and fill the segmentations on the image\n",
    "    image_with_segmentations = draw_filled_segmentations_with_opacity(image, segmentations)\n",
    "    \n",
    "    # Save or display the result\n",
    "    output_image_path = os.path.join(image_dir, f\"{core_name}_with_segmentations.png\")\n",
    "    cv2.imwrite(output_image_path, image_with_segmentations)\n",
    "    print(f\"Saved image with filled segmentations to {output_image_path}\")\n",
    "    \n",
    "    # Optionally, display the image (for local testing)\n",
    "    # cv2.imshow('Segmented Image', image_with_segmentations)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(f\"No segmentation file found for {image_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try skeletonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import skeletonize\n",
    "import networkx as nx\n",
    "\n",
    "def load_multiple_coords_from_txt(txt_file):\n",
    "    \"\"\"\n",
    "    Loads multiple (x, y) coordinates sets from a .txt file.\n",
    "    Each line in the file contains space-separated coordinates: x1 y1 x2 y2 ... xn yn.\n",
    "    Each line represents a different segmentation.\n",
    "    \"\"\"\n",
    "    segmentations = []\n",
    "    with open(txt_file, 'r') as file:\n",
    "        for line in file:\n",
    "            coords = list(map(float, line.strip().split()))  # Read all floats from the line\n",
    "            polygon = [(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]   # Convert into (y, x) pairs\n",
    "            segmentations.append(polygon)\n",
    "    return segmentations\n",
    "\n",
    "def create_filled_binary_mask(coords, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Creates a binary mask with a filled polygon from a single segmentation's (y, x) coordinates.\n",
    "    \"\"\"\n",
    "    binary_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "    polygon = np.array(coords, np.int32)  # Convert to NumPy array of int32\n",
    "    polygon = polygon.reshape((-1, 1, 2))  # Reshape for OpenCV fillPoly\n",
    "    cv2.fillPoly(binary_mask, [polygon], color=1) \n",
    "    \n",
    "     # Fill the polygon on the binary mask\n",
    "    return binary_mask\n",
    "\n",
    "def skeletonize_mask(binary_mask,i):\n",
    "    \"\"\"\n",
    "    Skeletonizes the binary mask using skimage's skeletonize function.\n",
    "    \"\"\"\n",
    "    skeleton = skeletonize(binary_mask)\n",
    "    cv2.imwrite(f'skeleton{i}.png', skeleton.astype(np.uint8) * 255)\n",
    "\n",
    "    return skeleton\n",
    "\n",
    "    \n",
    "\n",
    "def draw_skeleton(image, skeleton_coords, color=(0, 255, 0), thickness=1):\n",
    "    \"\"\"\n",
    "    Draws the skeleton on the image.\n",
    "    skeleton_coords: List of (y, x) points representing the skeleton.\n",
    "    color: The color to draw the skeleton (in BGR format).\n",
    "    thickness: The thickness of the lines representing the skeleton.\n",
    "    \"\"\"\n",
    "    for i in range(len(skeleton_coords) - 1):\n",
    "        start_point = (skeleton_coords[i][1], skeleton_coords[i][0])  # (x, y)\n",
    "        end_point = (skeleton_coords[i+1][1], skeleton_coords[i+1][0])  # (x, y)\n",
    "        cv2.line(image, start_point, end_point, color=color, thickness=thickness)\n",
    "    return image\n",
    "\n",
    "def draw_longest_path(image, longest_path, color=(0, 0, 255), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws the longest path on the image in a different color.\n",
    "    longest_path: List of (y, x) points representing the longest path.\n",
    "    color: The color to draw the longest path (in BGR format).\n",
    "    thickness: The thickness of the lines representing the longest path.\n",
    "    \"\"\"\n",
    "    for i in range(len(longest_path) - 1):\n",
    "        start_point = (longest_path[i][1], longest_path[i][0])  # (x, y)\n",
    "        end_point = (longest_path[i+1][1], longest_path[i+1][0])  # (x, y)\n",
    "        cv2.line(image, start_point, end_point, color=color, thickness=thickness)\n",
    "    return image\n",
    "\n",
    "def find_longest_path(skeleton_coords):\n",
    "    \"\"\"\n",
    "    Finds the longest path in the skeleton using graph analysis.\n",
    "    skeleton_coords: List of (y, x) points representing the skeleton.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for y, x in skeleton_coords:\n",
    "        G.add_node((y, x))  # Add node at each skeleton point\n",
    "        for dy in [-1, 0, 1]:\n",
    "            for dx in [-1, 0, 1]:\n",
    "                if dy == 0 and dx == 0:\n",
    "                    continue  # Skip the current point itself\n",
    "                neighbor = (y + dy, x + dx)\n",
    "                if neighbor in G.nodes:\n",
    "                    G.add_edge((y, x), neighbor)\n",
    "\n",
    "    connected_components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "    largest_component = max(connected_components, key=len)\n",
    "    endpoints = [node for node, degree in dict(largest_component.degree()).items() if degree == 1]\n",
    "\n",
    "    max_length = 0\n",
    "    longest_path = []\n",
    "\n",
    "    for source in endpoints:\n",
    "        lengths = nx.single_source_shortest_path_length(largest_component, source)\n",
    "        farthest_node = max(lengths, key=lengths.get)\n",
    "        length = lengths[farthest_node]\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            longest_path = nx.shortest_path(largest_component, source, farthest_node)\n",
    "\n",
    "    return longest_path\n",
    "\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right_resized\"\n",
    "image_name = \"undistorted_GX010067_33_625.jpg_gamma.jpg\"  # Replace with the name of your image\n",
    "image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "core_name = image_name.split('.')[0]\n",
    "\n",
    "# Path to the corresponding segmentations .txt file\n",
    "txt_file_path = os.path.join(image_dir, f\"{core_name}_segmentations.txt\")  # Replace with your .txt file path\n",
    "\n",
    "# Load image and segmentations\n",
    "image = cv2.imread(image_path)\n",
    "segmentations = load_multiple_coords_from_txt(txt_file_path)\n",
    "img_height, img_width = image.shape[:2]\n",
    "\n",
    "i=0\n",
    "# Process each segmentation individually\n",
    "for coords in segmentations:\n",
    "   \n",
    "    binary_mask = create_filled_binary_mask(coords, img_height, img_width)\n",
    "    skeleton = skeletonize_mask(binary_mask,i)\n",
    "    skeleton_coords = np.column_stack(np.nonzero(skeleton))\n",
    "    longest_path = find_longest_path(skeleton_coords)\n",
    "    \n",
    "    # Draw skeleton and longest path\n",
    "    image = draw_skeleton(image, skeleton_coords)\n",
    "    image = draw_longest_path(image, longest_path)\n",
    "    \n",
    "# Save or display the final output image as needed\n",
    "cv2.imwrite('output_image_with_skeleton_and_longest_path.png', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right_resized\"\n",
    "image_name = \"undistorted_GX010067_33_625.jpg_gamma.jpg\"  # Replace with the name of your image\n",
    "image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "core_name = image_name.split('.')[0]\n",
    "\n",
    "# Path to the corresponding segmentations .txt file\n",
    "txt_file_path = os.path.join(image_dir, f\"{core_name}_segmentations.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010152_36_378-jpg_gamma_jpg.rf.dde16353cb779b5cc6dabcd884e15547.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010152_36_378.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010155_18_219-jpg_gamma_jpg.rf.9057ffadeef9e786b3936806a3c9f36a.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010155_18_219.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010155_78_1170-jpg_gamma_jpg.rf.e004afd947d9bd621eb3843251abf362.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010155_78_1170.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010157_174_2582-jpg_gamma_jpg.rf.17f5d5766741fd481089300d1ad169b3.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010157_174_2582.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010157_177_2665-jpg_gamma_jpg.rf.0261ef6c41f32527ede8abb830f0c5b4.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010157_177_2665.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010161_136_2267-jpg_gamma-jpg_gamma_jpg.rf.d9954b4dcab4ac449c4210d47558d8cd.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010161_136_2267.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010161_54_676-jpg_gamma-jpg_gamma_jpg.rf.a76e2090f1504f2f3346505c8b6695c0.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010161_54_676.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010162_72_927-jpg_gamma_jpg.rf.1dc73de36406e10e543ee54034d31aa1.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010162_72_927.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010173_75_922-jpg_gamma_jpg.rf.9d2e8d784d1003aeb5785562ebf6b3ec.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010173_75_922.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010174_62_790-jpg_gamma_jpg.rf.751d4c5f73e63333ab2801a80cec7d5a.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010174_62_790.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010175_215_2644-jpg_gamma_jpg.rf.a5a6a089316911ffa24b620e9bd4b526.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010175_215_2644.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010175_266_3372-jpg_gamma-jpg_gamma_jpg.rf.382de27143dbd418f1c511227a90dc1e.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010175_266_3372.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010175_82_852-jpg_gamma_jpg.rf.c5143bb908a8a828b1ee6a1b774b8359.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010175_82_852.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010178_172_3604-jpg_gamma_jpg.rf.965ab8e85dd18756b57af8e5eefcd1fb.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010178_172_3604.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010178_189_3987-jpg_gamma_jpg.rf.673f87b9ad4ed71b37228f9e8391688d.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010178_189_3987.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010179_200_3927-jpg_gamma_jpg.rf.b3587f8a5710fe5c4f16e3e5ac4fb29f.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010179_200_3927.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010179_88_1697-jpg_gamma_jpg.rf.7d40fb8be30a514dac7b802056400240.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010179_88_1697.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010183_128_2852-jpg_gamma_jpg.rf.3fb084ffce7052947955101eb632c13f.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010183_128_2852.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010183_37_685-jpg_gamma_jpg.rf.bfcae27938d603211d8a7d9959bf39d2.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010183_37_685.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\GX010183_80_1633-jpg_gamma_jpg.rf.40bb3ba57d7f48e08d3d9d1de5bf864c.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010183_80_1633.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010157_160_2259.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010157_160.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010157_68_883.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010157_68.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010169_121_1249.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010169_121.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010174_104_1236.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010174_104.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010177_232_3047.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010177_232.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010178_107_2118.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010178_107.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010178_114_2258.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010178_114.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010180_91_1563.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010180_91.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010180_99_1788.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010180_99.jpg\n",
      "Renaming C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_GX010181_101_1542.jpg to C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\\undistorted_undistorted_GX010181_101.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\car to compare\\tester_choise\\test\\images\"\n",
    "yolo_dir = r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict19\\labels\"\n",
    "\n",
    "# Helper function to extract the core image name\n",
    "def extract_core_image_name(image_name):\n",
    "    # Remove everything after the first hyphen\n",
    "    core_name = image_name.split('-')[0]\n",
    "    \n",
    "    # Split the core name by underscores, and capture the first three parts\n",
    "    parts = core_name.split('_')\n",
    "    \n",
    "    # Rebuild the core name using the first three parts\n",
    "    core_name = '_'.join(parts[:3])\n",
    "    \n",
    "    return core_name\n",
    "\n",
    "# Helper function to find the matching YOLO file\n",
    "def find_matching_yolo_file(core_name, yolo_dir):\n",
    "    for yolo_file in os.listdir(yolo_dir):\n",
    "        if core_name in yolo_file:  # Check if the core name is part of the YOLO file name\n",
    "            return os.path.join(yolo_dir, yolo_file)\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Helper function to rename images\n",
    "def rename_images(image_dir, yolo_dir):\n",
    "    for image_name in os.listdir(image_dir):\n",
    "        if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            \n",
    "            # Extract the core name from the image filename\n",
    "            core_name = extract_core_image_name(image_name)\n",
    "            \n",
    "            # Find the matching YOLO file\n",
    "            yolo_file_path = find_matching_yolo_file(core_name, yolo_dir)\n",
    "            \n",
    "            if yolo_file_path:\n",
    "                # Create the new image name using the core name\n",
    "                new_image_name = f\"undistorted_{core_name}.jpg\"\n",
    "                new_image_path = os.path.join(image_dir, new_image_name)\n",
    "                \n",
    "                # Rename the image\n",
    "                print(f\"Renaming {image_path} to {new_image_path}\")\n",
    "                os.rename(image_path, new_image_path)\n",
    "            else:\n",
    "                print(f\"No matching YOLO file for {image_name}, skipping...\")\n",
    "\n",
    "# Run the renaming process\n",
    "rename_images(image_dir, yolo_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # current carapace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\\carapace\\right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:01<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\\carapace\\left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 25.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\\carapace\\car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 4/30 [00:00<00:00, 35.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ground truth found for undistorted_GX010161_136_2267.jpg_gamma.jpg_gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:00<00:00, 39.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ground truth found for undistorted_GX010169_121_1249\n",
      "No ground truth found for undistorted_GX010174_62_790.jpg_gamma\n",
      "No ground truth found for undistorted_GX010175_266_3372.jpg_gamma.jpg_gamma\n",
      "No ground truth found for undistorted_GX010175_82_852.jpg_gamma\n",
      "No ground truth found for undistorted_GX010177_232_3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:00<00:00, 52.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ground truth found for undistorted_GX010178_114_2258\n",
      "No ground truth found for undistorted_GX010178_172_3604.jpg_gamma\n",
      "No ground truth found for undistorted_GX010179_200_3927.jpg_gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 40.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5154/?notebook=True&subscription=acc58b7f-223f-452a-a831-cdff34d04533\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20bb7391910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main.py\n",
    "import fiftyone as fo\n",
    "import os\n",
    "from data_loader import load_data, create_dataset, process_images\n",
    "from utils import parse_pose_estimation, calculate_euclidean_distance, calculate_real_width, extract_identifier_from_gt\n",
    "# from measurements.metrics import  compute_dataset_map\n",
    "\n",
    "\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "# Reload the module\n",
    "reload(sys.modules['data_loader'])\n",
    "reload(sys.modules['utils'])\n",
    "\n",
    "\n",
    "# Paths to data files\n",
    "filtered_data_file_path = r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\src\\measurement\\ImageJ\\Filtered_Data.csv'\n",
    "metadata_file_path = r\"C:\\Users\\gbo10\\OneDrive\\research\\thesis and paper\\test images.xlsx\"\n",
    "right_folder_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right'\n",
    "right_prediction_folder_path = r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict35\\labels\"\n",
    "right_ground_truth_folder_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\current from roboflow -28.8-first sense\\measurement\\test\\test-right\\valid\\labels\"\n",
    "\n",
    "left_folder_path=r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left'\n",
    "left_prediction_folder_path=r\"C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict36\\labels\"\n",
    "left_ground_truth_folder_path=r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\current from roboflow -28.8-first sense\\measurement\\test\\test-left\\valid\\labels\"\n",
    "\n",
    "car_folder_path=r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\car'\n",
    "car_prediction_folder_path=r'C:\\Users\\gbo10\\Videos\\data-science\\Research-counting-algorithms\\runs\\pose\\predict38\\labels'\n",
    "car_ground_truth_folder_path=r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\current from roboflow -28.8-first sense\\measurement\\test\\test-square\\valid\\labels'\n",
    "\n",
    "\n",
    "# Load data and metadata\n",
    "filtered_df, metadata_df = load_data(filtered_data_file_path, metadata_file_path)\n",
    "\n",
    "# Create FiftyOne dataset\n",
    "dataset = create_dataset()\n",
    "\n",
    "for folder_path, prediction_folder_path, ground_truth_folder_path in zip([right_folder_path, left_folder_path, car_folder_path],\n",
    "                                                                         [right_prediction_folder_path, left_prediction_folder_path, car_prediction_folder_path],\n",
    "                                                                         [right_ground_truth_folder_path, left_ground_truth_folder_path, car_ground_truth_folder_path]):\n",
    "    # Get image paths\n",
    "    image_paths = [os.path.join(folder_path, img) for img in os.listdir(folder_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    # Get ground truth paths\n",
    "    ground_truth_paths = [os.path.join(ground_truth_folder_path, txt) for txt in os.listdir(ground_truth_folder_path) if txt.endswith('.txt')]\n",
    "\n",
    "    prediction_paths=[os.path.join(prediction_folder_path, txt) for txt in os.listdir(prediction_folder_path) if txt.endswith('.txt')]\n",
    "\n",
    "    \n",
    "    # Process images and predictions using segmentations and bounding boxes\n",
    "    pond_tag=folder_path.split('/')[-1]\n",
    "    print(pond_tag)\n",
    "    process_images(image_paths=image_paths,ground_truth_paths_text=ground_truth_paths,prediction_folder_path= prediction_folder_path,filtered_df= filtered_df,metadata_df= metadata_df,dataset= dataset,pond_type=pond_tag)\n",
    "\n",
    "# Process images and predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start the FiftyOne session\n",
    "session = fo.launch_app(dataset, port=5154)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\fifty_one', 'C:\\\\Users\\\\gbo10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\python39.zip', 'C:\\\\Users\\\\gbo10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\DLLs', 'C:\\\\Users\\\\gbo10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib', 'C:\\\\Users\\\\gbo10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv', '', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\.venv\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\gbo10\\\\Videos\\\\research\\\\counting_research_algorithms\\\\fifty_one']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add swimming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36048\\591749776.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36048\\591749776.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(updated_df, swimming_df, threshold)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mupdated_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprawn_bbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BoundingBox_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Extract the bounding box of the prawn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Get the corresponding rows from swimming_df based on image Label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mmatching_swimming_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswimming_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mswimming_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Check each matching row in swimming_df to see if the bounding boxes are close enough\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswim_row\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatching_swimming_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Helper function to calculate Euclidean distance between two bounding box points\n",
    "def calculate_euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Function to check if the bounding boxes are close enough based on a distance threshold\n",
    "def find_closest_swimming_bbox(prawn_bbox, swimming_rows, threshold=50):\n",
    "    prawn_point = (prawn_bbox[0], prawn_bbox[1])  # Top-left corner of the prawn bounding box\n",
    "    min_distance = float('inf')\n",
    "    closest_swimming_bbox = None\n",
    "\n",
    "    # Iterate through all swimming bounding boxes\n",
    "    for _, swim_row in swimming_rows.iterrows():\n",
    "        swim_point = (swim_row['BX'], swim_row['BY'])  # Top-left corner of the swimming bounding box\n",
    "\n",
    "        # Calculate the Euclidean distance\n",
    "        distance = calculate_euclidean_distance(prawn_point, swim_point)\n",
    "\n",
    "        # If the distance is smaller than the current minimum, update the closest match\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_swimming_bbox = swim_row\n",
    "\n",
    "    # Return True if a close enough match is found, otherwise False\n",
    "    return min_distance < threshold\n",
    "\n",
    "# Add the swimming status to the updated dataframe\n",
    "def add_swimming_status(updated_df, swimming_df, threshold=50):\n",
    "    # Initialize the swimming column with default value \"not swimming\"\n",
    "    updated_df['swimming'] = 'not swimming'\n",
    "\n",
    "    # Iterate over each prawn in updated_df\n",
    "    for i, row in updated_df.iterrows():\n",
    "        prawn_label = row['Label'].split(':')[1]  # Extract the part after 'carapace:' from prawn label\n",
    "        prawn_bbox = ast.literal_eval(row['BoundingBox_1'])  # Convert bounding box string to a tuple\n",
    "\n",
    "        # Get corresponding rows from swimming_df with the same Label\n",
    "        matching_swimming_rows = swimming_df[swimming_df['Label'].apply(lambda x: x.split(':')[1]) == prawn_label]\n",
    "\n",
    "        # Check each matching row in swimming_df to see if the bounding boxes are close enough\n",
    "        if not matching_swimming_rows.empty:\n",
    "            if find_closest_swimming_bbox(prawn_bbox, matching_swimming_rows, threshold):\n",
    "                # If a close match is found, mark the prawn as swimming\n",
    "                updated_df.at[i, 'swimming'] = 'swimming'\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "# Load the updated dataframe and the swimming dataframe\n",
    "updated_df = pd.read_excel(r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\fifty_one\\measurements\\Updated_Filtered_Data_with_real_length.xlsx')\n",
    "swimming_df = pd.read_excel(r'C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\src\\measurement\\ImageJ\\Swimming.xlsx')\n",
    "\n",
    "# Add the swimming status column\n",
    "updated_df_with_swimming = add_swimming_status(updated_df, swimming_df)\n",
    "\n",
    "# Save the updated dataframe to a new Excel file\n",
    "updated_df_with_swimming.to_excel('Updated_Filtered_Data_with_Swimming.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from random import randint, shuffle\n",
    "import fiftyone as fo\n",
    "import fiftyone.core.labels as fol\n",
    "\n",
    "# Helper Classes and Functions for Circle Calculation\n",
    "class Point:\n",
    "    def __init__(self, X=0, Y=0) -> None:\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "class Circle:\n",
    "    def __init__(self, c=Point(), r=0) -> None:    \n",
    "        self.C = c\n",
    "        self.R = r\n",
    "\n",
    "def dist(a, b):\n",
    "    return sqrt((a.X - b.X) ** 2 + (a.Y - b.Y) ** 2)\n",
    "\n",
    "def is_inside(c, p):\n",
    "    return dist(c.C, p) <= c.R\n",
    "\n",
    "def get_circle_center(bx, by, cx, cy):\n",
    "    B = bx * bx + by * by\n",
    "    C = cx * cx + cy * cy\n",
    "    D = bx * cy - by * cx\n",
    "    return Point((cy * B - by * C) / (2 * D), (bx * C - cx * B) / (2 * D))\n",
    "\n",
    "def circle_from1(A, B):\n",
    "    C = Point((A.X + B.X) / 2.0, (A.Y + B.Y) / 2.0)\n",
    "    return Circle(C, dist(A, B) / 2.0)\n",
    "\n",
    "def circle_from2(A, B, C):\n",
    "    I = get_circle_center(B.X - A.X, B.Y - A.Y, C.X - A.X, C.Y - A.Y)\n",
    "    I.X += A.X\n",
    "    I.Y += A.Y\n",
    "    return Circle(I, dist(I, A))\n",
    "\n",
    "def is_valid_circle(c, P):\n",
    "    for p in P:\n",
    "        if not is_inside(c, p):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def min_circle_trivial(P):\n",
    "    assert len(P) <= 3\n",
    "    if not P:\n",
    "        return Circle()\n",
    "    elif len(P) == 1:\n",
    "        return Circle(P[0], 0)\n",
    "    elif len(P) == 2:\n",
    "        return circle_from1(P[0], P[1])\n",
    "    for i in range(3):\n",
    "        for j in range(i + 1, 3):\n",
    "            c = circle_from1(P[i], P[j])\n",
    "            if is_valid_circle(c, P):\n",
    "                return c\n",
    "    return circle_from2(P[0], P[1], P[2])\n",
    "\n",
    "def welzl_helper(P, R, n):\n",
    "    if n == 0 or len(R) == 3:\n",
    "        return min_circle_trivial(R)\n",
    "    idx = randint(0, n - 1)\n",
    "    p = P[idx]\n",
    "    P[idx], P[n - 1] = P[n - 1], P[idx]\n",
    "    d = welzl_helper(P, R.copy(), n - 1)\n",
    "    if is_inside(d, p):\n",
    "        return d\n",
    "    R.append(p)\n",
    "    return welzl_helper(P, R.copy(), n - 1)\n",
    "\n",
    "def welzl(P):\n",
    "    P_copy = P.copy()\n",
    "    shuffle(P_copy)\n",
    "    return welzl_helper(P_copy, [], len(P_copy))\n",
    "\n",
    "# Helper function to load segmentations from a .txt file\n",
    "def load_segmentations_from_txt(txt_file):\n",
    "    \"\"\"\n",
    "    Loads segmentation polygons from a .txt file without scaling the coordinates.\n",
    "    Each line in the file contains space-separated coordinates: x1 y1 x2 y2 ... xn yn.\n",
    "    Assumes coordinates are already scaled to 640x640.\n",
    "    \"\"\"\n",
    "    segmentations = []\n",
    "    with open(txt_file, 'r') as file:\n",
    "        for line in file:\n",
    "            coords = list(map(float, line.strip().split()))  # Read all floats from the line\n",
    "            polygon = [(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]  # Directly read the coordinates\n",
    "            segmentations.append(polygon)\n",
    "    return segmentations\n",
    "\n",
    "\n",
    "# Initialize FiftyOne dataset\n",
    "dataset = fo.Dataset(\"prawn_segmentation\", overwrite=True)\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "# Assuming image size is always 640x640 based on your .txt files\n",
    "\n",
    "# Image directory and YOLO annotation files\n",
    "image_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\"\n",
    "segmentation_dir = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\" # Directory with the segmentation .txt files\n",
    "\n",
    "# Iterate over images\n",
    "for image_name in os.listdir(image_dir):\n",
    "    if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "       \n",
    "        core_name = image_name.split('.')[0]\n",
    "        # Create the corresponding segmentation txt file path\n",
    "        segmentation_file = os.path.join(segmentation_dir, f\"{core_name}_segmentations.txt\")\n",
    "        \n",
    "        if not os.path.exists(segmentation_file):\n",
    "            print(f\"Segmentation file not found for {core_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Load image (assuming it's also 640x640)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Load segmentations from the .txt file (no scaling needed)\n",
    "        all_polygons = load_segmentations_from_txt(segmentation_file)\n",
    "        \n",
    "        # Initialize list to store segmentations and attributes\n",
    "        all_attributes = []\n",
    "        for polygon in all_polygons:\n",
    "            points = [Point(p[0], p[1]) for p in polygon]\n",
    "            mec = welzl(points)  # Compute the minimum enclosing circle\n",
    "            diameter = mec.R * 2  # Get the diameter of the circle\n",
    "            all_attributes.append({'diameter': diameter})\n",
    "        \n",
    "        # Create Fift   yOne polylines\n",
    "        segmentations = []\n",
    "\n",
    "        for polygon, diameter in zip(all_polygons, all_attributes):\n",
    "            normalized_points = [[(x / 640.0, y / 640.0) for x, y in polygon]]\n",
    "            \n",
    "            segmentation = fo.Polyline(\n",
    "                points=normalized_points,\n",
    "                closed=True,\n",
    "                filled=True,\n",
    "                diameter=diameter['diameter']\n",
    "            )\n",
    "            segmentations.append(segmentation)\n",
    "\n",
    "        \n",
    "        # Add image and segmentation to FiftyOne dataset\n",
    "        sample = fo.Sample(filepath=image_path)\n",
    "        sample[\"segmentations\"] = fol.Polylines(polylines=segmentations)\n",
    "\n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "# Save dataset and launch FiftyOne app\n",
    "dataset.save()\n",
    "session = fo.launch_app(dataset, port=5153)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\gbo10\\AppData\\Local\\Temp\\ipykernel_69224\\3919491449.py\", line 45, in <module>\n",
      "    process_images(image_paths, prediction_folder_path, filtered_df, metadata_df, dataset,pond_tag=pond_tag)\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\fifty_one\\measurements\\data_loader_body.py\", line 505, in process_images\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\fifty_one\\measurements\\data_loader_body.py\", line 547, in add_metadata\n",
      "    add_prawn_detections(sample, matching_rows, filtered_df, filename)\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\fifty_one\\measurements\\data_loader_body.py\", line 640, in add_prawn_detections\n",
      "    process_detection_by_circle(segmentation, sample, filename, prawn_id, filtered_df)\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\fifty_one\\measurements\\data_loader_body.py\", line 414, in process_detection_by_circle\n",
      "    true_length = (filtered_df.loc[(filtered_df['Label'] == f'full body:{filename}') & (filtered_df['PrawnID'] == prawn_id), 'Length_1'].values[0],filtered_df.loc[(filtered_df['Label'] == f'full body:{filename}') & (filtered_df['PrawnID'] == prawn_id), 'Length_2'].values[0],filtered_df.loc[(filtered_df['Label'] == f'full body:{filename}') & (filtered_df['PrawnID'] == prawn_id), 'Length_3'].values[0]).mean()\n",
      "AttributeError: 'tuple' object has no attribute 'mean'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\.venv\\lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "from data_loader import load_data, create_dataset, process_images\n",
    "from utils import calculate_euclidean_distance, calculate_real_width, extract_identifier_from_gt\n",
    "from data_loader_body import load_data, create_dataset, process_images\n",
    "import MEC\n",
    "import skeletonization\n",
    "\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "# Reload the necessary modules\n",
    "reload(sys.modules['data_loader_body'])\n",
    "reload(sys.modules['utils'])\n",
    "reload(sys.modules['MEC'])\n",
    "reload(sys.modules['skeletonization'])\n",
    "\n",
    "# Paths to data files\n",
    "filtered_data_file_path = r\"C:\\Users\\gbo10\\Videos\\research\\counting_research_algorithms\\src\\measurement\\ImageJ\\final_full_statistics_with_prawn_ids_and_uncertainty - Copy.xlsx\"\n",
    "metadata_file_path = r\"C:\\Users\\gbo10\\OneDrive\\research\\thesis and paper\\test images.xlsx\"\n",
    "car_image_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\car'\n",
    "car_prediction_folder_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\car_resized\"\n",
    "\n",
    "left_image_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left'\n",
    "left_prediction_folder_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\left_resized\"\n",
    "\n",
    "right_image_path = r'C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right'\n",
    "right_prediction_folder_path = r\"C:\\Users\\gbo10\\OneDrive\\measurement_paper_images\\images used for imageJ\\check\\stabilized\\shai\\measurements/1\\carapace\\right_resized\"\n",
    "\n",
    "# Load data and metadata\n",
    "filtered_df, metadata_df = load_data(filtered_data_file_path, metadata_file_path)\n",
    "# Create the FiftyOne dataset\n",
    "\n",
    "\n",
    "dataset = create_dataset()\n",
    "\n",
    "\n",
    "for folder_path, prediction_folder_path in [(car_image_path, car_prediction_folder_path), (left_image_path, left_prediction_folder_path), (right_image_path, right_prediction_folder_path)]:\n",
    "    image_paths = [os.path.join(folder_path, image) for image in os.listdir(folder_path) if image.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "    prediction_paths_text = [os.path.join(prediction_folder_path, txt) for txt in os.listdir(prediction_folder_path) if txt.endswith('.txt')]\n",
    "\n",
    "    # Process images and predictions using segmentations and bounding boxes\n",
    "    pond_tag=folder_path.split('/')[-1]\n",
    "\n",
    "    process_images(image_paths, prediction_folder_path, filtered_df, metadata_df, dataset,pond_tag=pond_tag)\n",
    "\n",
    "# Start the FiftyOne session\n",
    "session = fo.launch_app(dataset, port=5153)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
